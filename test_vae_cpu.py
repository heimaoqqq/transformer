#!/usr/bin/env python3
"""
CPUç‰ˆæœ¬VAEæ¶æ„æµ‹è¯•
"""

import torch
from diffusers import AutoencoderKL

def test_vae_architecture_cpu():
    """åœ¨CPUä¸Šæµ‹è¯•VAEæ¶æ„"""
    print("ğŸ§ª CPUç‰ˆæœ¬VAEæ¶æ„æµ‹è¯•")
    print("=" * 50)
    
    device = "cpu"
    
    # æµ‹è¯•ä¿®å¤åçš„é…ç½® (3å±‚ä¸‹é‡‡æ ·)
    print("\nğŸ—ï¸  æµ‹è¯•ä¿®å¤åçš„é…ç½® (3å±‚DownEncoderBlock2D):")
    try:
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],  # 3å±‚
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],        # 3å±‚
            block_out_channels=[128, 256, 512],                                                   # 3å±‚é€šé“æ•°
            latent_channels=4,
            sample_size=128,
            layers_per_block=1,
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        ).to(device)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in vae.parameters())
        trainable_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)
        
        print(f"   âœ… æ€»å‚æ•°: {total_params:,}")
        print(f"   âœ… å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_input = torch.randn(1, 3, 128, 128).to(device)
        
        with torch.no_grad():
            # ç¼–ç 
            latent_dist = vae.encode(test_input).latent_dist
            latent = latent_dist.sample()
            print(f"   âœ… æ½œåœ¨ç©ºé—´å½¢çŠ¶: {latent.shape}")
            
            # è§£ç 
            reconstructed = vae.decode(latent).sample
            print(f"   âœ… é‡å»ºå½¢çŠ¶: {reconstructed.shape}")
            
            # éªŒè¯å½¢çŠ¶
            expected_latent_shape = (1, 4, 32, 32)
            expected_output_shape = (1, 3, 128, 128)
            
            if latent.shape == expected_latent_shape:
                print(f"   âœ… æ½œåœ¨ç©ºé—´å½¢çŠ¶æ­£ç¡®: {latent.shape}")
                success = True
            else:
                print(f"   âŒ æ½œåœ¨ç©ºé—´å½¢çŠ¶é”™è¯¯: {latent.shape}, æœŸæœ›: {expected_latent_shape}")
                success = False
                
            if reconstructed.shape == expected_output_shape:
                print(f"   âœ… é‡å»ºå½¢çŠ¶æ­£ç¡®: {reconstructed.shape}")
            else:
                print(f"   âŒ é‡å»ºå½¢çŠ¶é”™è¯¯: {reconstructed.shape}, æœŸæœ›: {expected_output_shape}")
                success = False
        
        # è®¡ç®—å‹ç¼©æ¯”
        input_pixels = 128 * 128 * 3
        latent_pixels = 32 * 32 * 4
        compression_ratio = input_pixels / latent_pixels
        
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
        print(f"   ğŸ“ è¾“å…¥åƒç´ : {input_pixels:,}")
        print(f"   ğŸ¯ æ½œåœ¨åƒç´ : {latent_pixels:,}")
        
        return success
        
    except Exception as e:
        print(f"   âŒ VAEæ¶æ„æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ VAEæ¶æ„ä¿®å¤éªŒè¯")
    
    success = test_vae_architecture_cpu()
    
    if success:
        print(f"\nğŸ‰ æ¶æ„ä¿®å¤æˆåŠŸ!")
        print(f"âœ… ç°åœ¨å¯ä»¥æ­£ç¡®å®ç° 128Ã—128 â†’ 32Ã—32 ä¸‹é‡‡æ ·")
        print(f"âœ… å‹ç¼©æ¯”: 12:1 (ç¬¦åˆé¢„æœŸ)")
        print(f"âœ… å¯ä»¥å¼€å§‹è®­ç»ƒ: python train_improved_quality.py")
    else:
        print(f"\nâŒ æ¶æ„ä»æœ‰é—®é¢˜!")
        print(f"ğŸ”§ éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    main()
