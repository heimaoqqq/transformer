#!/usr/bin/env python3
"""
Diffusersåº“å…¼å®¹æ€§ä¸“é¡¹æµ‹è¯•
éªŒè¯æ‰€æœ‰ä½¿ç”¨çš„APIå’Œå‚æ•°æ˜¯å¦åœ¨å½“å‰ç‰ˆæœ¬ä¸­å¯ç”¨
"""

import warnings
warnings.filterwarnings("ignore")

def test_diffusers_version():
    """æµ‹è¯•Diffusersç‰ˆæœ¬"""
    print("ğŸ” æ£€æŸ¥Diffusersç‰ˆæœ¬...")
    
    try:
        import diffusers
        version = diffusers.__version__
        print(f"âœ… Diffusersç‰ˆæœ¬: {version}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨èç‰ˆæœ¬
        from packaging import version as pkg_version
        if pkg_version.parse(version) >= pkg_version.parse("0.25.0"):
            print("âœ… ç‰ˆæœ¬ç¬¦åˆè¦æ±‚ (>= 0.25.0)")
            return True
        else:
            print(f"âš ï¸  ç‰ˆæœ¬è¿‡ä½ï¼Œæ¨èå‡çº§åˆ° >= 0.25.0")
            return False
            
    except ImportError:
        print("âŒ Diffusersæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_autoencoder_kl():
    """æµ‹è¯•AutoencoderKLçš„æ‰€æœ‰å‚æ•°"""
    print("\nğŸ”§ æµ‹è¯•AutoencoderKL...")
    
    try:
        from diffusers import AutoencoderKL
        
        # æµ‹è¯•æˆ‘ä»¬ä½¿ç”¨çš„æ‰€æœ‰å‚æ•°
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=[
                "DownEncoderBlock2D",
                "DownEncoderBlock2D", 
                "DownEncoderBlock2D",
                "DownEncoderBlock2D"
            ],
            up_block_types=[
                "UpDecoderBlock2D",
                "UpDecoderBlock2D",
                "UpDecoderBlock2D",
                "UpDecoderBlock2D"
            ],
            block_out_channels=[128, 256, 512, 512],
            latent_channels=4,
            sample_size=256,
            layers_per_block=2,
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        )
        
        print("âœ… AutoencoderKLåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ–¹æ³•
        import torch
        test_input = torch.randn(1, 3, 256, 256)
        
        with torch.no_grad():
            # æµ‹è¯•encodeæ–¹æ³•
            posterior = vae.encode(test_input)
            print("âœ… encodeæ–¹æ³•å¯ç”¨")
            
            # æµ‹è¯•latent_distå±æ€§
            latent_dist = posterior.latent_dist
            print("âœ… latent_distå±æ€§å¯ç”¨")
            
            # æµ‹è¯•sampleæ–¹æ³•
            latents = latent_dist.sample()
            print("âœ… sampleæ–¹æ³•å¯ç”¨")
            
            # æµ‹è¯•decodeæ–¹æ³•
            decoded = vae.decode(latents)
            print("âœ… decodeæ–¹æ³•å¯ç”¨")
            
            # æµ‹è¯•sampleå±æ€§
            output = decoded.sample
            print("âœ… sampleå±æ€§å¯ç”¨")
            
            # æµ‹è¯•configå±æ€§
            scaling_factor = vae.config.scaling_factor
            print(f"âœ… config.scaling_factorå¯ç”¨: {scaling_factor}")
        
        # æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
        try:
            vae.save_pretrained("./test_vae")
            loaded_vae = AutoencoderKL.from_pretrained("./test_vae")
            print("âœ… save_pretrained/from_pretrainedæ–¹æ³•å¯ç”¨")
            
            # æ¸…ç†
            import shutil
            shutil.rmtree("./test_vae")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜/åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ AutoencoderKLæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_unet2d_condition_model():
    """æµ‹è¯•UNet2DConditionModelçš„æ‰€æœ‰å‚æ•°"""
    print("\nğŸ”§ æµ‹è¯•UNet2DConditionModel...")
    
    try:
        from diffusers import UNet2DConditionModel
        
        # æµ‹è¯•æˆ‘ä»¬ä½¿ç”¨çš„æ‰€æœ‰å‚æ•°
        unet = UNet2DConditionModel(
            sample_size=32,
            in_channels=4,
            out_channels=4,
            layers_per_block=2,
            block_out_channels=(320, 640, 1280, 1280),
            down_block_types=(
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
            ),
            cross_attention_dim=768,
            attention_head_dim=8,
            use_linear_projection=True,
            class_embed_type=None,
            num_class_embeds=None,
            upcast_attention=False,
            resnet_time_scale_shift="default",
            time_embedding_type="positional",
            time_embedding_dim=None,
            time_embedding_act_fn=None,
            timestep_post_act=None,
            time_cond_proj_dim=None,
        )
        
        print("âœ… UNet2DConditionModelåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        import torch
        
        with torch.no_grad():
            latents = torch.randn(1, 4, 32, 32)
            timesteps = torch.randint(0, 1000, (1,))
            encoder_hidden_states = torch.randn(1, 1, 768)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            output = unet(
                latents,
                timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )
            
            print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output[0].shape}")
            
            # æµ‹è¯•return_dict=True
            output_dict = unet(
                latents,
                timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=True
            )
            
            print("âœ… return_dict=Trueå¯ç”¨")
            print(f"âœ… sampleå±æ€§å¯ç”¨: {output_dict.sample.shape}")
        
        # æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
        try:
            unet.save_pretrained("./test_unet")
            loaded_unet = UNet2DConditionModel.from_pretrained("./test_unet")
            print("âœ… save_pretrained/from_pretrainedæ–¹æ³•å¯ç”¨")
            
            # æ¸…ç†
            import shutil
            shutil.rmtree("./test_unet")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜/åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ UNet2DConditionModelæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_schedulers():
    """æµ‹è¯•è°ƒåº¦å™¨"""
    print("\nğŸ”§ æµ‹è¯•è°ƒåº¦å™¨...")
    
    try:
        from diffusers import DDPMScheduler, DDIMScheduler
        
        # æµ‹è¯•DDPMScheduler
        ddpm_scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.00085,
            beta_end=0.012,
            beta_schedule="scaled_linear",
            trained_betas=None,
            variance_type="fixed_small",
            clip_sample=False,
            prediction_type="epsilon",
            thresholding=False,
            dynamic_thresholding_ratio=0.995,
            clip_sample_range=1.0,
            sample_max_value=1.0,
        )
        
        print("âœ… DDPMScheduleråˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•DDIMScheduler
        ddim_scheduler = DDIMScheduler.from_config(ddpm_scheduler.config)
        print("âœ… DDIMScheduler.from_configå¯ç”¨")
        
        # æµ‹è¯•è°ƒåº¦å™¨æ–¹æ³•
        import torch
        
        latents = torch.randn(1, 4, 32, 32)
        noise = torch.randn_like(latents)
        timesteps = torch.randint(0, 1000, (1,))
        
        # æµ‹è¯•add_noise
        noisy_latents = ddpm_scheduler.add_noise(latents, noise, timesteps)
        print("âœ… add_noiseæ–¹æ³•å¯ç”¨")
        
        # æµ‹è¯•step
        noise_pred = torch.randn_like(latents)
        step_output = ddpm_scheduler.step(noise_pred, timesteps[0], latents, return_dict=False)
        print("âœ… stepæ–¹æ³•å¯ç”¨")
        
        # æµ‹è¯•set_timesteps
        ddim_scheduler.set_timesteps(50)
        print("âœ… set_timestepsæ–¹æ³•å¯ç”¨")
        print(f"âœ… timestepså±æ€§å¯ç”¨: {len(ddim_scheduler.timesteps)}")
        
        # æµ‹è¯•init_noise_sigma
        init_sigma = ddim_scheduler.init_noise_sigma
        print(f"âœ… init_noise_sigmaå¯ç”¨: {init_sigma}")
        
        # æµ‹è¯•scale_model_input
        scaled_input = ddim_scheduler.scale_model_input(latents, timesteps[0])
        print("âœ… scale_model_inputæ–¹æ³•å¯ç”¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ è°ƒåº¦å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_pipeline_components():
    """æµ‹è¯•Pipelineç›¸å…³ç»„ä»¶"""
    print("\nğŸ”§ æµ‹è¯•Pipelineç»„ä»¶...")
    
    try:
        # æµ‹è¯•æ˜¯å¦å¯ä»¥å¯¼å…¥PipelineåŸºç±»
        from diffusers import DiffusionPipeline
        print("âœ… DiffusionPipelineå¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•StableDiffusionPipeline (ç”¨äºå‚è€ƒ)
        from diffusers import StableDiffusionPipeline
        print("âœ… StableDiffusionPipelineå¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ Pipelineç»„ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_training_utils():
    """æµ‹è¯•è®­ç»ƒç›¸å…³å·¥å…·"""
    print("\nğŸ”§ æµ‹è¯•è®­ç»ƒå·¥å…·...")
    
    try:
        # æµ‹è¯•EMA (å¦‚æœä½¿ç”¨)
        try:
            from diffusers import EMAModel
            print("âœ… EMAModelå¯ç”¨")
        except ImportError:
            print("âš ï¸  EMAModelä¸å¯ç”¨ (å¯é€‰)")
        
        # æµ‹è¯•ä¼˜åŒ–ç›¸å…³
        from diffusers.optimization import get_scheduler
        print("âœ… get_schedulerå¯ç”¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå·¥å…·æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_full_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
    print("\nğŸ”§ æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹...")
    
    try:
        import torch
        from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºæ¨¡å‹
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            latent_channels=4,
            sample_size=256,
        ).to(device)
        
        unet = UNet2DConditionModel(
            sample_size=32,
            in_channels=4,
            out_channels=4,
            cross_attention_dim=768,
        ).to(device)
        
        scheduler = DDIMScheduler(num_train_timesteps=1000)
        scheduler.set_timesteps(50)
        
        print("âœ… æ‰€æœ‰æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æ¨¡æ‹Ÿå®Œæ•´çš„ç”Ÿæˆæµç¨‹
        with torch.no_grad():
            # 1. ç¼–ç å›¾åƒ
            test_image = torch.randn(1, 3, 256, 256).to(device)
            latents = vae.encode(test_image).latent_dist.sample()
            latents = latents * vae.config.scaling_factor
            
            print("âœ… å›¾åƒç¼–ç æˆåŠŸ")
            
            # 2. æ·»åŠ å™ªå£° (è®­ç»ƒæ—¶)
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, 1000, (1,)).to(device)
            noisy_latents = scheduler.add_noise(latents, noise, timesteps)
            
            print("âœ… å™ªå£°æ·»åŠ æˆåŠŸ")
            
            # 3. æ¡ä»¶ç¼–ç 
            encoder_hidden_states = torch.randn(1, 1, 768).to(device)
            
            # 4. UNeté¢„æµ‹
            noise_pred = unet(
                noisy_latents,
                timesteps,
                encoder_hidden_states=encoder_hidden_states
            ).sample
            
            print("âœ… UNeté¢„æµ‹æˆåŠŸ")
            
            # 5. å»å™ªè¿‡ç¨‹ (æ¨ç†æ—¶)
            latents_gen = torch.randn(1, 4, 32, 32).to(device)
            latents_gen = latents_gen * scheduler.init_noise_sigma
            
            for t in scheduler.timesteps[:5]:  # åªæµ‹è¯•å‰5æ­¥
                latent_model_input = scheduler.scale_model_input(latents_gen, t)
                
                noise_pred = unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=encoder_hidden_states
                ).sample
                
                latents_gen = scheduler.step(noise_pred, t, latents_gen).prev_sample
            
            print("âœ… å»å™ªè¿‡ç¨‹æˆåŠŸ")
            
            # 6. è§£ç å›¾åƒ
            latents_gen = latents_gen / vae.config.scaling_factor
            generated_image = vae.decode(latents_gen).sample
            
            print("âœ… å›¾åƒè§£ç æˆåŠŸ")
            print(f"âœ… å®Œæ•´æµç¨‹: {test_image.shape} -> {latents.shape} -> {generated_image.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª Diffusersåº“å…¼å®¹æ€§ä¸“é¡¹æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("Diffusersç‰ˆæœ¬", test_diffusers_version),
        ("AutoencoderKL", test_autoencoder_kl),
        ("UNet2DConditionModel", test_unet2d_condition_model),
        ("è°ƒåº¦å™¨", test_schedulers),
        ("Pipelineç»„ä»¶", test_pipeline_components),
        ("è®­ç»ƒå·¥å…·", test_training_utils),
        ("å®Œæ•´å·¥ä½œæµç¨‹", test_full_workflow),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("\nğŸ‰ æ‰€æœ‰Diffusers APIæµ‹è¯•é€šè¿‡ï¼")
        print("âœ… å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨å½“å‰ç‰ˆæœ¬è¿›è¡Œè®­ç»ƒ")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("å»ºè®®:")
        print("1. å‡çº§Diffusers: pip install --upgrade diffusers>=0.25.0")
        print("2. æ£€æŸ¥PyTorchç‰ˆæœ¬å…¼å®¹æ€§")
        print("3. é‡æ–°è¿è¡Œæµ‹è¯•")
    
    return passed == len(results)

if __name__ == "__main__":
    main()
