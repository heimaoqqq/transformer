#!/usr/bin/env python3
"""
GPUè‡ªé€‚åº”è®­ç»ƒå™¨
- P100: FP32 + å¤§æ‰¹æ¬¡
- T4: FP16 + é€‚ä¸­æ‰¹æ¬¡
"""

import os
import sys
import subprocess
import torch
from pathlib import Path

def detect_gpu_type():
    """æ£€æµ‹GPUç±»å‹"""
    if not torch.cuda.is_available():
        return None
    
    gpu_name = torch.cuda.get_device_properties(0).name
    
    if "P100" in gpu_name:
        return "P100"
    elif "T4" in gpu_name:
        return "T4"
    else:
        return "Unknown"

def get_gpu_optimized_config(gpu_type):
    """æ ¹æ®GPUç±»å‹è·å–ä¼˜åŒ–é…ç½®"""
    
    if gpu_type == "P100":
        return {
            "batch_size": 8,            # P100ä¿å®ˆæ‰¹æ¬¡
            "mixed_precision": "no",    # P100æ²¡æœ‰Tensor Core
            "learning_rate": "0.0002",  # é€‚ä¸­å­¦ä¹ ç‡
            "gradient_accumulation": 2, # é€‚ä¸­ç´¯ç§¯
            "num_workers": 2,           # é€‚ä¸­çº¿ç¨‹
            "memory_efficient": True,   # ä¿å®ˆå†…å­˜ç®¡ç†
        }
    elif gpu_type == "T4":
        return {
            "batch_size": 4,            # T4ä¿å®ˆæ‰¹æ¬¡
            "mixed_precision": "fp16",  # T4æœ‰Tensor Core
            "learning_rate": "0.00015", # é™ä½å­¦ä¹ ç‡
            "gradient_accumulation": 4, # å¢åŠ ç´¯ç§¯ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡
            "num_workers": 1,           # å•çº¿ç¨‹é¿å…ç«äº‰
            "memory_efficient": True,   # T4éœ€è¦èŠ‚çœå†…å­˜
        }
    else:
        # ä¿å®ˆé…ç½®
        return {
            "batch_size": 4,
            "mixed_precision": "no",
            "learning_rate": "0.0001",
            "gradient_accumulation": 4,
            "num_workers": 1,
            "memory_efficient": True,
        }

def setup_gpu_environment(gpu_type, config):
    """è®¾ç½®GPUç¯å¢ƒ"""
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # å•GPUé¿å…é€šä¿¡å¼€é”€
    os.environ['PYTHONUNBUFFERED'] = '1'
    
    if config["memory_efficient"]:
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    else:
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    # GPUç‰¹å®šä¼˜åŒ–
    if gpu_type == "P100":
        # P100ä¼˜åŒ–ï¼šFP32æ€§èƒ½å¼º
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
    elif gpu_type == "T4":
        # T4ä¼˜åŒ–ï¼šTensor Core
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.allow_tf32 = True

def launch_adaptive_training():
    """å¯åŠ¨è‡ªé€‚åº”è®­ç»ƒ"""
    
    # æ£€æµ‹GPUç±»å‹
    gpu_type = detect_gpu_type()
    
    if gpu_type is None:
        print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°CUDA GPU")
        return False
    
    print(f"ğŸ® æ£€æµ‹åˆ°GPU: {gpu_type}")
    
    # è·å–ä¼˜åŒ–é…ç½®
    config = get_gpu_optimized_config(gpu_type)
    
    # è®¾ç½®ç¯å¢ƒ
    setup_gpu_environment(gpu_type, config)
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    props = torch.cuda.get_device_properties(0)
    print(f"   åç§°: {props.name}")
    print(f"   å†…å­˜: {props.total_memory / 1024**3:.1f} GB")
    print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    
    print(f"\nğŸš€ å¯åŠ¨{gpu_type}ä¼˜åŒ–è®­ç»ƒ...")
    
    cmd = [
        "python", "-u",
        "training/train_vae.py",
        "--data_dir", "/kaggle/input/dataset",
        "--output_dir", "/kaggle/working/outputs/vae",
        "--batch_size", str(config["batch_size"]),
        "--num_epochs", "25",  # é€‚ä¸­epochæ•°
        "--learning_rate", config["learning_rate"],
        "--mixed_precision", config["mixed_precision"],
        "--gradient_accumulation_steps", str(config["gradient_accumulation"]),
        "--kl_weight", "1e-6",
        "--perceptual_weight", "0.0",
        "--freq_weight", "0.05",
        "--resolution", "256",
        "--num_workers", str(config["num_workers"]),
        "--save_interval", "5",
        "--log_interval", "2",
        "--sample_interval", "50",
        "--experiment_name", f"kaggle_vae_{gpu_type.lower()}"
    ]
    
    print(f"ğŸ“Š {gpu_type}ä¼˜åŒ–é…ç½®:")
    print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"   ğŸ”¢ æ··åˆç²¾åº¦: {config['mixed_precision']}")
    print(f"   ğŸ“ˆ å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"   âš¡ æ¢¯åº¦ç´¯ç§¯: {config['gradient_accumulation']}")
    print(f"   ğŸ§µ æ•°æ®çº¿ç¨‹: {config['num_workers']}")
    print(f"   ğŸ’¾ å†…å­˜ä¼˜åŒ–: {config['memory_efficient']}")
    
    # é¢„æœŸæ€§èƒ½
    if gpu_type == "P100":
        print(f"\nğŸ“Š P100é¢„æœŸæ€§èƒ½:")
        print(f"   ğŸš€ FP32é«˜æ€§èƒ½è®¡ç®—")
        print(f"   ğŸ’¾ å¤§å†…å­˜æ”¯æŒå¤§æ‰¹æ¬¡")
        print(f"   â±ï¸  é¢„æœŸæ¯è½®: 8-12åˆ†é’Ÿ")
        print(f"   ğŸ¯ æ€»è®­ç»ƒæ—¶é—´: 3-5å°æ—¶")
    elif gpu_type == "T4":
        print(f"\nğŸ“Š T4é¢„æœŸæ€§èƒ½:")
        print(f"   ğŸš€ FP16 Tensor CoreåŠ é€Ÿ")
        print(f"   ğŸ’¾ å†…å­˜èŠ‚çœ50%")
        print(f"   â±ï¸  é¢„æœŸæ¯è½®: 12-18åˆ†é’Ÿ")
        print(f"   ğŸ¯ æ€»è®­ç»ƒæ—¶é—´: 5-7å°æ—¶")
    
    print(f"\nCommand: {' '.join(cmd)}")
    print("=" * 80)
    
    try:
        # å¯åŠ¨è®­ç»ƒ
        process = subprocess.Popen(
            cmd,
            stdout=None,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=0
        )
        
        return_code = process.wait()
        
        if return_code == 0:
            print(f"\nâœ… {gpu_type}ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
            return True
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥ (é€€å‡ºç : {return_code})")
            return False
            
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        process.terminate()
        return False
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¯åŠ¨å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ GPUè‡ªé€‚åº”VAEè®­ç»ƒ")
    print("=" * 50)
    
    gpu_type = detect_gpu_type()
    
    if gpu_type == "P100":
        print("ğŸ¯ P100ä¼˜åŒ–ç­–ç•¥:")
        print("   âœ… FP32ç²¾åº¦ (P100æ²¡æœ‰Tensor Core)")
        print("   âœ… å¤§æ‰¹æ¬¡è®­ç»ƒ (16GBå†…å­˜)")
        print("   âœ… é«˜å­¦ä¹ ç‡ (é…åˆå¤§æ‰¹æ¬¡)")
        print("   âœ… æ— æ¢¯åº¦ç´¯ç§¯ (æ€§èƒ½å¼º)")
        print("   âœ… å¤šçº¿ç¨‹æ•°æ®åŠ è½½")
        
    elif gpu_type == "T4":
        print("ğŸ¯ T4ä¼˜åŒ–ç­–ç•¥:")
        print("   âœ… FP16æ··åˆç²¾åº¦ (Tensor Core)")
        print("   âœ… é€‚ä¸­æ‰¹æ¬¡ (15GBå†…å­˜)")
        print("   âœ… å†…å­˜ä¼˜åŒ–")
        print("   âœ… é€‚ä¸­æ¢¯åº¦ç´¯ç§¯")
        
    print("\nğŸ—ï¸  é€šç”¨ä¼˜åŒ–:")
    print("   âœ… 3å±‚ä¸‹é‡‡æ · (55Må‚æ•°)")
    print("   âœ… å•GPUé¿å…é€šä¿¡å¼€é”€")
    print("   âœ… 256Ã—256é«˜åˆ†è¾¨ç‡")
    print("   âœ… 32Ã—32Ã—4æ½œåœ¨ç©ºé—´")
    
    success = launch_adaptive_training()
    
    if success:
        print(f"\nğŸ‰ {gpu_type}ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: /kaggle/working/outputs/vae/final_model")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main()
