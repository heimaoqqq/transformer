#!/usr/bin/env python3
"""
VAEä¸LDMå…¼å®¹æ€§æ£€æŸ¥è„šæœ¬
ç¡®ä¿VAEå’Œæ‰©æ•£æ¨¡å‹é…ç½®å®Œå…¨å…¼å®¹
"""

import torch
import warnings
warnings.filterwarnings("ignore")

def check_vae_ldm_compatibility():
    """æ£€æŸ¥VAEä¸LDMå…¼å®¹æ€§"""
    print("ğŸ” VAEä¸LDMå…¼å®¹æ€§æ£€æŸ¥")
    print("=" * 60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    compatibility_issues = []
    
    # 1. åˆ›å»ºVAE (è®­ç»ƒé…ç½®)
    print(f"\n1ï¸âƒ£ åˆ›å»ºVAE (è®­ç»ƒé…ç½®):")
    try:
        from diffusers import AutoencoderKL
        
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256, 512],
            latent_channels=4,
            sample_size=128,
            layers_per_block=1,
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        ).to(device)
        
        print(f"   âœ… VAEåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•VAEå‹ç¼©
        test_input = torch.randn(1, 3, 128, 128).to(device)
        with torch.no_grad():
            latent = vae.encode(test_input).latent_dist.sample()
            reconstructed = vae.decode(latent).sample
        
        vae_latent_shape = latent.shape
        vae_compression = test_input.shape[-1] // latent.shape[-1]
        
        print(f"   ğŸ“ VAEè¾“å…¥: {test_input.shape}")
        print(f"   ğŸ¯ VAEæ½œåœ¨: {vae_latent_shape}")
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {vae_compression}å€")
        
    except Exception as e:
        print(f"   âŒ VAEåˆ›å»ºå¤±è´¥: {e}")
        compatibility_issues.append("VAEåˆ›å»ºå¤±è´¥")
        return False
    
    # 2. åˆ›å»ºUNet (LDMé…ç½®)
    print(f"\n2ï¸âƒ£ åˆ›å»ºUNet (LDMé…ç½®):")
    try:
        from diffusers import UNet2DConditionModel
        
        unet = UNet2DConditionModel(
            sample_size=32,  # åº”è¯¥åŒ¹é…VAEæ½œåœ¨ç©ºé—´
            in_channels=4,
            out_channels=4,
            layers_per_block=2,
            block_out_channels=(320, 640, 1280, 1280),
            down_block_types=(
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
            ),
            cross_attention_dim=768,
        ).to(device)
        
        print(f"   âœ… UNetåˆ›å»ºæˆåŠŸ")
        print(f"   ğŸ“ UNet sample_size: {unet.config.sample_size}")
        print(f"   ğŸ”— cross_attention_dim: {unet.config.cross_attention_dim}")
        
    except Exception as e:
        print(f"   âŒ UNetåˆ›å»ºå¤±è´¥: {e}")
        compatibility_issues.append("UNetåˆ›å»ºå¤±è´¥")
        return False
    
    # 3. æ£€æŸ¥å°ºå¯¸å…¼å®¹æ€§
    print(f"\n3ï¸âƒ£ æ£€æŸ¥å°ºå¯¸å…¼å®¹æ€§:")
    
    # æ£€æŸ¥VAEæ½œåœ¨ç©ºé—´ä¸UNet sample_size
    expected_latent_size = vae_latent_shape[-1]  # åº”è¯¥æ˜¯32
    unet_sample_size = unet.config.sample_size
    
    if expected_latent_size == unet_sample_size:
        print(f"   âœ… æ½œåœ¨ç©ºé—´å°ºå¯¸åŒ¹é…: VAE={expected_latent_size}, UNet={unet_sample_size}")
    else:
        print(f"   âŒ æ½œåœ¨ç©ºé—´å°ºå¯¸ä¸åŒ¹é…: VAE={expected_latent_size}, UNet={unet_sample_size}")
        compatibility_issues.append(f"æ½œåœ¨ç©ºé—´å°ºå¯¸ä¸åŒ¹é…: VAE={expected_latent_size}, UNet={unet_sample_size}")
    
    # æ£€æŸ¥é€šé“æ•°
    vae_latent_channels = vae_latent_shape[1]  # åº”è¯¥æ˜¯4
    unet_in_channels = unet.config.in_channels
    unet_out_channels = unet.config.out_channels
    
    if vae_latent_channels == unet_in_channels == unet_out_channels:
        print(f"   âœ… é€šé“æ•°åŒ¹é…: VAE={vae_latent_channels}, UNetè¾“å…¥={unet_in_channels}, UNetè¾“å‡º={unet_out_channels}")
    else:
        print(f"   âŒ é€šé“æ•°ä¸åŒ¹é…: VAE={vae_latent_channels}, UNetè¾“å…¥={unet_in_channels}, UNetè¾“å‡º={unet_out_channels}")
        compatibility_issues.append("é€šé“æ•°ä¸åŒ¹é…")
    
    # 4. æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹
    print(f"\n4ï¸âƒ£ æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹:")
    try:
        from diffusers import DDPMScheduler
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = DDPMScheduler(num_train_timesteps=1000)
        
        # åˆ›å»ºæ¡ä»¶ç¼–ç å™¨
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).parent / "training"))
        from train_diffusion import UserConditionEncoder
        
        condition_encoder = UserConditionEncoder(
            num_users=31,
            embed_dim=768
        ).to(device)
        
        with torch.no_grad():
            # æ¨¡æ‹Ÿå®Œæ•´æµç¨‹
            batch_size = 2
            
            # 1. è¾“å…¥å›¾åƒ
            input_images = torch.randn(batch_size, 3, 128, 128).to(device)
            print(f"   ğŸ“¥ è¾“å…¥å›¾åƒ: {input_images.shape}")
            
            # 2. VAEç¼–ç 
            posterior = vae.encode(input_images).latent_dist
            latents = posterior.sample()
            latents = latents * vae.config.scaling_factor
            print(f"   ğŸ”„ VAEç¼–ç : {input_images.shape} â†’ {latents.shape}")
            
            # 3. æ·»åŠ å™ªå£°
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, 1000, (batch_size,)).to(device)
            noisy_latents = scheduler.add_noise(latents, noise, timesteps)
            print(f"   ğŸ”Š æ·»åŠ å™ªå£°: {latents.shape} â†’ {noisy_latents.shape}")
            
            # 4. æ¡ä»¶ç¼–ç 
            user_ids = torch.tensor([1, 15]).to(device)
            encoder_hidden_states = condition_encoder(user_ids)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(1)
            print(f"   ğŸ­ æ¡ä»¶ç¼–ç : {user_ids.shape} â†’ {encoder_hidden_states.shape}")
            
            # 5. UNeté¢„æµ‹
            model_pred = unet(
                noisy_latents,
                timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )[0]
            print(f"   ğŸ¯ UNeté¢„æµ‹: {noisy_latents.shape} â†’ {model_pred.shape}")
            
            # 6. VAEè§£ç 
            clean_latents = latents / vae.config.scaling_factor
            reconstructed = vae.decode(clean_latents).sample
            print(f"   ğŸ”„ VAEè§£ç : {latents.shape} â†’ {reconstructed.shape}")
            
            # éªŒè¯æ‰€æœ‰å½¢çŠ¶
            shape_checks = [
                ("VAEæ½œåœ¨ç©ºé—´", latents.shape, (batch_size, 4, 32, 32)),
                ("UNetè¾“å…¥", noisy_latents.shape, latents.shape),
                ("UNetè¾“å‡º", model_pred.shape, latents.shape),
                ("é‡å»ºå›¾åƒ", reconstructed.shape, input_images.shape),
                ("æ¡ä»¶åµŒå…¥", encoder_hidden_states.shape, (batch_size, 1, 768)),
            ]
            
            all_shapes_correct = True
            for name, actual, expected in shape_checks:
                if actual == expected:
                    print(f"   âœ… {name}: {actual}")
                else:
                    print(f"   âŒ {name}: {actual}, æœŸæœ›: {expected}")
                    compatibility_issues.append(f"{name}å½¢çŠ¶ä¸åŒ¹é…")
                    all_shapes_correct = False
            
            if all_shapes_correct:
                print(f"   ğŸ‰ æ‰€æœ‰å½¢çŠ¶æ£€æŸ¥é€šè¿‡ï¼")
            
    except Exception as e:
        print(f"   âŒ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        compatibility_issues.append(f"å®Œæ•´å·¥ä½œæµç¨‹å¤±è´¥: {e}")
    
    # 5. æ£€æŸ¥é…ç½®å‚æ•°ä¸€è‡´æ€§
    print(f"\n5ï¸âƒ£ æ£€æŸ¥é…ç½®å‚æ•°ä¸€è‡´æ€§:")
    
    config_checks = [
        ("åˆ†è¾¨ç‡", 128, "VAEå’ŒLDMéƒ½åº”ä½¿ç”¨128Ã—128"),
        ("æ½œåœ¨ç©ºé—´å°ºå¯¸", 32, "VAEè¾“å‡ºå’ŒUNet sample_size"),
        ("æ½œåœ¨é€šé“æ•°", 4, "VAEå’ŒUNeté€šé“æ•°"),
        ("æ¡ä»¶ç»´åº¦", 768, "UNet cross_attention_dim"),
        ("å‹ç¼©æ¯”", 4, "128Ã·32=4å€å‹ç¼©"),
    ]
    
    for name, expected, description in config_checks:
        print(f"   âœ… {name}: {expected} ({description})")
    
    # 6. æ€»ç»“
    print(f"\nğŸ“Š å…¼å®¹æ€§æ£€æŸ¥æ€»ç»“:")
    if not compatibility_issues:
        print(f"   ğŸ‰ æ‰€æœ‰å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡ï¼")
        print(f"   âœ… VAEä¸LDMé…ç½®å®Œå…¨å…¼å®¹")
        print(f"   âœ… å¯ä»¥å®‰å…¨å¼€å§‹è®­ç»ƒ")
        
        print(f"\nğŸ“‹ ç¡®è®¤çš„é…ç½®:")
        print(f"   - è¾“å…¥åˆ†è¾¨ç‡: 128Ã—128")
        print(f"   - VAEæ½œåœ¨ç©ºé—´: 32Ã—32Ã—4")
        print(f"   - UNet sample_size: 32")
        print(f"   - å‹ç¼©æ¯”: 4å€")
        print(f"   - æ¡ä»¶ç»´åº¦: 768")
        
        return True
    else:
        print(f"   âŒ å‘ç° {len(compatibility_issues)} ä¸ªå…¼å®¹æ€§é—®é¢˜:")
        for i, issue in enumerate(compatibility_issues, 1):
            print(f"      {i}. {issue}")
        return False

def show_compatible_training_commands():
    """æ˜¾ç¤ºå…¼å®¹çš„è®­ç»ƒå‘½ä»¤"""
    print(f"\nğŸš€ å…¼å®¹çš„è®­ç»ƒå‘½ä»¤:")
    
    print(f"\n1ï¸âƒ£ VAEè®­ç»ƒ:")
    print(f"python training/train_vae.py \\")
    print(f"    --resolution 128 \\")
    print(f"    --down_block_types \"DownEncoderBlock2D,DownEncoderBlock2D,DownEncoderBlock2D\" \\")
    print(f"    --sample_size 128")
    
    print(f"\n2ï¸âƒ£ LDMè®­ç»ƒ:")
    print(f"python training/train_diffusion.py \\")
    print(f"    --resolution 128 \\")
    print(f"    --vae_path \"outputs/vae/final_model\"")

def main():
    """ä¸»å‡½æ•°"""
    success = check_vae_ldm_compatibility()
    
    if success:
        show_compatible_training_commands()
        print(f"\nâœ… å…¼å®¹æ€§æ£€æŸ¥å®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    else:
        print(f"\nâŒ å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥ï¼Œè¯·ä¿®å¤é—®é¢˜åé‡è¯•ã€‚")

if __name__ == "__main__":
    main()
