#!/usr/bin/env python3
"""
æµ‹è¯•æ£€æŸ¥ç‚¹æ¸…ç†é€»è¾‘
éªŒè¯åªä¿ç•™æœ€æ–°1ä¸ªæ£€æŸ¥ç‚¹çš„åŠŸèƒ½
"""

import torch
import torch.nn as nn
from pathlib import Path
import tempfile
import shutil

class DummyModel(nn.Module):
    """ç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.linear(x)

def save_checkpoint(model, optimizer, epoch, step, output_dir):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ (åªä¿ç•™æœ€æ–°çš„1ä¸ª) - æµ‹è¯•ç‰ˆæœ¬"""
    checkpoint_dir = Path(output_dir) / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)

    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œéœ€è¦é€šè¿‡.moduleè®¿é—®åŸå§‹æ¨¡å‹
    model_to_save = model.module if hasattr(model, 'module') else model

    checkpoint = {
        'model_state_dict': model_to_save.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'step': step,
    }

    # æ–°æ£€æŸ¥ç‚¹æ–‡ä»¶å
    new_checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pt"
    
    # åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ (ä¿ç•™æœ€æ–°1ä¸ª)
    try:
        for old_checkpoint in checkpoint_dir.glob("checkpoint_epoch_*.pt"):
            if old_checkpoint != new_checkpoint_path:
                old_checkpoint.unlink()
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_checkpoint.name}")
    except Exception as e:
        print(f"âš ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜æ–°æ£€æŸ¥ç‚¹
    torch.save(checkpoint, new_checkpoint_path)
    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {new_checkpoint_path.name}")
    
    # è¿”å›å½“å‰æ£€æŸ¥ç‚¹æ•°é‡ç”¨äºæµ‹è¯•
    return len(list(checkpoint_dir.glob("checkpoint_epoch_*.pt")))

def test_checkpoint_cleanup():
    """æµ‹è¯•æ£€æŸ¥ç‚¹æ¸…ç†åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ£€æŸ¥ç‚¹æ¸…ç†é€»è¾‘")
    print("=" * 50)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œä¼˜åŒ–å™¨
        model = DummyModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # æ¨¡æ‹Ÿå¤šä¸ªepochçš„ä¿å­˜
        checkpoint_counts = []
        for epoch in range(5):
            count = save_checkpoint(model, optimizer, epoch, epoch * 100, temp_dir)
            checkpoint_counts.append(count)
            print(f"Epoch {epoch+1}: æ£€æŸ¥ç‚¹æ•°é‡ = {count}")
        
        # éªŒè¯ç»“æœ
        print(f"\nğŸ“Š æ£€æŸ¥ç‚¹æ•°é‡å˜åŒ–: {checkpoint_counts}")
        
        # æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
        checkpoint_dir = Path(temp_dir) / "checkpoints"
        final_files = list(checkpoint_dir.glob("checkpoint_epoch_*.pt"))
        print(f"ğŸ¯ æœ€ç»ˆæ£€æŸ¥ç‚¹æ–‡ä»¶: {[f.name for f in final_files]}")
        
        # éªŒè¯åªæœ‰1ä¸ªæ–‡ä»¶
        if len(final_files) == 1:
            print("âœ… æµ‹è¯•é€šè¿‡: åªä¿ç•™äº†æœ€æ–°çš„1ä¸ªæ£€æŸ¥ç‚¹")
            print(f"âœ… æœ€æ–°æ£€æŸ¥ç‚¹: {final_files[0].name}")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: åº”è¯¥åªæœ‰1ä¸ªæ£€æŸ¥ç‚¹ï¼Œå®é™…æœ‰{len(final_files)}ä¸ª")
        
        # éªŒè¯æ˜¯æœ€æ–°çš„epoch
        if final_files and "epoch_5" in final_files[0].name:
            print("âœ… éªŒè¯é€šè¿‡: ä¿ç•™çš„æ˜¯æœ€æ–°çš„epoch")
        else:
            print("âŒ éªŒè¯å¤±è´¥: ä¿ç•™çš„ä¸æ˜¯æœ€æ–°çš„epoch")

def test_disk_space_savings():
    """æµ‹è¯•ç£ç›˜ç©ºé—´èŠ‚çœæ•ˆæœ"""
    print(f"\nğŸ’¾ ç£ç›˜ç©ºé—´èŠ‚çœåˆ†æ")
    print("=" * 50)
    
    # ä¼°ç®—æ£€æŸ¥ç‚¹æ–‡ä»¶å¤§å°
    model = DummyModel()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # åˆ›å»ºä¸€ä¸ªæ£€æŸ¥ç‚¹æ¥ä¼°ç®—å¤§å°
    with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as temp_file:
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': 0,
            'step': 0,
        }
        torch.save(checkpoint, temp_file.name)
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = Path(temp_file.name).stat().st_size
        print(f"ğŸ“ å•ä¸ªæ£€æŸ¥ç‚¹å¤§å°: {file_size / 1024:.2f} KB")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        Path(temp_file.name).unlink()
    
    # è®¡ç®—èŠ‚çœçš„ç©ºé—´
    epochs = 50  # å‡è®¾è®­ç»ƒ50ä¸ªepoch
    save_interval = 5  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡
    total_checkpoints = epochs // save_interval
    
    old_total_size = total_checkpoints * file_size
    new_total_size = 1 * file_size  # åªä¿ç•™1ä¸ª
    saved_space = old_total_size - new_total_size
    
    print(f"ğŸ“Š ç©ºé—´ä½¿ç”¨å¯¹æ¯” (è®­ç»ƒ{epochs}ä¸ªepoch):")
    print(f"   æ—§æ–¹æ¡ˆ: {total_checkpoints}ä¸ªæ£€æŸ¥ç‚¹ = {old_total_size / 1024 / 1024:.2f} MB")
    print(f"   æ–°æ–¹æ¡ˆ: 1ä¸ªæ£€æŸ¥ç‚¹ = {new_total_size / 1024 / 1024:.2f} MB")
    print(f"   ğŸ’° èŠ‚çœç©ºé—´: {saved_space / 1024 / 1024:.2f} MB ({saved_space / old_total_size * 100:.1f}%)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ£€æŸ¥ç‚¹æ¸…ç†åŠŸèƒ½æµ‹è¯•")
    print("ğŸ¯ ç›®æ ‡: åªä¿ç•™æœ€æ–°1ä¸ªæ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´")
    print()
    
    # è¿è¡Œæµ‹è¯•
    test_checkpoint_cleanup()
    test_disk_space_savings()
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"âœ… æ–°çš„ä¿å­˜é€»è¾‘å·²éªŒè¯ï¼Œå¯ä»¥å¤§å¹…èŠ‚çœç£ç›˜ç©ºé—´")

if __name__ == "__main__":
    main()
