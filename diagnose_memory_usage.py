#!/usr/bin/env python3
"""
GPUå†…å­˜ä½¿ç”¨è¯Šæ–­å·¥å…·
æ‰¾å‡ºå†…å­˜å ç”¨çš„å…·ä½“åŸå› 
"""

import torch
import gc
import psutil
import os

def check_system_memory():
    """æ£€æŸ¥ç³»ç»Ÿå†…å­˜"""
    print("ğŸ’» ç³»ç»Ÿå†…å­˜çŠ¶æ€:")
    memory = psutil.virtual_memory()
    print(f"   æ€»å†…å­˜: {memory.total / 1024**3:.2f} GB")
    print(f"   å·²ä½¿ç”¨: {memory.used / 1024**3:.2f} GB ({memory.percent:.1f}%)")
    print(f"   å¯ç”¨: {memory.available / 1024**3:.2f} GB")

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜"""
    print("\nğŸ® GPUå†…å­˜çŠ¶æ€:")
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            reserved = torch.cuda.memory_reserved(i) / 1024**3
            total = props.total_memory / 1024**3
            
            print(f"   GPU {i} ({props.name}):")
            print(f"      æ€»å†…å­˜: {total:.2f} GB")
            print(f"      å·²åˆ†é…: {allocated:.2f} GB ({allocated/total*100:.1f}%)")
            print(f"      å·²ä¿ç•™: {reserved:.2f} GB ({reserved/total*100:.1f}%)")
            print(f"      å¯ç”¨: {total-reserved:.2f} GB")
    else:
        print("   âŒ CUDAä¸å¯ç”¨")

def check_process_memory():
    """æ£€æŸ¥å½“å‰è¿›ç¨‹å†…å­˜"""
    print("\nğŸ” å½“å‰è¿›ç¨‹å†…å­˜:")
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    print(f"   RSS: {memory_info.rss / 1024**3:.2f} GB")
    print(f"   VMS: {memory_info.vms / 1024**3:.2f} GB")

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½çš„å†…å­˜ä½¿ç”¨"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½å†…å­˜ä½¿ç”¨:")
    
    # æ¸…ç†åˆå§‹å†…å­˜
    gc.collect()
    torch.cuda.empty_cache()
    
    print("1ï¸âƒ£ åˆå§‹çŠ¶æ€:")
    check_gpu_memory()
    
    try:
        # æµ‹è¯•VAEåŠ è½½
        print("\n2ï¸âƒ£ åŠ è½½VAE...")
        from diffusers import AutoencoderKL
        
        # æ¨¡æ‹ŸVAEé…ç½®
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256, 512],
            latent_channels=4,
            sample_size=128,
        )
        
        if torch.cuda.is_available():
            vae = vae.cuda()
        
        check_gpu_memory()
        
        # æµ‹è¯•UNetåŠ è½½
        print("\n3ï¸âƒ£ åŠ è½½UNet...")
        from diffusers import UNet2DConditionModel
        
        unet = UNet2DConditionModel(
            sample_size=32,
            in_channels=4,
            out_channels=4,
            layers_per_block=2,
            block_out_channels=(128, 256, 512, 512),  # ä¸­å‹é…ç½®
            down_block_types=(
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
            ),
            cross_attention_dim=512,  # ä¸ä¸­å‹é…ç½®åŒ¹é…
            attention_head_dim=8,
            use_linear_projection=True,
        )
        
        if torch.cuda.is_available():
            unet = unet.cuda()
        
        check_gpu_memory()
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("\n4ï¸âƒ£ æµ‹è¯•å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            # æ¨¡æ‹Ÿè¾“å…¥
            if torch.cuda.is_available():
                test_input = torch.randn(1, 3, 128, 128).cuda()
                
                # VAEç¼–ç 
                latents = vae.encode(test_input).latent_dist.sample()
                print(f"   VAEç¼–ç å: {latents.shape}")
                check_gpu_memory()
                
                # UNetå‰å‘
                timesteps = torch.randint(0, 1000, (1,)).cuda()
                conditions = torch.randn(1, 1, 512).cuda()  # åŒ¹é…æ–°çš„cross_attention_dim
                
                noise_pred = unet(latents, timesteps, encoder_hidden_states=conditions, return_dict=False)[0]
                print(f"   UNeté¢„æµ‹å: {noise_pred.shape}")
                check_gpu_memory()
                
                # VAEè§£ç 
                reconstructed = vae.decode(latents).sample
                print(f"   VAEè§£ç å: {reconstructed.shape}")
                check_gpu_memory()
        
        # æ¸…ç†æµ‹è¯•
        print("\n5ï¸âƒ£ æ¸…ç†æµ‹è¯•æ¨¡å‹...")
        del vae, unet
        if 'latents' in locals():
            del latents, noise_pred, reconstructed, test_input
        gc.collect()
        torch.cuda.empty_cache()
        check_gpu_memory()
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        check_gpu_memory()

def analyze_memory_usage():
    """åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼"""
    print("\nğŸ“Š å†…å­˜ä½¿ç”¨åˆ†æ:")
    
    if torch.cuda.is_available():
        # è·å–è¯¦ç»†çš„å†…å­˜ç»Ÿè®¡
        memory_stats = torch.cuda.memory_stats()
        
        print("è¯¦ç»†å†…å­˜ç»Ÿè®¡:")
        for key, value in memory_stats.items():
            if 'bytes' in key:
                print(f"   {key}: {value / 1024**3:.2f} GB")
            else:
                print(f"   {key}: {value}")

def suggest_optimizations():
    """å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ"""
    print("\nğŸ’¡ å†…å­˜ä¼˜åŒ–å»ºè®®:")
    
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"åŸºäº {total_memory:.1f}GB GPU çš„ä¼˜åŒ–å»ºè®®:")
        
        if total_memory < 12:
            print("   ğŸ”´ ä½å†…å­˜GPU (<12GB):")
            print("      - batch_size = 1")
            print("      - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
            print("      - ä½¿ç”¨CPUå¸è½½")
            print("      - å‡å°‘é‡‡æ ·æ­¥æ•°")
        elif total_memory < 20:
            print("   ğŸŸ¡ ä¸­ç­‰å†…å­˜GPU (12-20GB):")
            print("      - batch_size = 1-2")
            print("      - å¯ç”¨æ··åˆç²¾åº¦")
            print("      - å®šæœŸæ¸…ç†å†…å­˜")
        else:
            print("   ğŸŸ¢ é«˜å†…å­˜GPU (>20GB):")
            print("      - batch_size = 2-4")
            print("      - æ­£å¸¸è®­ç»ƒé…ç½®")
    
    print("\nğŸ”§ é€šç”¨ä¼˜åŒ–:")
    print("   - è®¾ç½® PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128")
    print("   - ä½¿ç”¨ torch.cuda.empty_cache() å®šæœŸæ¸…ç†")
    print("   - å¯ç”¨ torch.backends.cudnn.benchmark = True")
    print("   - å‡å°‘sample_intervalé¢‘ç‡")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” GPUå†…å­˜ä½¿ç”¨è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
    check_system_memory()
    check_gpu_memory()
    check_process_memory()
    
    # 2. æµ‹è¯•æ¨¡å‹åŠ è½½
    test_model_loading()
    
    # 3. åˆ†æå†…å­˜ä½¿ç”¨
    analyze_memory_usage()
    
    # 4. å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ
    suggest_optimizations()
    
    print(f"\nâœ… è¯Šæ–­å®Œæˆ")

if __name__ == "__main__":
    main()
