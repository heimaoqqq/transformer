#!/usr/bin/env python3
"""
è°ƒè¯•ç‰ˆè®­ç»ƒè„šæœ¬ - æ‰¾å‡ºè¿›åº¦æ¡ä¸åŠ¨çš„åŸå› 
"""

import os
import sys
import subprocess
import torch
import time
from pathlib import Path

def setup_debug_environment():
    """è®¾ç½®è°ƒè¯•ç¯å¢ƒ"""
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'
    os.environ['PYTHONUNBUFFERED'] = '1'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # å¯ç”¨åŒæ­¥æ‰§è¡Œä¾¿äºè°ƒè¯•

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("ğŸ” æµ‹è¯•æ•°æ®åŠ è½½...")
    
    try:
        sys.path.insert(0, '/kaggle/working/VAE')
        from utils.data_loader import MicroDopplerDataset
        from torch.utils.data import DataLoader
        
        print("   âœ… å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MicroDopplerDataset(
            data_dir="/kaggle/input/dataset",
            resolution=256,
            split="train"
        )
        
        print(f"   âœ… æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=2,  # å°æ‰¹æ¬¡æµ‹è¯•
            shuffle=False,
            num_workers=0,  # å•çº¿ç¨‹
            pin_memory=True
        )
        
        print(f"   âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸï¼Œæ‰¹æ¬¡æ•°: {len(dataloader)}")
        
        # æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        print("   ğŸ”„ æµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªæ‰¹æ¬¡...")
        start_time = time.time()
        
        for i, batch in enumerate(dataloader):
            load_time = time.time() - start_time
            print(f"   âœ… æ‰¹æ¬¡ {i} åŠ è½½æˆåŠŸ ({load_time:.2f}s)")
            print(f"      å›¾åƒå½¢çŠ¶: {batch['image'].shape}")
            print(f"      å›¾åƒèŒƒå›´: [{batch['image'].min():.3f}, {batch['image'].max():.3f}]")
            
            if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
                break
            
            start_time = time.time()
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_forward():
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    
    try:
        from diffusers import AutoencoderKL
        
        print("   ğŸ”„ åˆ›å»ºVAEæ¨¡å‹...")
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=[
                "DownEncoderBlock2D",
                "DownEncoderBlock2D",
                "DownEncoderBlock2D",
                "DownEncoderBlock2D",
            ],
            up_block_types=[
                "UpDecoderBlock2D",
                "UpDecoderBlock2D", 
                "UpDecoderBlock2D",
                "UpDecoderBlock2D",
            ],
            block_out_channels=[128, 256, 512, 512],
            latent_channels=4,
            sample_size=256,
        )
        
        print("   âœ… VAEæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # ç§»åŠ¨åˆ°GPU
        device = torch.device("cuda:0")
        vae = vae.to(device)
        print(f"   âœ… æ¨¡å‹ç§»åŠ¨åˆ° {device}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        test_images = torch.randn(batch_size, 3, 256, 256).to(device)
        print(f"   âœ… æµ‹è¯•æ•°æ®åˆ›å»º: {test_images.shape}")
        
        # æµ‹è¯•ç¼–ç 
        print("   ğŸ”„ æµ‹è¯•VAEç¼–ç ...")
        start_time = time.time()
        
        with torch.no_grad():
            posterior = vae.encode(test_images).latent_dist
            latents = posterior.sample()
        
        encode_time = time.time() - start_time
        print(f"   âœ… ç¼–ç æˆåŠŸ ({encode_time:.2f}s): {latents.shape}")
        
        # æµ‹è¯•è§£ç 
        print("   ğŸ”„ æµ‹è¯•VAEè§£ç ...")
        start_time = time.time()
        
        with torch.no_grad():
            reconstruction = vae.decode(latents).sample
        
        decode_time = time.time() - start_time
        print(f"   âœ… è§£ç æˆåŠŸ ({decode_time:.2f}s): {reconstruction.shape}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_accelerate():
    """æµ‹è¯•Accelerate"""
    print("\nğŸ” æµ‹è¯•Accelerate...")
    
    try:
        from accelerate import Accelerator
        
        accelerator = Accelerator(
            gradient_accumulation_steps=4,
            mixed_precision="fp16"
        )
        
        print(f"   âœ… Acceleratoråˆ›å»ºæˆåŠŸ")
        print(f"      è®¾å¤‡: {accelerator.device}")
        print(f"      è¿›ç¨‹æ•°: {accelerator.num_processes}")
        print(f"      åˆ†å¸ƒå¼ç±»å‹: {accelerator.distributed_type}")
        print(f"      æ˜¯å¦ä¸»è¿›ç¨‹: {accelerator.is_main_process}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Accelerateæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def launch_debug_training():
    """å¯åŠ¨è°ƒè¯•è®­ç»ƒ"""
    
    setup_debug_environment()
    
    print("ğŸš€ è°ƒè¯•è®­ç»ƒå¯åŠ¨å™¨")
    print("=" * 50)
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    if torch.cuda.is_available():
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            print(f"   GPU {i}: {props.name} - {props.total_memory / 1024**3:.1f} GB")
    
    # é€æ­¥æµ‹è¯•
    print("\n" + "="*50)
    print("å¼€å§‹é€æ­¥è°ƒè¯•...")
    
    # 1. æµ‹è¯•æ•°æ®åŠ è½½
    if not test_data_loading():
        print("âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥")
        return False
    
    # 2. æµ‹è¯•æ¨¡å‹
    if not test_model_forward():
        print("âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥")
        return False
    
    # 3. æµ‹è¯•Accelerate
    if not test_accelerate():
        print("âŒ Accelerateæµ‹è¯•å¤±è´¥")
        return False
    
    print("\n" + "="*50)
    print("âœ… æ‰€æœ‰ç»„ä»¶æµ‹è¯•é€šè¿‡!")
    print("ğŸ’¡ é—®é¢˜å¯èƒ½åœ¨è®­ç»ƒå¾ªç¯çš„å…·ä½“å®ç°ä¸­")
    
    # å¯åŠ¨ç®€åŒ–è®­ç»ƒæµ‹è¯•
    print("\nğŸ”„ å¯åŠ¨ç®€åŒ–è®­ç»ƒæµ‹è¯•...")
    
    cmd = [
        "python", "-u",
        "/kaggle/working/VAE/simple_train.py"
    ]
    
    try:
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1
        )
        
        # å®æ—¶è¾“å‡º
        for line in process.stdout:
            print(line.rstrip())
        
        return_code = process.wait()
        
        if return_code == 0:
            print("\nâœ… ç®€åŒ–è®­ç»ƒæµ‹è¯•æˆåŠŸ!")
            return True
        else:
            print(f"\nâŒ ç®€åŒ–è®­ç»ƒæµ‹è¯•å¤±è´¥ (é€€å‡ºç : {return_code})")
            return False
            
    except Exception as e:
        print(f"\nâŒ ç®€åŒ–è®­ç»ƒå¯åŠ¨å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    success = launch_debug_training()
    
    if success:
        print("\nğŸ‰ è°ƒè¯•å®Œæˆ!")
        print("ğŸ’¡ å¯ä»¥å°è¯•è¿è¡Œå®Œæ•´è®­ç»ƒ")
    else:
        print("\nâŒ è°ƒè¯•å‘ç°é—®é¢˜!")
        print("ğŸ’¡ è¯·æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()
