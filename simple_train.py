#!/usr/bin/env python3
"""
ç®€åŒ–çš„è®­ç»ƒè„šæœ¬ - é€æ­¥è°ƒè¯•
"""

import os
import sys
import torch
import torch.nn as nn
from pathlib import Path
from tqdm import tqdm
import time

# è®¾ç½®æ— ç¼“å†²è¾“å‡º
os.environ['PYTHONUNBUFFERED'] = '1'

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir
sys.path.insert(0, str(project_root))

def simple_vae_training():
    """ç®€åŒ–çš„VAEè®­ç»ƒ"""
    
    print("ğŸš€ å¼€å§‹ç®€åŒ–VAEè®­ç»ƒ")
    print("=" * 50)
    
    # 1. æ£€æŸ¥GPU
    print("1ï¸âƒ£ æ£€æŸ¥GPU...")
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    device = torch.device("cuda:0")
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n2ï¸âƒ£ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        from utils.data_loader import MicroDopplerDataset
        from torch.utils.data import DataLoader
        
        dataset = MicroDopplerDataset(
            data_dir="/kaggle/input/dataset",
            resolution=128,  # ä½¿ç”¨æ›´å°çš„åˆ†è¾¨ç‡
            split="train"
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=2,  # å¾ˆå°çš„æ‰¹æ¬¡
            shuffle=True,
            num_workers=0,  # å•çº¿ç¨‹
            pin_memory=False  # ç¦ç”¨pin_memory
        )
        
        print(f"âœ… æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"âœ… æ‰¹æ¬¡æ•°: {len(dataloader)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # 3. åˆ›å»ºæ¨¡å‹
    print("\n3ï¸âƒ£ åˆ›å»ºæ¨¡å‹...")
    try:
        from diffusers import AutoencoderKL
        
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256],
            latent_channels=4,
            sample_size=128,
        )
        
        vae = vae.to(device)
        vae.train()
        
        print("âœ… VAEæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # 4. åˆ›å»ºä¼˜åŒ–å™¨
    print("\n4ï¸âƒ£ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = torch.optim.AdamW(vae.parameters(), lr=1e-4)
    mse_loss = nn.MSELoss()
    
    # 5. æµ‹è¯•è®­ç»ƒå¾ªç¯
    print("\n5ï¸âƒ£ å¼€å§‹è®­ç»ƒå¾ªç¯...")

    try:
        epoch = 1
        max_steps = 5  # åªè®­ç»ƒ5æ­¥

        print(f"   ğŸ“Š æ•°æ®åŠ è½½å™¨é•¿åº¦: {len(dataloader)}")
        print(f"   ğŸ“Š æœ€å¤§æ­¥æ•°: {max_steps}")

        # å…ˆæµ‹è¯•æ•°æ®è¿­ä»£å™¨
        print("   ğŸ”„ æµ‹è¯•æ•°æ®è¿­ä»£å™¨...")
        data_iter = iter(dataloader)

        for step in range(max_steps):
            print(f"\n   ğŸ”„ æ­¥éª¤ {step+1}/{max_steps}")

            # è·å–æ•°æ®
            print("      ğŸ“¥ è·å–æ‰¹æ¬¡æ•°æ®...")
            try:
                batch = next(data_iter)
                print(f"      âœ… æ•°æ®è·å–æˆåŠŸ: {batch['image'].shape}")
            except StopIteration:
                print("      âš ï¸  æ•°æ®è¿­ä»£å™¨ç»“æŸ")
                break
            except Exception as e:
                print(f"      âŒ æ•°æ®è·å–å¤±è´¥: {e}")
                break

            # ç§»åŠ¨åˆ°GPU
            print("      ğŸ“¤ ç§»åŠ¨æ•°æ®åˆ°GPU...")
            images = batch['image'].to(device)
            print(f"      âœ… å›¾åƒç§»åŠ¨æˆåŠŸ: {images.shape}")

            # å‰å‘ä¼ æ’­
            print("      ğŸ”„ å‰å‘ä¼ æ’­...")
            start_time = time.time()

            try:
                print("         ğŸ”„ VAEç¼–ç ...")
                posterior = vae.encode(images).latent_dist
                latents = posterior.sample()
                print(f"         âœ… ç¼–ç å®Œæˆ: {latents.shape}")

                print("         ğŸ”„ VAEè§£ç ...")
                reconstruction = vae.decode(latents).sample
                print(f"         âœ… è§£ç å®Œæˆ: {reconstruction.shape}")

                forward_time = time.time() - start_time
                print(f"      âœ… å‰å‘ä¼ æ’­å®Œæˆ ({forward_time:.2f}s)")

            except Exception as e:
                print(f"      âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False

            # è®¡ç®—æŸå¤±
            print("      ğŸ“Š è®¡ç®—æŸå¤±...")
            try:
                recon_loss = mse_loss(reconstruction, images)
                kl_loss = posterior.kl().mean()
                total_loss = recon_loss + 1e-6 * kl_loss

                print(f"      âœ… æŸå¤±è®¡ç®—å®Œæˆ: {total_loss.item():.4f}")

            except Exception as e:
                print(f"      âŒ æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                return False

            # åå‘ä¼ æ’­
            print("      ğŸ”„ åå‘ä¼ æ’­...")
            try:
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()

                print("      âœ… åå‘ä¼ æ’­å®Œæˆ")

            except Exception as e:
                print(f"      âŒ åå‘ä¼ æ’­å¤±è´¥: {e}")
                return False

            # æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
            print(f"      ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ")
        
        print("\nğŸ‰ ç®€åŒ–è®­ç»ƒå®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¾ªç¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    success = simple_vae_training()
    
    if success:
        print("\nâœ… ç®€åŒ–è®­ç»ƒæˆåŠŸ!")
        print("ğŸ’¡ å¯ä»¥å°è¯•è¿è¡Œå®Œæ•´è®­ç»ƒ")
    else:
        print("\nâŒ ç®€åŒ–è®­ç»ƒå¤±è´¥!")
        print("ğŸ’¡ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()
