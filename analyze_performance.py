#!/usr/bin/env python3
"""
VAEè®­ç»ƒæ€§èƒ½åˆ†æå·¥å…·
"""

import torch
import time
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/kaggle/working/VAE')

def analyze_vae_architecture():
    """åˆ†æVAEæ¶æ„"""
    print("ğŸ” VAEæ¶æ„åˆ†æ")
    print("=" * 50)
    
    from diffusers import AutoencoderKL
    
    # å½“å‰é…ç½®
    vae = AutoencoderKL(
        in_channels=3,
        out_channels=3,
        down_block_types=[
            "DownEncoderBlock2D",
            "DownEncoderBlock2D", 
            "DownEncoderBlock2D",
            "DownEncoderBlock2D"  # 4å±‚ä¸‹é‡‡æ ·
        ],
        up_block_types=[
            "UpDecoderBlock2D",
            "UpDecoderBlock2D",
            "UpDecoderBlock2D",
            "UpDecoderBlock2D"    # 4å±‚ä¸Šé‡‡æ ·
        ],
        block_out_channels=[128, 256, 512, 512],
        latent_channels=4,
        sample_size=256,
        layers_per_block=2,
    )
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in vae.parameters())
    trainable_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f} MB (FP32)")
    
    # åˆ†æå‹ç¼©æ¯”
    input_size = 256 * 256 * 3  # è¾“å…¥åƒç´ æ•°
    latent_size = 32 * 32 * 4   # æ½œåœ¨ç©ºé—´å¤§å° (256/8 = 32)
    compression_ratio = input_size / latent_size
    
    print(f"\nğŸ“ å‹ç¼©åˆ†æ:")
    print(f"   è¾“å…¥å°ºå¯¸: 256Ã—256Ã—3 = {input_size:,} åƒç´ ")
    print(f"   æ½œåœ¨å°ºå¯¸: 32Ã—32Ã—4 = {latent_size:,} åƒç´ ")
    print(f"   å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
    print(f"   ä¸‹é‡‡æ ·å€æ•°: 2^4 = 16å€ (æ¯è¾¹)")
    
    return vae

def benchmark_forward_pass():
    """åŸºå‡†æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("\nğŸš€ å‰å‘ä¼ æ’­åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    device = torch.device("cuda:0")
    vae = analyze_vae_architecture()
    vae = vae.to(device)
    vae.eval()
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
    batch_sizes = [1, 2, 4, 8]
    
    for batch_size in batch_sizes:
        print(f"\nğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_input = torch.randn(batch_size, 3, 256, 256).to(device)
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = vae.encode(test_input).latent_dist.sample()
        
        torch.cuda.synchronize()
        
        # æµ‹è¯•ç¼–ç 
        times = []
        for _ in range(10):
            torch.cuda.synchronize()
            start_time = time.time()
            
            with torch.no_grad():
                posterior = vae.encode(test_input).latent_dist
                latents = posterior.sample()
            
            torch.cuda.synchronize()
            times.append(time.time() - start_time)
        
        encode_time = sum(times) / len(times)
        
        # æµ‹è¯•è§£ç 
        times = []
        for _ in range(10):
            torch.cuda.synchronize()
            start_time = time.time()
            
            with torch.no_grad():
                reconstruction = vae.decode(latents).sample
            
            torch.cuda.synchronize()
            times.append(time.time() - start_time)
        
        decode_time = sum(times) / len(times)
        
        total_time = encode_time + decode_time
        throughput = batch_size / total_time
        
        print(f"   ç¼–ç æ—¶é—´: {encode_time*1000:.1f}ms")
        print(f"   è§£ç æ—¶é—´: {decode_time*1000:.1f}ms")
        print(f"   æ€»æ—¶é—´: {total_time*1000:.1f}ms")
        print(f"   ååé‡: {throughput:.1f} æ ·æœ¬/ç§’")
        
        # å†…å­˜ä½¿ç”¨
        memory_used = torch.cuda.max_memory_allocated() / 1024**3
        print(f"   GPUå†…å­˜: {memory_used:.1f} GB")
        
        torch.cuda.empty_cache()

def analyze_dataset_size():
    """åˆ†ææ•°æ®é›†å¤§å°"""
    print("\nğŸ“ æ•°æ®é›†åˆ†æ")
    print("=" * 50)
    
    try:
        from utils.data_loader import MicroDopplerDataset
        
        dataset = MicroDopplerDataset(
            data_dir="/kaggle/input/dataset",
            resolution=256,
            split="train"
        )
        
        dataset_size = len(dataset)
        batch_size = 4  # å½“å‰é…ç½®
        steps_per_epoch = dataset_size // batch_size
        
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   æ•°æ®é›†å¤§å°: {dataset_size:,} æ ·æœ¬")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   æ¯è½®æ­¥æ•°: {steps_per_epoch:,}")
        
        # ä¼°ç®—è®­ç»ƒæ—¶é—´
        estimated_time_per_step = 4.0  # ç§’ (åŸºäºè§‚å¯Ÿ)
        time_per_epoch = steps_per_epoch * estimated_time_per_step
        
        print(f"\nâ±ï¸  æ—¶é—´ä¼°ç®—:")
        print(f"   æ¯æ­¥æ—¶é—´: ~{estimated_time_per_step:.1f}ç§’")
        print(f"   æ¯è½®æ—¶é—´: ~{time_per_epoch/60:.1f}åˆ†é’Ÿ")
        print(f"   40è½®æ€»æ—¶é—´: ~{time_per_epoch*40/3600:.1f}å°æ—¶")
        
        return dataset_size, steps_per_epoch
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ†æå¤±è´¥: {e}")
        return None, None

def suggest_optimizations():
    """å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ"""
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®")
    print("=" * 50)
    
    print("ğŸš€ é€Ÿåº¦ä¼˜åŒ–æ–¹æ¡ˆ:")
    print("   1. å¢åŠ æ‰¹æ¬¡å¤§å° (å¦‚æœå†…å­˜å…è®¸)")
    print("      - å½“å‰: batch_size=4")
    print("      - å»ºè®®: batch_size=6-8")
    print("      - æ•ˆæœ: æå‡30-50%ååé‡")
    
    print("\n   2. å‡å°‘æ¨¡å‹å¤æ‚åº¦")
    print("      - å½“å‰: 4å±‚ä¸‹é‡‡æ · + 2å±‚/å—")
    print("      - å»ºè®®: 3å±‚ä¸‹é‡‡æ · æˆ– 1å±‚/å—")
    print("      - æ•ˆæœ: å‡å°‘50%è®¡ç®—é‡")
    
    print("\n   3. ä¼˜åŒ–æ•°æ®åŠ è½½")
    print("      - å½“å‰: num_workers=1")
    print("      - å»ºè®®: num_workers=2-4")
    print("      - æ•ˆæœ: å‡å°‘æ•°æ®ç­‰å¾…æ—¶é—´")
    
    print("\n   4. ä½¿ç”¨æ›´é«˜æ•ˆçš„ä¼˜åŒ–å™¨")
    print("      - å½“å‰: AdamW")
    print("      - å»ºè®®: AdamW + å­¦ä¹ ç‡è°ƒåº¦")
    print("      - æ•ˆæœ: æ›´å¿«æ”¶æ•›")
    
    print("\nğŸ¯ è´¨é‡vsé€Ÿåº¦æƒè¡¡:")
    print("   - é«˜è´¨é‡: 256Ã—256, 4å±‚, batch_size=4 (å½“å‰)")
    print("   - å¹³è¡¡: 256Ã—256, 3å±‚, batch_size=6")
    print("   - å¿«é€Ÿ: 128Ã—128, 3å±‚, batch_size=8")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” VAEè®­ç»ƒæ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    # åˆ†ææ¶æ„
    vae = analyze_vae_architecture()
    
    # åŸºå‡†æµ‹è¯•
    benchmark_forward_pass()
    
    # åˆ†ææ•°æ®é›†
    analyze_dataset_size()
    
    # ä¼˜åŒ–å»ºè®®
    suggest_optimizations()
    
    print("\n" + "=" * 80)
    print("âœ… æ€§èƒ½åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
