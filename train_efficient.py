#!/usr/bin/env python3
"""
é«˜æ•ˆVAEè®­ç»ƒå¯åŠ¨å™¨
- 3å±‚ä¸‹é‡‡æ · (256â†’32)
- FP16æ··åˆç²¾åº¦
- ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
"""

import os
import sys
import subprocess
import torch
from pathlib import Path

def setup_efficient_environment():
    """è®¾ç½®é«˜æ•ˆè®­ç»ƒç¯å¢ƒ"""
    # ä¼˜åŒ–å†…å­˜åˆ†é…
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
    
    # å¯ç”¨ä¼˜åŒ–
    os.environ['PYTHONUNBUFFERED'] = '1'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    
    # å¯ç”¨cuDNNä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

def launch_efficient_training():
    """å¯åŠ¨é«˜æ•ˆè®­ç»ƒ"""
    
    setup_efficient_environment()
    
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    # æ˜¾ç¤ºGPUå†…å­˜ä¿¡æ¯
    if torch.cuda.is_available():
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            print(f"   GPU {i}: {props.name} - {props.total_memory / 1024**3:.1f} GB")
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()
    
    if gpu_count > 1:
        print("ğŸš€ å¯åŠ¨é«˜æ•ˆåŒGPUè®­ç»ƒ...")
        
        # è®¾ç½®å¤šGPUç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
        os.environ['WORLD_SIZE'] = str(gpu_count)
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        
        cmd = [
            "accelerate", "launch",
            "--config_file", "accelerate_config.yaml",
            "--num_processes", str(gpu_count),
            "training/train_vae.py",
            "--data_dir", "/kaggle/input/dataset",
            "--output_dir", "/kaggle/working/outputs/vae",
            "--batch_size", "12",       # å¤§å¹…å¢åŠ æ‰¹æ¬¡å¤§å°
            "--num_epochs", "30",       # å‡å°‘epochæ•°
            "--learning_rate", "0.0002", # æé«˜å­¦ä¹ ç‡é…åˆå¤§æ‰¹æ¬¡
            "--mixed_precision", "fp16", # ç¡®ä¿FP16
            "--gradient_accumulation_steps", "1", # å‡å°‘æ¢¯åº¦ç´¯ç§¯
            "--kl_weight", "1e-6",
            "--perceptual_weight", "0.0",
            "--freq_weight", "0.05",
            "--resolution", "256",
            "--num_workers", "4",       # å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
            "--save_interval", "5",     # æ›´é¢‘ç¹ä¿å­˜
            "--log_interval", "2",      # æ›´é¢‘ç¹æ—¥å¿—
            "--sample_interval", "50",  # æ›´é¢‘ç¹é‡‡æ ·
            "--experiment_name", "kaggle_vae_efficient"
        ]
        
        print(f"ğŸ“Š é«˜æ•ˆé…ç½®:")
        print(f"   ğŸ—ï¸  æ¶æ„: 3å±‚ä¸‹é‡‡æ · (256â†’128â†’64â†’32)")
        print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: 12 (æ¯GPU 6ä¸ª)")
        print(f"   ğŸ”¢ æ··åˆç²¾åº¦: FP16")
        print(f"   ğŸ§µ æ•°æ®çº¿ç¨‹: 4")
        print(f"   âš¡ æ¢¯åº¦ç´¯ç§¯: 1 (å®æ—¶æ›´æ–°)")
        print(f"   ğŸ“ˆ å­¦ä¹ ç‡: 2e-4")
        
    else:
        print("ğŸš€ å¯åŠ¨é«˜æ•ˆå•GPUè®­ç»ƒ...")
        
        cmd = [
            "python", "-u",
            "training/train_vae.py",
            "--data_dir", "/kaggle/input/dataset",
            "--output_dir", "/kaggle/working/outputs/vae",
            "--batch_size", "8",        # å•GPUå¤§æ‰¹æ¬¡
            "--num_epochs", "30",
            "--learning_rate", "0.0002",
            "--mixed_precision", "fp16",
            "--gradient_accumulation_steps", "2",
            "--kl_weight", "1e-6",
            "--perceptual_weight", "0.0",
            "--freq_weight", "0.05",
            "--resolution", "256",
            "--num_workers", "4",
            "--save_interval", "5",
            "--log_interval", "2",
            "--sample_interval", "50",
            "--experiment_name", "kaggle_vae_efficient"
        ]
        
        print(f"ğŸ“Š é«˜æ•ˆé…ç½®:")
        print(f"   ğŸ—ï¸  æ¶æ„: 3å±‚ä¸‹é‡‡æ ·")
        print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: 8")
        print(f"   ğŸ”¢ æ··åˆç²¾åº¦: FP16")
        print(f"   ğŸ§µ æ•°æ®çº¿ç¨‹: 4")
    
    print(f"\nCommand: {' '.join(cmd)}")
    print("=" * 80)
    
    try:
        # å¯åŠ¨è®­ç»ƒ
        process = subprocess.Popen(
            cmd,
            stdout=None,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=0
        )
        
        return_code = process.wait()
        
        if return_code == 0:
            print("\nâœ… é«˜æ•ˆè®­ç»ƒå®Œæˆ!")
            return True
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥ (é€€å‡ºç : {return_code})")
            return False
            
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        process.terminate()
        return False
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¯åŠ¨å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é«˜æ•ˆVAEè®­ç»ƒ (3å±‚ä¸‹é‡‡æ · + FP16)")
    print("=" * 50)
    
    print("ğŸ¯ æ•ˆç‡ä¼˜åŒ–:")
    print("   âœ… 3å±‚ä¸‹é‡‡æ · (vs 4å±‚) â†’ 50%å‚æ•°å‡å°‘")
    print("   âœ… FP16æ··åˆç²¾åº¦ â†’ 50%å†…å­˜èŠ‚çœ")
    print("   âœ… å¤§æ‰¹æ¬¡è®­ç»ƒ (4â†’12) â†’ 3å€ååé‡")
    print("   âœ… å¤šçº¿ç¨‹æ•°æ®åŠ è½½ (1â†’4) â†’ 4å€æ•°æ®é€Ÿåº¦")
    print("   âœ… å®æ—¶æ¢¯åº¦æ›´æ–° (4â†’1) â†’ 4å€å“åº”é€Ÿåº¦")
    
    print("\nğŸ“Š é¢„æœŸæ”¹è¿›:")
    print("   ğŸš€ è®­ç»ƒé€Ÿåº¦: +200-300%")
    print("   â±ï¸  æ¯è½®æ—¶é—´: 30åˆ†é’Ÿâ†’8-12åˆ†é’Ÿ")
    print("   ğŸ’¾ å†…å­˜ä½¿ç”¨: ~10-12GB (vs 14GB)")
    print("   ğŸ¯ è´¨é‡: ä¿æŒé«˜è´¨é‡ (256Ã—256)")
    
    print("\nğŸ—ï¸  æ¶æ„å¯¹æ¯”:")
    print("   ä¹‹å‰: 256â†’128â†’64â†’32â†’16â†’32 (4å±‚)")
    print("   ç°åœ¨: 256â†’128â†’64â†’32 (3å±‚)")
    print("   å‹ç¼©æ¯”: 64:1 â†’ 64:1 (ç›¸åŒ)")
    print("   å‚æ•°é‡: ~83M â†’ ~40M (å‡åŠ)")
    
    success = launch_efficient_training()
    
    if success:
        print("\nğŸ‰ é«˜æ•ˆè®­ç»ƒå®Œæˆ!")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: /kaggle/working/outputs/vae/final_model")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        print("ğŸ’¡ å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥:")
        print("   - å‡å°‘batch_sizeåˆ°8")
        print("   - å‡å°‘num_workersåˆ°2")
        sys.exit(1)

if __name__ == "__main__":
    main()
