#!/usr/bin/env python3
"""
LDM(æ‰©æ•£æ¨¡å‹)è®­ç»ƒé…ç½®æµ‹è¯•è„šæœ¬
éªŒè¯æ‰©æ•£æ¨¡å‹æ¶æ„é…ç½®æ˜¯å¦æ­£ç¡®
"""

import torch
import warnings
warnings.filterwarnings("ignore")

def test_ldm_config():
    """æµ‹è¯•LDMé…ç½®"""
    print("ğŸ” æµ‹è¯•LDM(æ‰©æ•£æ¨¡å‹)è®­ç»ƒé…ç½®")
    print("=" * 60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # æµ‹è¯•UNeté…ç½®
    print(f"\nğŸ¯ æµ‹è¯•UNeté…ç½®:")
    try:
        from diffusers import UNet2DConditionModel
        
        # ä½¿ç”¨ä¿®å¤åçš„é…ç½®
        unet = UNet2DConditionModel(
            sample_size=32,  # ä¿®å¤: ç›´æ¥è®¾ç½®ä¸ºå®é™…æ½œåœ¨å°ºå¯¸
            in_channels=4,   # VAEæ½œåœ¨ç»´åº¦
            out_channels=4,
            layers_per_block=2,
            block_out_channels=(320, 640, 1280, 1280),
            down_block_types=(
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
            ),
            cross_attention_dim=768,
            attention_head_dim=8,
            use_linear_projection=True,
            class_embed_type=None,
            num_class_embeds=None,
        ).to(device)
        
        total_params = sum(p.numel() for p in unet.parameters())
        print(f"   âœ… UNetåˆ›å»ºæˆåŠŸ - å‚æ•°é‡: {total_params:,}")
        print(f"   ğŸ“ sample_size: 32 (åŒ¹é…VAEæ½œåœ¨ç©ºé—´)")
        print(f"   ğŸ”— cross_attention_dim: 768")
        print(f"   ğŸ§± layers_per_block: 2")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        with torch.no_grad():
            # æ¨¡æ‹Ÿè¾“å…¥
            latents = torch.randn(2, 4, 32, 32).to(device)
            timesteps = torch.randint(0, 1000, (2,)).to(device)
            encoder_hidden_states = torch.randn(2, 1, 768).to(device)
            
            # UNetå‰å‘ä¼ æ’­
            noise_pred = unet(
                latents,
                timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )[0]
            
            print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ:")
            print(f"      è¾“å…¥æ½œåœ¨: {latents.shape}")
            print(f"      æ—¶é—´æ­¥: {timesteps.shape}")
            print(f"      æ¡ä»¶åµŒå…¥: {encoder_hidden_states.shape}")
            print(f"      å™ªå£°é¢„æµ‹: {noise_pred.shape}")
            
            if noise_pred.shape == latents.shape:
                print(f"   âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")
            else:
                print(f"   âŒ è¾“å‡ºå½¢çŠ¶é”™è¯¯")
                return False
        
    except Exception as e:
        print(f"   âŒ UNeté…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•è°ƒåº¦å™¨é…ç½®
    print(f"\nâ° æµ‹è¯•è°ƒåº¦å™¨é…ç½®:")
    try:
        from diffusers import DDPMScheduler, DDIMScheduler
        
        # DDPMè°ƒåº¦å™¨
        ddpm_scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.00085,
            beta_end=0.012,
            beta_schedule="scaled_linear",
            variance_type="fixed_small",
            clip_sample=False,
            prediction_type="epsilon",
        )
        
        print(f"   âœ… DDPMè°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")
        print(f"      è®­ç»ƒæ—¶é—´æ­¥: {ddpm_scheduler.config.num_train_timesteps}")
        print(f"      betaèŒƒå›´: {ddpm_scheduler.config.beta_start} - {ddpm_scheduler.config.beta_end}")
        print(f"      é¢„æµ‹ç±»å‹: {ddpm_scheduler.config.prediction_type}")
        
        # DDIMè°ƒåº¦å™¨ (ç”¨äºæ¨ç†)
        ddim_scheduler = DDIMScheduler.from_config(ddpm_scheduler.config)
        ddim_scheduler.set_timesteps(50)
        
        print(f"   âœ… DDIMè°ƒåº¦å™¨åˆ›å»ºæˆåŠŸ")
        print(f"      æ¨ç†æ­¥æ•°: 50")
        
        # æµ‹è¯•è°ƒåº¦å™¨åŠŸèƒ½
        with torch.no_grad():
            test_latents = torch.randn(1, 4, 32, 32)
            test_noise = torch.randn_like(test_latents)
            test_timesteps = torch.randint(0, 1000, (1,))
            
            # æ·»åŠ å™ªå£°
            noisy_latents = ddpm_scheduler.add_noise(test_latents, test_noise, test_timesteps)
            print(f"   âœ… å™ªå£°æ·»åŠ æµ‹è¯•é€šè¿‡: {noisy_latents.shape}")
            
            # å»å™ªæ­¥éª¤
            step_result = ddim_scheduler.step(test_noise, test_timesteps[0], test_latents)
            print(f"   âœ… å»å™ªæ­¥éª¤æµ‹è¯•é€šè¿‡: {step_result.prev_sample.shape}")
        
    except Exception as e:
        print(f"   âŒ è°ƒåº¦å™¨é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ¡ä»¶ç¼–ç å™¨
    print(f"\nğŸ­ æµ‹è¯•æ¡ä»¶ç¼–ç å™¨:")
    try:
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).parent / "training"))
        
        from train_diffusion import UserConditionEncoder
        
        # åˆ›å»ºæ¡ä»¶ç¼–ç å™¨
        num_users = 31  # å‡è®¾31ä¸ªç”¨æˆ·
        condition_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=768,
            dropout=0.1
        ).to(device)
        
        total_params = sum(p.numel() for p in condition_encoder.parameters())
        print(f"   âœ… æ¡ä»¶ç¼–ç å™¨åˆ›å»ºæˆåŠŸ - å‚æ•°é‡: {total_params:,}")
        print(f"   ğŸ‘¥ ç”¨æˆ·æ•°é‡: {num_users}")
        print(f"   ğŸ“ åµŒå…¥ç»´åº¦: 768")
        
        # æµ‹è¯•æ¡ä»¶ç¼–ç 
        with torch.no_grad():
            user_ids = torch.tensor([0, 5, 10]).to(device)
            user_embeds = condition_encoder(user_ids)
            
            print(f"   âœ… æ¡ä»¶ç¼–ç æµ‹è¯•é€šè¿‡:")
            print(f"      è¾“å…¥ç”¨æˆ·ID: {user_ids.shape}")
            print(f"      è¾“å‡ºåµŒå…¥: {user_embeds.shape}")
            
            if user_embeds.shape == (3, 768):
                print(f"   âœ… åµŒå…¥å½¢çŠ¶æ­£ç¡®")
            else:
                print(f"   âŒ åµŒå…¥å½¢çŠ¶é”™è¯¯")
                return False
        
    except Exception as e:
        print(f"   âŒ æ¡ä»¶ç¼–ç å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹
    print(f"\nğŸ”„ æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹:")
    try:
        with torch.no_grad():
            # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
            batch_size = 2
            latents = torch.randn(batch_size, 4, 32, 32).to(device)
            user_ids = torch.tensor([1, 15]).to(device)
            
            # ç¼–ç æ¡ä»¶
            encoder_hidden_states = condition_encoder(user_ids)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(1)  # [B, 1, embed_dim]
            
            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, 1000, (batch_size,)).to(device)
            noisy_latents = ddpm_scheduler.add_noise(latents, noise, timesteps)
            
            # UNeté¢„æµ‹
            model_pred = unet(
                noisy_latents,
                timesteps,
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )[0]
            
            # è®¡ç®—æŸå¤±
            loss = torch.nn.functional.mse_loss(model_pred, noise)
            
            print(f"   âœ… å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡:")
            print(f"      æ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"      æ½œåœ¨ç©ºé—´: {latents.shape}")
            print(f"      æ¡ä»¶åµŒå…¥: {encoder_hidden_states.shape}")
            print(f"      å™ªå£°é¢„æµ‹: {model_pred.shape}")
            print(f"      è®­ç»ƒæŸå¤±: {loss.item():.6f}")
        
    except Exception as e:
        print(f"   âŒ å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print(f"\nğŸ“‹ LDMè®­ç»ƒé…ç½®æ€»ç»“:")
    config = {
        "resolution": 128,
        "latent_size": 32,
        "unet_sample_size": 32,
        "cross_attention_dim": 768,
        "num_train_timesteps": 1000,
        "batch_size": 4,
        "learning_rate": 1e-4,
        "num_epochs": 100,
        "num_users": 31
    }
    
    for key, value in config.items():
        print(f"   âœ… {key}: {value}")
    
    return True

def show_training_command():
    """æ˜¾ç¤ºæ­£ç¡®çš„LDMè®­ç»ƒå‘½ä»¤"""
    print(f"\nğŸš€ æ­£ç¡®çš„LDMè®­ç»ƒå‘½ä»¤:")
    print(f"python training/train_diffusion.py \\")
    print(f"    --data_dir \"/kaggle/input/dataset\" \\")
    print(f"    --vae_path \"/kaggle/working/outputs/vae/final_model\" \\")
    print(f"    --resolution 128 \\")
    print(f"    --batch_size 4 \\")
    print(f"    --num_epochs 100 \\")
    print(f"    --learning_rate 1e-4 \\")
    print(f"    --cross_attention_dim 768 \\")
    print(f"    --output_dir \"/kaggle/working/outputs/diffusion\"")

def main():
    """ä¸»å‡½æ•°"""
    success = test_ldm_config()
    
    if success:
        show_training_command()
        print(f"\nâœ… LDMé…ç½®æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    else:
        print(f"\nâŒ LDMé…ç½®æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")

if __name__ == "__main__":
    main()
