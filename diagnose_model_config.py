#!/usr/bin/env python3
"""
è¯Šæ–­æ¨¡å‹é…ç½®è„šæœ¬
æ£€æŸ¥UNetå’Œæ¡ä»¶ç¼–ç å™¨çš„ç»´åº¦åŒ¹é…é—®é¢˜
"""

import torch
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

def diagnose_unet_config(unet_path):
    """è¯Šæ–­UNeté…ç½®"""
    print("ğŸ” è¯Šæ–­UNeté…ç½®...")
    
    try:
        from diffusers import UNet2DConditionModel
        
        # åŠ è½½UNet
        print(f"ä»è·¯å¾„åŠ è½½UNet: {unet_path}")
        unet = UNet2DConditionModel.from_pretrained(unet_path)
        
        print(f"\nğŸ“‹ UNeté…ç½®ä¿¡æ¯:")
        print(f"  - cross_attention_dim: {unet.config.cross_attention_dim}")
        print(f"  - in_channels: {unet.config.in_channels}")
        print(f"  - out_channels: {unet.config.out_channels}")
        print(f"  - sample_size: {unet.config.sample_size}")
        print(f"  - layers_per_block: {unet.config.layers_per_block}")
        print(f"  - block_out_channels: {unet.config.block_out_channels}")
        print(f"  - attention_head_dim: {unet.config.attention_head_dim}")
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in unet.parameters())
        print(f"  - æ€»å‚æ•°é‡: {total_params:,}")
        
        return unet.config.cross_attention_dim
        
    except Exception as e:
        print(f"âŒ UNeté…ç½®è¯Šæ–­å¤±è´¥: {e}")
        return None

def diagnose_condition_encoder(condition_encoder_path, expected_embed_dim, num_users=31):
    """è¯Šæ–­æ¡ä»¶ç¼–ç å™¨é…ç½®"""
    print(f"\nğŸ­ è¯Šæ–­æ¡ä»¶ç¼–ç å™¨é…ç½®...")
    
    try:
        from training.train_diffusion import UserConditionEncoder
        
        # å°è¯•åŠ è½½æ¡ä»¶ç¼–ç å™¨
        print(f"ä»è·¯å¾„åŠ è½½æ¡ä»¶ç¼–ç å™¨: {condition_encoder_path}")
        
        # å¤„ç†è·¯å¾„
        if Path(condition_encoder_path).is_dir():
            condition_encoder_file = Path(condition_encoder_path) / "condition_encoder.pt"
            if not condition_encoder_file.exists():
                print(f"âŒ æ¡ä»¶ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {condition_encoder_file}")
                return False
            condition_encoder_path = str(condition_encoder_file)
        
        # åˆ›å»ºæ¡ä»¶ç¼–ç å™¨å®ä¾‹
        condition_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=expected_embed_dim
        )
        
        # åŠ è½½æƒé‡
        state_dict = torch.load(condition_encoder_path, map_location='cpu')
        
        print(f"\nğŸ“‹ æ¡ä»¶ç¼–ç å™¨çŠ¶æ€å­—å…¸ä¿¡æ¯:")
        for key, tensor in state_dict.items():
            print(f"  - {key}: {tensor.shape}")
        
        # æ£€æŸ¥åµŒå…¥å±‚ç»´åº¦
        if 'user_embedding.weight' in state_dict:
            embed_shape = state_dict['user_embedding.weight'].shape
            actual_num_users, actual_embed_dim = embed_shape
            
            print(f"\nğŸ“ åµŒå…¥å±‚ç»´åº¦åˆ†æ:")
            print(f"  - å®é™…ç”¨æˆ·æ•°: {actual_num_users}")
            print(f"  - å®é™…åµŒå…¥ç»´åº¦: {actual_embed_dim}")
            print(f"  - æœŸæœ›ç”¨æˆ·æ•°: {num_users}")
            print(f"  - æœŸæœ›åµŒå…¥ç»´åº¦: {expected_embed_dim}")
            
            if actual_embed_dim != expected_embed_dim:
                print(f"âš ï¸  ç»´åº¦ä¸åŒ¹é…!")
                print(f"   æ¡ä»¶ç¼–ç å™¨åµŒå…¥ç»´åº¦: {actual_embed_dim}")
                print(f"   UNetæœŸæœ›ç»´åº¦: {expected_embed_dim}")
                return False
            else:
                print(f"âœ… ç»´åº¦åŒ¹é…!")
                return True
        else:
            print(f"âŒ æ‰¾ä¸åˆ°user_embedding.weight")
            return False
            
    except Exception as e:
        print(f"âŒ æ¡ä»¶ç¼–ç å™¨è¯Šæ–­å¤±è´¥: {e}")
        return False

def suggest_fix(unet_cross_attention_dim, condition_encoder_embed_dim):
    """å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
    print(f"\nğŸ”§ ä¿®å¤å»ºè®®:")
    
    if unet_cross_attention_dim != condition_encoder_embed_dim:
        print(f"âŒ ç»´åº¦ä¸åŒ¹é…é—®é¢˜:")
        print(f"   UNet cross_attention_dim: {unet_cross_attention_dim}")
        print(f"   æ¡ä»¶ç¼–ç å™¨ embed_dim: {condition_encoder_embed_dim}")
        
        print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"1. é‡æ–°è®­ç»ƒæ¡ä»¶ç¼–ç å™¨ï¼Œä½¿ç”¨embed_dim={unet_cross_attention_dim}")
        print(f"2. æˆ–è€…é‡æ–°è®­ç»ƒUNetï¼Œä½¿ç”¨cross_attention_dim={condition_encoder_embed_dim}")
        print(f"3. æˆ–è€…åœ¨æ¨ç†æ—¶åˆ›å»ºæ–°çš„æ¡ä»¶ç¼–ç å™¨ï¼Œä½¿ç”¨æ­£ç¡®çš„ç»´åº¦")
        
        print(f"\nğŸš€ å¿«é€Ÿä¿®å¤ (æ¨è):")
        print(f"åœ¨inference/generate.pyä¸­ï¼Œä¿®æ”¹æ¡ä»¶ç¼–ç å™¨åˆ›å»º:")
        print(f"```python")
        print(f"# æ–¹æ¡ˆ1: ä½¿ç”¨UNetçš„cross_attention_dim")
        print(f"self.condition_encoder = UserConditionEncoder(")
        print(f"    num_users=num_users,")
        print(f"    embed_dim={unet_cross_attention_dim}  # ä½¿ç”¨UNetçš„ç»´åº¦")
        print(f")")
        print(f"```")
        
        print(f"\nâš ï¸  æ³¨æ„: è¿™éœ€è¦é‡æ–°è®­ç»ƒæ¡ä»¶ç¼–ç å™¨æˆ–ä½¿ç”¨å…¼å®¹çš„é¢„è®­ç»ƒæƒé‡")
    else:
        print(f"âœ… ç»´åº¦åŒ¹é…ï¼Œé—®é¢˜å¯èƒ½åœ¨å…¶ä»–åœ°æ–¹")

def main():
    """ä¸»è¯Šæ–­å‡½æ•°"""
    print("ğŸ” æ¨¡å‹é…ç½®è¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    # ç¤ºä¾‹è·¯å¾„ - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    unet_path = "/kaggle/input/diffusion-final-model"
    condition_encoder_path = "/kaggle/input/diffusion-final-model/condition_encoder.pt"
    num_users = 31
    
    print(f"ğŸ“ è¯Šæ–­è·¯å¾„:")
    print(f"  UNetè·¯å¾„: {unet_path}")
    print(f"  æ¡ä»¶ç¼–ç å™¨è·¯å¾„: {condition_encoder_path}")
    print(f"  ç”¨æˆ·æ•°é‡: {num_users}")
    
    # è¯Šæ–­UNet
    unet_cross_attention_dim = diagnose_unet_config(unet_path)
    
    if unet_cross_attention_dim is None:
        print("âŒ æ— æ³•è·å–UNeté…ç½®ï¼Œåœæ­¢è¯Šæ–­")
        return
    
    # è¯Šæ–­æ¡ä»¶ç¼–ç å™¨
    condition_encoder_ok = diagnose_condition_encoder(
        condition_encoder_path, 
        unet_cross_attention_dim, 
        num_users
    )
    
    # æä¾›ä¿®å¤å»ºè®®
    suggest_fix(unet_cross_attention_dim, unet_cross_attention_dim if condition_encoder_ok else "æœªçŸ¥")
    
    print(f"\n" + "=" * 50)
    print(f"ğŸ¯ è¯Šæ–­å®Œæˆ!")

if __name__ == "__main__":
    main()
