#!/usr/bin/env python3
"""
åŸºäºè®­ç»ƒæ—¶æ¨ç†é€»è¾‘çš„ç”Ÿæˆè„šæœ¬
å®Œå…¨å¤åˆ¶è®­ç»ƒæ—¶çš„generate_sampleså‡½æ•°é€»è¾‘
"""

import os
import torch
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
from typing import List
from tqdm import tqdm

from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from training.train_diffusion import UserConditionEncoder

def generate_images_training_style(
    vae_path: str,
    unet_path: str,
    condition_encoder_path: str,
    user_ids: List[int],
    num_users: int,
    num_images_per_user: int = 1,
    num_inference_steps: int = 20,
    output_dir: str = "./generated_images",
    device: str = "auto",
    seed: int = 42,
    data_dir: str = None  # æ–°å¢ï¼šç”¨äºè·å–æ­£ç¡®çš„ç”¨æˆ·æ˜ å°„
):
    """
    ä½¿ç”¨è®­ç»ƒæ—¶çš„é€»è¾‘ç”Ÿæˆå›¾åƒ
    å®Œå…¨å¤åˆ¶train_diffusion.pyä¸­çš„generate_sampleså‡½æ•°

    Args:
        data_dir: è®­ç»ƒæ•°æ®ç›®å½•ï¼Œç”¨äºè·å–æ­£ç¡®çš„ç”¨æˆ·IDæ˜ å°„
    """
    
    # è®¾å¤‡æ£€æµ‹
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)

    # è·å–æ­£ç¡®çš„ç”¨æˆ·IDæ˜ å°„ (ä¿®å¤å…³é”®é—®é¢˜)
    user_id_to_idx = {}
    if data_dir is not None:
        print("ğŸ” è·å–è®­ç»ƒæ—¶çš„ç”¨æˆ·IDæ˜ å°„...")
        from pathlib import Path
        data_path = Path(data_dir)
        all_users = []

        # æ‰«ææ•°æ®ç›®å½•ï¼Œè·å–æ‰€æœ‰ç”¨æˆ·ID (ä¸è®­ç»ƒæ—¶é€»è¾‘ä¸€è‡´)
        for user_dir in data_path.iterdir():
            if user_dir.is_dir() and user_dir.name.startswith('ID_'):
                try:
                    user_id = int(user_dir.name.split('_')[1])
                    all_users.append(user_id)
                except ValueError:
                    continue

        # æ’åºå¹¶åˆ›å»ºæ˜ å°„ (ä¸è®­ç»ƒæ—¶é€»è¾‘ä¸€è‡´)
        all_users = sorted(all_users)
        user_id_to_idx = {user_id: idx for idx, user_id in enumerate(all_users)}

        print(f"  æ‰¾åˆ° {len(all_users)} ä¸ªç”¨æˆ·: {all_users}")
        print(f"  ç”¨æˆ·IDæ˜ å°„: {user_id_to_idx}")
    else:
        print("âš ï¸  æœªæä¾›æ•°æ®ç›®å½•ï¼Œä½¿ç”¨ç®€å•æ˜ å°„ (å¯èƒ½ä¸æ­£ç¡®)")
        # å›é€€åˆ°ç®€å•æ˜ å°„
        for user_id in user_ids:
            user_id_to_idx[user_id] = user_id - 1 if user_id > 0 else user_id
    
    # 1. åŠ è½½VAE (ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹å¼)
    print("Loading VAE...")
    vae = AutoencoderKL.from_pretrained(vae_path)
    vae.to(device)
    vae.eval()
    
    # 2. åŠ è½½UNet (ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹å¼)
    print("Loading UNet...")
    unet = UNet2DConditionModel.from_pretrained(unet_path)
    unet.to(device)
    unet.eval()
    
    print(f"UNeté…ç½®:")
    print(f"  - cross_attention_dim: {unet.config.cross_attention_dim}")
    print(f"  - in_channels: {unet.config.in_channels}")
    print(f"  - sample_size: {unet.config.sample_size}")
    
    # 3. åˆ›å»ºæ¡ä»¶ç¼–ç å™¨ (ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ–¹å¼)
    print("Creating Condition Encoder...")
    condition_encoder = UserConditionEncoder(
        num_users=num_users,
        embed_dim=unet.config.cross_attention_dim  # ä½¿ç”¨UNetçš„cross_attention_dim
    )
    
    # 4. åŠ è½½æ¡ä»¶ç¼–ç å™¨æƒé‡
    print("Loading Condition Encoder weights...")
    if Path(condition_encoder_path).is_dir():
        condition_encoder_file = Path(condition_encoder_path) / "condition_encoder.pt"
    else:
        condition_encoder_file = Path(condition_encoder_path)
    
    if condition_encoder_file.exists():
        try:
            condition_encoder.load_state_dict(torch.load(condition_encoder_file, map_location=device))
            print("âœ… æˆåŠŸåŠ è½½æ¡ä»¶ç¼–ç å™¨æƒé‡")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¡ä»¶ç¼–ç å™¨æƒé‡å¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
    else:
        print(f"âš ï¸  æ¡ä»¶ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {condition_encoder_file}")
        print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
    
    condition_encoder.to(device)
    condition_encoder.eval()
    
    # 5. åˆ›å»ºå™ªå£°è°ƒåº¦å™¨ (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
    print("Creating noise scheduler...")
    noise_scheduler = DDPMScheduler(
        num_train_timesteps=1000,
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        variance_type="fixed_small",
        clip_sample=False,
        prediction_type="epsilon",
        thresholding=False,
        dynamic_thresholding_ratio=0.995,
        clip_sample_range=1.0,
        sample_max_value=1.0,
    )
    
    # 6. åˆ›å»ºDDIMè°ƒåº¦å™¨ç”¨äºæ¨ç† (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
    ddim_scheduler = DDIMScheduler.from_config(noise_scheduler.config)
    ddim_scheduler.set_timesteps(num_inference_steps)
    
    print(f"âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆ...")
    
    # 7. ç”Ÿæˆå›¾åƒ (å®Œå…¨å¤åˆ¶è®­ç»ƒæ—¶çš„é€»è¾‘)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    with torch.no_grad():
        for user_id in user_ids:
            print(f"Generating {num_images_per_user} images for user {user_id}...")
            
            user_dir = output_path / f"user_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)
            
            for img_idx in range(num_images_per_user):
                # éšæœºå™ªå£° (ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒ)
                latents = torch.randn(1, 4, 32, 32, device=device)
                
                # ç”¨æˆ·æ¡ä»¶ç¼–ç  (ä½¿ç”¨æ­£ç¡®çš„æ˜ å°„)
                if user_id in user_id_to_idx:
                    user_idx = user_id_to_idx[user_id]
                    print(f"  ç”¨æˆ· {user_id} â†’ ç´¢å¼• {user_idx}")
                else:
                    print(f"  âš ï¸  ç”¨æˆ· {user_id} ä¸åœ¨æ˜ å°„ä¸­ï¼Œä½¿ç”¨é»˜è®¤ç´¢å¼• 0")
                    user_idx = 0

                user_idx_tensor = torch.tensor([user_idx], device=device)
                encoder_hidden_states = condition_encoder(user_idx_tensor)
                encoder_hidden_states = encoder_hidden_states.unsqueeze(1)
                
                # å»å™ªè¿‡ç¨‹ (ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒ)
                for t in tqdm(ddim_scheduler.timesteps, desc=f"User {user_id}, Image {img_idx+1}"):
                    timestep = t.unsqueeze(0).to(device)
                    
                    # é¢„æµ‹å™ªå£°
                    noise_pred = unet(
                        latents,
                        timestep,
                        encoder_hidden_states=encoder_hidden_states,
                        return_dict=False
                    )[0]
                    
                    # å»å™ªæ­¥éª¤
                    latents = ddim_scheduler.step(noise_pred, t, latents, return_dict=False)[0]
                
                # VAEè§£ç  (ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒ)
                vae_model = vae.module if hasattr(vae, 'module') else vae
                latents = latents / vae_model.config.scaling_factor
                image = vae_model.decode(latents).sample
                
                # è½¬æ¢ä¸ºPILå›¾åƒ (ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒ)
                image = image.clamp(0, 1)
                image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
                image = (image * 255).astype(np.uint8)
                
                # ä¿å­˜å›¾åƒ
                pil_image = Image.fromarray(image)
                pil_image.save(user_dir / f"generated_{img_idx:03d}.png")
                
                # æ¸…ç†å†…å­˜
                del latents
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
    
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼å›¾åƒä¿å­˜åˆ°: {output_dir}")

def main():
    parser = argparse.ArgumentParser(description="Generate images using training-style logic")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--vae_path", type=str, required=True, help="VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--unet_path", type=str, required=True, help="UNetæ¨¡å‹è·¯å¾„")
    parser.add_argument("--condition_encoder_path", type=str, required=True, help="æ¡ä»¶ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--num_users", type=int, required=True, help="ç”¨æˆ·æ€»æ•°")
    parser.add_argument("--data_dir", type=str, help="è®­ç»ƒæ•°æ®ç›®å½• (ç”¨äºè·å–æ­£ç¡®çš„ç”¨æˆ·IDæ˜ å°„)")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--user_ids", type=int, nargs="+", required=True, help="è¦ç”Ÿæˆçš„ç”¨æˆ·IDåˆ—è¡¨")
    parser.add_argument("--num_images_per_user", type=int, default=5, help="æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„å›¾åƒæ•°é‡")
    parser.add_argument("--num_inference_steps", type=int, default=20, help="æ¨ç†æ­¥æ•°")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./generated_images", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    generate_images_training_style(
        vae_path=args.vae_path,
        unet_path=args.unet_path,
        condition_encoder_path=args.condition_encoder_path,
        user_ids=args.user_ids,
        num_users=args.num_users,
        num_images_per_user=args.num_images_per_user,
        num_inference_steps=args.num_inference_steps,
        output_dir=args.output_dir,
        device=args.device,
        seed=args.seed,
        data_dir=args.data_dir
    )

if __name__ == "__main__":
    main()
