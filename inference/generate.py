#!/usr/bin/env python3
"""
å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ¡ä»¶ç”Ÿæˆè„šæœ¬
åŸºäºè®­ç»ƒå¥½çš„VAEå’Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ç”ŸæˆæŒ‡å®šç”¨æˆ·çš„å›¾åƒ
"""

import os
import torch
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
import json
from typing import List, Optional, Union, Dict
from tqdm import tqdm

from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from training.train_diffusion import UserConditionEncoder

class MicroDopplerGenerator:
    """å¾®å¤šæ™®å‹’å›¾åƒç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        vae_path: str,
        unet_path: str,
        condition_encoder_path: str,
        num_users: int,
        device: str = "cuda",
        scheduler_type: str = "ddim",
        user_id_mapping: Optional[Dict[int, int]] = None
    ):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            vae_path: VAEæ¨¡å‹è·¯å¾„
            unet_path: UNetæ¨¡å‹è·¯å¾„
            condition_encoder_path: æ¡ä»¶ç¼–ç å™¨è·¯å¾„
            num_users: ç”¨æˆ·æ€»æ•°
            device: è®¾å¤‡
            scheduler_type: è°ƒåº¦å™¨ç±»å‹ ("ddim" æˆ– "ddpm")
            user_id_mapping: ç”¨æˆ·IDåˆ°ç´¢å¼•çš„æ˜ å°„ {user_id: user_idx}
        """
        self.device = device
        self.num_users = num_users
        self.user_id_mapping = user_id_mapping or {}  # ç”¨æˆ·IDåˆ°ç´¢å¼•çš„æ˜ å°„
        
        # åŠ è½½VAE
        print("Loading VAE...")
        self.vae = AutoencoderKL.from_pretrained(vae_path)
        self.vae.to(device)
        self.vae.eval()
        
        # åŠ è½½UNet
        print("Loading UNet...")
        self.unet = UNet2DConditionModel.from_pretrained(unet_path)
        self.unet.to(device)
        self.unet.eval()
        
        # åŠ è½½æ¡ä»¶ç¼–ç å™¨
        print("Loading Condition Encoder...")
        self.condition_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=self.unet.config.cross_attention_dim
        )
        # å¤„ç†æ¡ä»¶ç¼–ç å™¨è·¯å¾„ - å¯èƒ½æ˜¯ç›®å½•æˆ–æ–‡ä»¶
        if Path(condition_encoder_path).is_dir():
            # å¦‚æœæ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾condition_encoder.ptæ–‡ä»¶
            condition_encoder_file = Path(condition_encoder_path) / "condition_encoder.pt"
            if not condition_encoder_file.exists():
                raise FileNotFoundError(f"æ¡ä»¶ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {condition_encoder_file}")
            condition_encoder_path = str(condition_encoder_file)

        self.condition_encoder.load_state_dict(torch.load(condition_encoder_path, map_location=device))
        self.condition_encoder.to(device)
        self.condition_encoder.eval()
        
        # åˆ›å»ºè°ƒåº¦å™¨ - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é…ç½®
        if scheduler_type == "ddim":
            # å…ˆåˆ›å»ºDDPMè°ƒåº¦å™¨é…ç½®ï¼Œç„¶åè½¬æ¢ä¸ºDDIM
            ddpm_scheduler = DDPMScheduler(
                num_train_timesteps=1000,
                beta_start=0.00085,
                beta_end=0.012,
                beta_schedule="scaled_linear",
                variance_type="fixed_small",
                clip_sample=False,
                prediction_type="epsilon",
                thresholding=False,
                dynamic_thresholding_ratio=0.995,
                clip_sample_range=1.0,
                sample_max_value=1.0,
            )
            self.scheduler = DDIMScheduler.from_config(ddpm_scheduler.config)
        else:
            self.scheduler = DDPMScheduler(
                num_train_timesteps=1000,
                beta_start=0.00085,
                beta_end=0.012,
                beta_schedule="scaled_linear",
                variance_type="fixed_small",
                clip_sample=False,
                prediction_type="epsilon",
                thresholding=False,
                dynamic_thresholding_ratio=0.995,
                clip_sample_range=1.0,
                sample_max_value=1.0,
            )
        
        print(f"Generator initialized with {scheduler_type} scheduler")
    
    @torch.no_grad()
    def generate(
        self,
        user_ids: Union[int, List[int]],
        num_images_per_user: int = 1,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        generator: Optional[torch.Generator] = None,
        return_latents: bool = False
    ) -> List[Image.Image]:
        """
        ç”Ÿæˆå¾®å¤šæ™®å‹’å›¾åƒ
        
        Args:
            user_ids: ç”¨æˆ·IDæˆ–ç”¨æˆ·IDåˆ—è¡¨
            num_images_per_user: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„å›¾åƒæ•°é‡
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦
            generator: éšæœºæ•°ç”Ÿæˆå™¨
            return_latents: æ˜¯å¦è¿”å›æ½œåœ¨è¡¨ç¤º
            
        Returns:
            ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨
        """
        # å¤„ç†ç”¨æˆ·ID
        if isinstance(user_ids, int):
            user_ids = [user_ids]
        
        # éªŒè¯ç”¨æˆ·ID - è€ƒè™‘ç”¨æˆ·IDæ˜ å°„
        for user_id in user_ids:
            # è·å–å®é™…çš„ç”¨æˆ·ç´¢å¼•
            user_idx = self.user_id_mapping.get(user_id, user_id - 1 if user_id > 0 else 0)
            if user_idx < 0 or user_idx >= self.num_users:
                raise ValueError(f"Invalid user_id {user_id} (mapped to index {user_idx}). Index must be in range [0, {self.num_users-1}]")
        
        # è®¾ç½®è°ƒåº¦å™¨
        self.scheduler.set_timesteps(num_inference_steps)
        
        generated_images = []
        
        for user_id in user_ids:
            print(f"Generating {num_images_per_user} images for user {user_id}...")
            
            for i in range(num_images_per_user):
                # åˆå§‹åŒ–éšæœºå™ªå£°
                latents = torch.randn(
                    (1, self.unet.config.in_channels, 32, 32),  # 256/8 = 32
                    generator=generator,
                    device=self.device,
                    dtype=self.unet.dtype
                )
                
                # ç¼©æ”¾åˆå§‹å™ªå£°
                latents = latents * self.scheduler.init_noise_sigma
                
                # ç¼–ç ç”¨æˆ·æ¡ä»¶ - ä¿®å¤: å°†user_idè½¬æ¢ä¸ºuser_idx
                user_idx = self.user_id_mapping.get(user_id, user_id - 1 if user_id > 0 else 0)
                user_tensor = torch.tensor([user_idx], device=self.device)
                encoder_hidden_states = self.condition_encoder(user_tensor)
                encoder_hidden_states = encoder_hidden_states.unsqueeze(1)  # [1, 1, embed_dim]
                
                # æ— æ¡ä»¶åµŒå…¥ (ç”¨äºclassifier-free guidance)
                if guidance_scale > 1.0:
                    uncond_user_tensor = torch.tensor([0], device=self.device)  # å‡è®¾0æ˜¯æ— æ¡ä»¶token
                    uncond_encoder_hidden_states = self.condition_encoder(uncond_user_tensor)
                    uncond_encoder_hidden_states = uncond_encoder_hidden_states.unsqueeze(1)
                    
                    # æ‹¼æ¥æ¡ä»¶å’Œæ— æ¡ä»¶åµŒå…¥
                    encoder_hidden_states = torch.cat([
                        uncond_encoder_hidden_states, encoder_hidden_states
                    ])
                    
                    # å¤åˆ¶latentsç”¨äºclassifier-free guidance
                    latents = torch.cat([latents] * 2)
                
                # å»å™ªå¾ªç¯
                for t in tqdm(self.scheduler.timesteps, desc=f"User {user_id}, Image {i+1}"):
                    # æ‰©å±•latentsç”¨äºclassifier-free guidance
                    latent_model_input = self.scheduler.scale_model_input(latents, t)
                    
                    # é¢„æµ‹å™ªå£°
                    noise_pred = self.unet(
                        latent_model_input,
                        t,
                        encoder_hidden_states=encoder_hidden_states,
                        return_dict=False
                    )[0]
                    
                    # æ‰§è¡Œclassifier-free guidance
                    if guidance_scale > 1.0:
                        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
                        
                        # åªä¿ç•™æ¡ä»¶éƒ¨åˆ†çš„latents
                        latents = latents.chunk(2)[1]
                    
                    # è®¡ç®—å‰ä¸€ä¸ªå™ªå£°æ ·æœ¬
                    latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]
                
                # å¦‚æœéœ€è¦è¿”å›æ½œåœ¨è¡¨ç¤º
                if return_latents:
                    generated_images.append(latents)
                else:
                    # VAEè§£ç 
                    latents = latents / self.vae.config.scaling_factor
                    image = self.vae.decode(latents).sample
                    
                    # è½¬æ¢ä¸ºPILå›¾åƒ
                    # ä¿®å¤: VAEè¾“å‡ºå·²ç»åœ¨[0,1]èŒƒå›´ï¼Œä¸éœ€è¦é¢å¤–çš„å½’ä¸€åŒ–
                    image = image.clamp(0, 1)
                    image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
                    image = (image * 255).astype(np.uint8)
                    generated_images.append(Image.fromarray(image))
        
        return generated_images
    
    def generate_interpolation(
        self,
        user_id1: int,
        user_id2: int,
        num_steps: int = 10,
        num_inference_steps: int = 50,
        generator: Optional[torch.Generator] = None
    ) -> List[Image.Image]:
        """
        åœ¨ä¸¤ä¸ªç”¨æˆ·ä¹‹é—´ç”Ÿæˆæ’å€¼å›¾åƒ
        
        Args:
            user_id1: èµ·å§‹ç”¨æˆ·ID
            user_id2: ç»“æŸç”¨æˆ·ID
            num_steps: æ’å€¼æ­¥æ•°
            num_inference_steps: æ¨ç†æ­¥æ•°
            generator: éšæœºæ•°ç”Ÿæˆå™¨
            
        Returns:
            æ’å€¼å›¾åƒåˆ—è¡¨
        """
        # è·å–ç”¨æˆ·åµŒå…¥
        user1_tensor = torch.tensor([user_id1], device=self.device)
        user2_tensor = torch.tensor([user_id2], device=self.device)
        
        user1_embed = self.condition_encoder(user1_tensor)
        user2_embed = self.condition_encoder(user2_tensor)
        
        # è®¾ç½®è°ƒåº¦å™¨
        self.scheduler.set_timesteps(num_inference_steps)
        
        interpolated_images = []
        
        for i in range(num_steps):
            # è®¡ç®—æ’å€¼æƒé‡
            alpha = i / (num_steps - 1)
            
            # æ’å€¼ç”¨æˆ·åµŒå…¥
            interpolated_embed = (1 - alpha) * user1_embed + alpha * user2_embed
            interpolated_embed = interpolated_embed.unsqueeze(1)
            
            # ç”Ÿæˆå›¾åƒ
            latents = torch.randn(
                (1, self.unet.config.in_channels, 32, 32),
                generator=generator,
                device=self.device,
                dtype=self.unet.dtype
            )
            latents = latents * self.scheduler.init_noise_sigma
            
            # å»å™ªå¾ªç¯
            for t in self.scheduler.timesteps:
                latent_model_input = self.scheduler.scale_model_input(latents, t)
                
                noise_pred = self.unet(
                    latent_model_input,
                    t,
                    encoder_hidden_states=interpolated_embed,
                    return_dict=False
                )[0]
                
                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]
            
            # VAEè§£ç 
            latents = latents / self.vae.config.scaling_factor
            image = self.vae.decode(latents).sample
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            # ä¿®å¤: VAEè¾“å‡ºå·²ç»åœ¨[0,1]èŒƒå›´ï¼Œä¸éœ€è¦é¢å¤–çš„å½’ä¸€åŒ–
            image = image.clamp(0, 1)
            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
            image = (image * 255).astype(np.uint8)
            interpolated_images.append(Image.fromarray(image))
        
        return interpolated_images

def main():
    parser = argparse.ArgumentParser(description="Generate Micro-Doppler Images")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--vae_path", type=str, required=True, help="VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--unet_path", type=str, required=True, help="UNetæ¨¡å‹è·¯å¾„")
    parser.add_argument("--condition_encoder_path", type=str, required=True, help="æ¡ä»¶ç¼–ç å™¨è·¯å¾„ (æ–‡ä»¶æˆ–åŒ…å«condition_encoder.ptçš„ç›®å½•)")
    parser.add_argument("--num_users", type=int, required=True, help="ç”¨æˆ·æ€»æ•°")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--user_ids", type=int, nargs="+", required=True, help="è¦ç”Ÿæˆçš„ç”¨æˆ·IDåˆ—è¡¨")
    parser.add_argument("--num_images_per_user", type=int, default=5, help="æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„å›¾åƒæ•°é‡")
    parser.add_argument("--num_inference_steps", type=int, default=50, help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--guidance_scale", type=float, default=7.5, help="å¼•å¯¼å¼ºåº¦")
    parser.add_argument("--scheduler_type", type=str, default="ddim", choices=["ddim", "ddpm"], help="è°ƒåº¦å™¨ç±»å‹")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./generated_images", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡ (cuda/cpu/autoï¼Œé»˜è®¤autoè‡ªåŠ¨æ£€æµ‹)")
    
    # ç‰¹æ®ŠåŠŸèƒ½
    parser.add_argument("--interpolation", action="store_true", help="ç”Ÿæˆæ’å€¼å›¾åƒ")
    parser.add_argument("--interpolation_users", type=int, nargs=2, help="æ’å€¼çš„ä¸¤ä¸ªç”¨æˆ·ID")
    parser.add_argument("--interpolation_steps", type=int, default=10, help="æ’å€¼æ­¥æ•°")
    
    args = parser.parse_args()

    # è®¾å¤‡è‡ªåŠ¨æ£€æµ‹
    if args.device == "auto":
        if torch.cuda.is_available():
            device = "cuda"
            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨GPUåŠ é€Ÿ")
        else:
            device = "cpu"
            print(f"ğŸ’» æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨CPU")
    else:
        device = args.device
        print(f"ğŸ”§ ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}")

    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        generator = torch.Generator().manual_seed(args.seed)
    else:
        generator = None
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator_model = MicroDopplerGenerator(
        vae_path=args.vae_path,
        unet_path=args.unet_path,
        condition_encoder_path=args.condition_encoder_path,
        num_users=args.num_users,
        device=device,
        scheduler_type=args.scheduler_type
    )
    
    if args.interpolation and args.interpolation_users:
        # ç”Ÿæˆæ’å€¼å›¾åƒ
        print(f"Generating interpolation between users {args.interpolation_users[0]} and {args.interpolation_users[1]}")
        
        images = generator_model.generate_interpolation(
            user_id1=args.interpolation_users[0],
            user_id2=args.interpolation_users[1],
            num_steps=args.interpolation_steps,
            num_inference_steps=args.num_inference_steps,
            generator=generator
        )
        
        # ä¿å­˜æ’å€¼å›¾åƒ
        interp_dir = output_dir / f"interpolation_{args.interpolation_users[0]}_{args.interpolation_users[1]}"
        interp_dir.mkdir(exist_ok=True)
        
        for i, img in enumerate(images):
            img.save(interp_dir / f"step_{i:03d}.png")
        
        # åˆ›å»ºæ‹¼æ¥å›¾åƒ
        width, height = images[0].size
        combined = Image.new('RGB', (width * len(images), height))
        for i, img in enumerate(images):
            combined.paste(img, (i * width, 0))
        combined.save(interp_dir / "combined.png")
        
        print(f"Interpolation images saved to {interp_dir}")
    
    else:
        # ç”ŸæˆæŒ‡å®šç”¨æˆ·çš„å›¾åƒ
        print(f"Generating images for users: {args.user_ids}")
        
        images = generator_model.generate(
            user_ids=args.user_ids,
            num_images_per_user=args.num_images_per_user,
            num_inference_steps=args.num_inference_steps,
            guidance_scale=args.guidance_scale,
            generator=generator
        )
        
        # ä¿å­˜å›¾åƒ
        img_idx = 0
        for user_id in args.user_ids:
            user_dir = output_dir / f"user_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)
            
            for i in range(args.num_images_per_user):
                images[img_idx].save(user_dir / f"generated_{i:03d}.png")
                img_idx += 1
        
        print(f"Generated images saved to {output_dir}")

if __name__ == "__main__":
    main()
