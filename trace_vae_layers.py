#!/usr/bin/env python3
"""
è¿½è¸ªVAEæ¯ä¸€å±‚çš„è¾“å‡ºå°ºå¯¸å˜åŒ–
éªŒè¯ä¸‹é‡‡æ ·çš„å…·ä½“è¿‡ç¨‹
"""

import torch
from diffusers import AutoencoderKL
import torch.nn as nn

class LayerTracker:
    """è¿½è¸ªæ¯ä¸€å±‚çš„è¾“å‡º"""
    def __init__(self):
        self.layer_outputs = []
        self.layer_names = []
    
    def hook_fn(self, name):
        def hook(module, input, output):
            if isinstance(output, torch.Tensor):
                self.layer_outputs.append(output.shape)
                self.layer_names.append(name)
                print(f"   {name}: {input[0].shape} â†’ {output.shape}")
            elif hasattr(output, 'sample'):  # DiagonalGaussianDistribution
                self.layer_outputs.append(output.sample().shape)
                self.layer_names.append(name)
                print(f"   {name}: {input[0].shape} â†’ {output.sample().shape}")
        return hook

def trace_vae_architecture():
    """è¯¦ç»†è¿½è¸ªVAEæ¶æ„çš„æ¯ä¸€å±‚"""
    print("ğŸ” è¯¦ç»†è¿½è¸ªVAEæ¶æ„ - æ¯ä¸€å±‚çš„å°ºå¯¸å˜åŒ–")
    print("=" * 70)
    
    device = "cpu"  # ä½¿ç”¨CPUé¿å…CUDAé—®é¢˜
    
    # åˆ›å»º3å±‚é…ç½®çš„VAE (æˆ‘ä»¬ä¿®å¤åçš„é…ç½®)
    print("\nğŸ—ï¸ åˆ›å»º3å±‚DownEncoderBlock2Dé…ç½®:")
    vae = AutoencoderKL(
        in_channels=3,
        out_channels=3,
        down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
        up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
        block_out_channels=[128, 256, 512],
        latent_channels=4,
        sample_size=128,
        layers_per_block=1,
        act_fn="silu",
        norm_num_groups=32,
        scaling_factor=0.18215,
    ).to(device)
    
    # åˆ›å»ºè¿½è¸ªå™¨
    tracker = LayerTracker()
    
    # æ³¨å†Œé’©å­å‡½æ•°æ¥è¿½è¸ªç¼–ç å™¨çš„æ¯ä¸€å±‚
    print("\nğŸ“Š ç¼–ç å™¨å±‚çº§ç»“æ„:")
    print("   ç¼–ç å™¨è¾“å…¥å±‚:")
    vae.encoder.conv_in.register_forward_hook(tracker.hook_fn("conv_in"))
    
    print("   ä¸‹é‡‡æ ·å—:")
    for i, down_block in enumerate(vae.encoder.down_blocks):
        down_block.register_forward_hook(tracker.hook_fn(f"down_block_{i}"))
    
    print("   ä¸­é—´å±‚:")
    vae.encoder.mid_block.register_forward_hook(tracker.hook_fn("mid_block"))
    
    print("   è¾“å‡ºå±‚:")
    vae.encoder.conv_norm_out.register_forward_hook(tracker.hook_fn("conv_norm_out"))
    vae.encoder.conv_out.register_forward_hook(tracker.hook_fn("conv_out"))
    
    # æµ‹è¯•è¾“å…¥
    test_input = torch.randn(1, 3, 128, 128).to(device)
    print(f"\nğŸ¯ è¾“å…¥å›¾åƒ: {test_input.shape}")
    print("\nğŸ“ˆ ç¼–ç è¿‡ç¨‹ (é€å±‚è¿½è¸ª):")
    
    with torch.no_grad():
        # ç¼–ç 
        posterior = vae.encode(test_input).latent_dist
        latent = posterior.sample()
        
        print(f"\nâœ… æœ€ç»ˆæ½œåœ¨ç©ºé—´: {latent.shape}")
        
        # è§£ç 
        print(f"\nğŸ“‰ è§£ç è¿‡ç¨‹:")
        reconstructed = vae.decode(latent).sample
        print(f"âœ… é‡å»ºè¾“å‡º: {reconstructed.shape}")
    
    # åˆ†æç»“æœ
    print(f"\nğŸ¯ åˆ†æç»“æœ:")
    print(f"   è¾“å…¥: 128Ã—128 = 16,384 åƒç´ ")
    print(f"   æ½œåœ¨: 32Ã—32 = 1,024 åƒç´ ")
    print(f"   ç©ºé—´å‹ç¼©æ¯”: {128//32}Ã—{128//32} = {(128//32)**2}å€")
    print(f"   æ€»å‹ç¼©æ¯”: {(128*128*3)//(32*32*4)}:1")

def analyze_downsampling_pattern():
    """åˆ†æä¸‹é‡‡æ ·æ¨¡å¼"""
    print("\n" + "="*70)
    print("ğŸ”¬ åˆ†æä¸‹é‡‡æ ·æ¨¡å¼")
    
    # æµ‹è¯•ä¸åŒå±‚æ•°çš„é…ç½®
    configs = [
        (1, [128], "1å±‚"),
        (2, [128, 256], "2å±‚"), 
        (3, [128, 256, 512], "3å±‚"),
        (4, [128, 256, 512, 512], "4å±‚")
    ]
    
    device = "cpu"
    
    for num_layers, channels, name in configs:
        print(f"\nğŸ“Š {name}é…ç½®:")
        try:
            down_blocks = ["DownEncoderBlock2D"] * num_layers
            up_blocks = ["UpDecoderBlock2D"] * num_layers
            
            vae = AutoencoderKL(
                in_channels=3,
                out_channels=3,
                down_block_types=down_blocks,
                up_block_types=up_blocks,
                block_out_channels=channels,
                latent_channels=4,
                sample_size=128,
                layers_per_block=1,
                act_fn="silu",
                norm_num_groups=32,
                scaling_factor=0.18215,
            ).to(device)
            
            test_input = torch.randn(1, 3, 128, 128).to(device)
            with torch.no_grad():
                latent = vae.encode(test_input).latent_dist.sample()
            
            actual_factor = 128 // latent.shape[-1]
            theoretical_factor = 2 ** num_layers
            
            print(f"   è¾“å…¥: {test_input.shape}")
            print(f"   æ½œåœ¨: {latent.shape}")
            print(f"   å®é™…ä¸‹é‡‡æ ·: {actual_factor}å€ (128â†’{latent.shape[-1]})")
            print(f"   ç†è®ºä¸‹é‡‡æ ·: {theoretical_factor}å€")
            print(f"   è§„å¾‹: å®é™… = ç†è®º Ã· 2 (ç¬¬ä¸€å±‚ä¸ä¸‹é‡‡æ ·)")
            
        except Exception as e:
            print(f"   âŒ {name}é…ç½®å¤±è´¥: {e}")

if __name__ == "__main__":
    trace_vae_architecture()
    analyze_downsampling_pattern()
