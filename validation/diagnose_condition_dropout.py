#!/usr/bin/env python3
"""
è¯Šæ–­æ¡ä»¶dropouté—®é¢˜
æ£€æŸ¥æ¡ä»¶æ‰©æ•£æ¨¡å‹æ˜¯å¦å› ä¸ºæ¡ä»¶dropoutè€Œæ²¡æœ‰å­¦åˆ°ç”¨æˆ·ç‰¹å¾
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

def analyze_condition_dropout_impact():
    """åˆ†ææ¡ä»¶dropoutå¯¹è®­ç»ƒçš„å½±å“"""
    print("ğŸ” åˆ†ææ¡ä»¶dropoutå¯¹è®­ç»ƒçš„å½±å“")
    
    # ä»è®­ç»ƒä»£ç ä¸­æˆ‘ä»¬çœ‹åˆ°ï¼š
    # if np.random.random() < args.condition_dropout:
    #     user_conditions = torch.zeros_like(user_indices)  # æ— æ¡ä»¶
    # else:
    #     user_conditions = user_indices  # æœ‰æ¡ä»¶
    
    print("\nğŸ“‹ è®­ç»ƒä»£ç åˆ†æ:")
    print("  æ¡ä»¶dropouté€»è¾‘:")
    print("    - å¦‚æœéšæœºæ•° < condition_dropout: ä½¿ç”¨é›¶å‘é‡(æ— æ¡ä»¶)")
    print("    - å¦åˆ™: ä½¿ç”¨çœŸå®ç”¨æˆ·ID")
    
    # æ¨¡æ‹Ÿä¸åŒdropoutç‡çš„å½±å“
    dropout_rates = [0.0, 0.1, 0.2, 0.5, 0.9]
    
    print(f"\nğŸ“Š ä¸åŒdropoutç‡çš„å½±å“åˆ†æ:")
    for dropout_rate in dropout_rates:
        # æ¨¡æ‹Ÿ1000æ¬¡è®­ç»ƒæ­¥éª¤
        num_steps = 1000
        conditional_steps = 0
        
        for _ in range(num_steps):
            if np.random.random() >= dropout_rate:  # æ³¨æ„ï¼šè¿™é‡Œæ˜¯>=ï¼Œè¡¨ç¤ºä½¿ç”¨æ¡ä»¶
                conditional_steps += 1
        
        conditional_ratio = conditional_steps / num_steps
        
        print(f"  Dropoutç‡ {dropout_rate:.1f}: æ¡ä»¶è®­ç»ƒæ¯”ä¾‹ {conditional_ratio:.1%}")
        
        if conditional_ratio < 0.5:
            print(f"    âŒ æ¡ä»¶è®­ç»ƒä¸è¶³ï¼Œæ¨¡å‹å¯èƒ½å­¦ä¸åˆ°ç”¨æˆ·ç‰¹å¾")
        elif conditional_ratio < 0.8:
            print(f"    âš ï¸  æ¡ä»¶è®­ç»ƒè¾ƒå°‘ï¼Œç”¨æˆ·ç‰¹å¾å¯èƒ½è¾ƒå¼±")
        else:
            print(f"    âœ… æ¡ä»¶è®­ç»ƒå……åˆ†")
    
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    print(f"  1. æ¡ä»¶dropoutçš„ç›®çš„æ˜¯è®©æ¨¡å‹å­¦ä¼šæ— æ¡ä»¶ç”Ÿæˆ")
    print(f"  2. ä½†å¦‚æœdropoutç‡å¤ªé«˜ï¼Œæ¨¡å‹ä¸»è¦å­¦æ— æ¡ä»¶ç”Ÿæˆ")
    print(f"  3. è¿™ä¼šå¯¼è‡´ç”¨æˆ·ç‰¹å¾å¾ˆå¼±æˆ–å®Œå…¨æ²¡æœ‰")
    
    return True

def test_condition_encoder_embeddings(condition_encoder_path: str, num_users: int = 31):
    """æµ‹è¯•æ¡ä»¶ç¼–ç å™¨çš„åµŒå…¥æ˜¯å¦æœ‰æ„ä¹‰"""
    print(f"\nğŸ§  æµ‹è¯•æ¡ä»¶ç¼–ç å™¨åµŒå…¥")
    
    if not Path(condition_encoder_path).exists():
        print(f"âŒ æ¡ä»¶ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {condition_encoder_path}")
        return False
    
    try:
        # åŠ è½½æ¡ä»¶ç¼–ç å™¨
        from training.train_diffusion import UserConditionEncoder
        
        # åˆ›å»ºæ¡ä»¶ç¼–ç å™¨ (éœ€è¦çŸ¥é“embed_dim)
        condition_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=512  # å‡è®¾æ˜¯512ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
        )
        
        # åŠ è½½æƒé‡
        state_dict = torch.load(condition_encoder_path, map_location='cpu')
        condition_encoder.load_state_dict(state_dict)
        condition_encoder.eval()
        
        print(f"âœ… æˆåŠŸåŠ è½½æ¡ä»¶ç¼–ç å™¨")
        
        # æµ‹è¯•ä¸åŒç”¨æˆ·çš„åµŒå…¥
        with torch.no_grad():
            user_embeddings = []
            for user_idx in range(min(5, num_users)):  # æµ‹è¯•å‰5ä¸ªç”¨æˆ·
                user_tensor = torch.tensor([user_idx])
                embedding = condition_encoder(user_tensor)
                user_embeddings.append(embedding.numpy())
                
                print(f"  ç”¨æˆ· {user_idx}: åµŒå…¥èŒƒæ•° {torch.norm(embedding).item():.3f}")
        
        # åˆ†æåµŒå…¥å·®å¼‚
        if len(user_embeddings) >= 2:
            user_embeddings = np.array(user_embeddings)
            
            # è®¡ç®—ç”¨æˆ·é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(user_embeddings)
            
            print(f"\nğŸ“Š ç”¨æˆ·åµŒå…¥ç›¸ä¼¼åº¦åˆ†æ:")
            avg_similarity = np.mean(similarities[np.triu_indices_from(similarities, k=1)])
            print(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
            
            if avg_similarity > 0.95:
                print(f"  âŒ ç”¨æˆ·åµŒå…¥å‡ ä¹ç›¸åŒï¼Œæ²¡æœ‰å­¦åˆ°ç”¨æˆ·ç‰¹å¾")
                return False
            elif avg_similarity > 0.8:
                print(f"  âš ï¸  ç”¨æˆ·åµŒå…¥ç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œç”¨æˆ·ç‰¹å¾è¾ƒå¼±")
                return True
            else:
                print(f"  âœ… ç”¨æˆ·åµŒå…¥æœ‰æ˜æ˜¾å·®å¼‚ï¼Œå­¦åˆ°äº†ç”¨æˆ·ç‰¹å¾")
                return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ¡ä»¶ç¼–ç å™¨å¤±è´¥: {e}")
        return False

def simulate_generation_with_different_conditions():
    """æ¨¡æ‹Ÿä¸åŒæ¡ä»¶ä¸‹çš„ç”Ÿæˆæ•ˆæœ"""
    print(f"\nğŸ¨ æ¨¡æ‹Ÿä¸åŒæ¡ä»¶ä¸‹çš„ç”Ÿæˆæ•ˆæœ")
    
    # æ¨¡æ‹Ÿæ¡ä»¶ç¼–ç å™¨
    embed_dim = 512
    num_users = 5
    
    # æƒ…å†µ1ï¼šç”¨æˆ·åµŒå…¥å‡ ä¹ç›¸åŒï¼ˆæ¡ä»¶dropoutå¤ªé«˜çš„ç»“æœï¼‰
    print(f"\næƒ…å†µ1: ç”¨æˆ·åµŒå…¥å‡ ä¹ç›¸åŒ")
    similar_embeddings = []
    base_embedding = np.random.randn(embed_dim)
    for i in range(num_users):
        # æ·»åŠ å¾ˆå°çš„å™ªå£°
        embedding = base_embedding + np.random.randn(embed_dim) * 0.01
        similar_embeddings.append(embedding)
    
    similar_embeddings = np.array(similar_embeddings)
    from sklearn.metrics.pairwise import cosine_similarity
    sim_similarities = cosine_similarity(similar_embeddings)
    avg_sim = np.mean(sim_similarities[np.triu_indices_from(sim_similarities, k=1)])
    print(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_sim:.3f}")
    print(f"  é¢„æœŸæ•ˆæœ: æ‰€æœ‰ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒéƒ½å¾ˆç›¸ä¼¼")
    
    # æƒ…å†µ2ï¼šç”¨æˆ·åµŒå…¥æœ‰æ˜æ˜¾å·®å¼‚ï¼ˆæ­£å¸¸è®­ç»ƒçš„ç»“æœï¼‰
    print(f"\næƒ…å†µ2: ç”¨æˆ·åµŒå…¥æœ‰æ˜æ˜¾å·®å¼‚")
    diverse_embeddings = []
    for i in range(num_users):
        # æ¯ä¸ªç”¨æˆ·æœ‰ç‹¬ç‰¹çš„åµŒå…¥
        embedding = np.random.randn(embed_dim)
        diverse_embeddings.append(embedding)
    
    diverse_embeddings = np.array(diverse_embeddings)
    div_similarities = cosine_similarity(diverse_embeddings)
    avg_div = np.mean(div_similarities[np.triu_indices_from(div_similarities, k=1)])
    print(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_div:.3f}")
    print(f"  é¢„æœŸæ•ˆæœ: ä¸åŒç”¨æˆ·ç”Ÿæˆçš„å›¾åƒæœ‰æ˜æ˜¾å·®å¼‚")
    
    return True

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯Šæ–­æ¡ä»¶dropouté—®é¢˜")
    parser.add_argument("--condition_encoder_path", type=str, help="æ¡ä»¶ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--num_users", type=int, default=31, help="ç”¨æˆ·æ•°é‡")
    
    args = parser.parse_args()
    
    print("ğŸ” è¯Šæ–­æ¡ä»¶dropouté—®é¢˜")
    print("=" * 50)
    
    # åˆ†ææ¡ä»¶dropoutå½±å“
    analyze_condition_dropout_impact()
    
    # æµ‹è¯•æ¡ä»¶ç¼–ç å™¨åµŒå…¥
    if args.condition_encoder_path:
        test_condition_encoder_embeddings(args.condition_encoder_path, args.num_users)
    
    # æ¨¡æ‹Ÿç”Ÿæˆæ•ˆæœ
    simulate_generation_with_different_conditions()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ è¯Šæ–­ç»“è®º:")
    print("1. å¦‚æœæ¡ä»¶dropoutç‡å¤ªé«˜(>0.5)ï¼Œæ¨¡å‹ä¸»è¦å­¦æ— æ¡ä»¶ç”Ÿæˆ")
    print("2. è¿™ä¼šå¯¼è‡´ç”¨æˆ·åµŒå…¥ç›¸ä¼¼ï¼Œç”Ÿæˆå›¾åƒç¼ºä¹ç”¨æˆ·ç‰¹å¾")
    print("3. å»ºè®®æ£€æŸ¥è®­ç»ƒæ—¶ä½¿ç”¨çš„condition_dropoutå‚æ•°")
    print("4. å¦‚æœç”¨æˆ·åµŒå…¥ç›¸ä¼¼åº¦>0.9ï¼Œè¯´æ˜æ²¡æœ‰å­¦åˆ°ç”¨æˆ·ç‰¹å¾")
    
    print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
    print("1. é™ä½condition_dropoutç‡åˆ°0.1æˆ–æ›´ä½")
    print("2. é‡æ–°è®­ç»ƒæ¨¡å‹")
    print("3. æˆ–è€…ä½¿ç”¨æ›´å¼ºçš„æŒ‡å¯¼å¼ºåº¦æ¥å¼ºåŒ–ç”¨æˆ·ç‰¹å¾")

if __name__ == "__main__":
    main()
