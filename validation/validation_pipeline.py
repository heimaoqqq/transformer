#!/usr/bin/env python3
"""
ç°ä»£åŒ–çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹éªŒè¯ç³»ç»Ÿ
å‚è€ƒæˆç†Ÿé¡¹ç›®çš„è®¾è®¡æ¨¡å¼ï¼Œæä¾›å®Œæ•´çš„éªŒè¯æµç¨‹
"""

import os
import sys
import argparse
import json
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from validation.user_classifier import UserValidationSystem

@dataclass
class ValidationConfig:
    """éªŒè¯é…ç½®ç±» - å‚è€ƒHuggingFaceçš„é…ç½®æ¨¡å¼"""
    # åŸºæœ¬é…ç½®
    target_user_id: int
    real_data_root: str
    output_dir: str = "./validation_results"
    
    # åˆ†ç±»å™¨é…ç½®
    classifier_epochs: int = 30
    classifier_batch_size: int = 32
    classifier_lr: float = 5e-4
    max_samples_per_class: int = 1000
    confidence_threshold: float = 0.8
    
    # ç”Ÿæˆé…ç½®
    num_images_to_generate: int = 100  # å¢åŠ åˆ°100å¼ ï¼Œè·å¾—æ›´å¯é çš„ç»Ÿè®¡ç»“æœ
    num_inference_steps: int = 50  # DDIMæ¨ç†æ­¥æ•°ï¼Œå»ºè®®50-200
    batch_size: int = 10  # æ‰¹é‡ç”Ÿæˆå¤§å°ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å­˜
    
    # æ¨¡å‹è·¯å¾„
    vae_path: Optional[str] = None
    unet_path: Optional[str] = None
    condition_encoder_path: Optional[str] = None
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    
    def __post_init__(self):
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"

class ConditionalDiffusionValidator:
    """ç°ä»£åŒ–çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹éªŒè¯å™¨ - å‚è€ƒDiffusersçš„Pipelineè®¾è®¡"""
    
    def __init__(self, config: ValidationConfig):
        self.config = config
        self.validation_system = UserValidationSystem(device=config.device)
        self.output_path = Path(config.output_dir)
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹ç»„ä»¶ (å»¶è¿ŸåŠ è½½)
        self.vae = None
        self.unet = None
        self.condition_encoder = None
        self.scheduler = None
        self.user_id_mapping = None
        
    def load_models(self) -> bool:
        """åŠ è½½æ‰€æœ‰å¿…è¦çš„æ¨¡å‹ç»„ä»¶"""
        if not all([self.config.vae_path, self.config.unet_path, self.config.condition_encoder_path]):
            print("âŒ ç¼ºå°‘æ¨¡å‹è·¯å¾„ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
            return False
            
        try:
            print("ğŸ“‚ åŠ è½½æ¨¡å‹ç»„ä»¶...")
            
            # åŠ è½½VAE
            from diffusers import AutoencoderKL
            self.vae = AutoencoderKL.from_pretrained(self.config.vae_path)
            self.vae = self.vae.to(self.config.device)
            print("  âœ… VAEåŠ è½½å®Œæˆ")
            
            # åŠ è½½UNet
            from diffusers import UNet2DConditionModel
            self.unet = UNet2DConditionModel.from_pretrained(self.config.unet_path)
            self.unet = self.unet.to(self.config.device)
            print("  âœ… UNetåŠ è½½å®Œæˆ")
            
            # è·å–ç”¨æˆ·IDæ˜ å°„
            self.user_id_mapping = self._get_user_id_mapping()
            num_users = len(self.user_id_mapping)
            print(f"  ğŸ“Š ç”¨æˆ·æ˜ å°„: {self.user_id_mapping}")
            
            # åŠ è½½æ¡ä»¶ç¼–ç å™¨
            from training.train_diffusion import UserConditionEncoder
            self.condition_encoder = UserConditionEncoder(
                num_users=num_users,
                embed_dim=self.unet.config.cross_attention_dim
            )
            
            condition_encoder_state = torch.load(self.config.condition_encoder_path, map_location='cpu')
            self.condition_encoder.load_state_dict(condition_encoder_state)
            self.condition_encoder = self.condition_encoder.to(self.config.device)
            print("  âœ… æ¡ä»¶ç¼–ç å™¨åŠ è½½å®Œæˆ")
            
            # åˆ›å»ºè°ƒåº¦å™¨ (ä¸è®­ç»ƒæ—¶ä¸€è‡´)
            from diffusers import DDPMScheduler, DDIMScheduler
            noise_scheduler = DDPMScheduler(
                num_train_timesteps=1000,
                beta_start=0.00085,
                beta_end=0.012,
                beta_schedule="scaled_linear",
                clip_sample=False,
                prediction_type="epsilon",
            )
            # ä½¿ç”¨DDIMè°ƒåº¦å™¨è¿›è¡Œæ¨ç† (ä¸è®­ç»ƒæ—¶ç”Ÿæˆæ ·æœ¬ä¸€è‡´)
            self.scheduler = DDIMScheduler.from_config(noise_scheduler.config)
            print("  âœ… è°ƒåº¦å™¨åˆ›å»ºå®Œæˆ")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_user_id_mapping(self) -> Dict[int, int]:
        """è·å–ç”¨æˆ·IDæ˜ å°„ - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼Œå¹¶è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥"""
        data_path = Path(self.config.real_data_root)
        all_users = []

        print(f"  ğŸ” æ‰«ææ•°æ®ç›®å½•: {data_path}")

        for user_dir in data_path.iterdir():
            if user_dir.is_dir() and user_dir.name.startswith('ID_'):
                try:
                    user_id = int(user_dir.name.split('_')[1])
                    all_users.append(user_id)

                    # æ£€æŸ¥å›¾åƒæ•°é‡
                    image_files = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                    print(f"    ID_{user_id:2d}: {len(image_files):3d} å¼ å›¾åƒ")

                except ValueError:
                    print(f"    âš ï¸  æ— æ•ˆç›®å½•å: {user_dir.name}")
                    continue

        all_users = sorted(all_users)
        user_mapping = {user_id: idx for idx, user_id in enumerate(all_users)}

        print(f"  ğŸ“Š ç”¨æˆ·æ˜ å°„ (è®­ç»ƒæ—¶ä¸€è‡´): {user_mapping}")

        # æ£€æŸ¥ç›®æ ‡ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        if self.config.target_user_id not in user_mapping:
            print(f"  âŒ ç›®æ ‡ç”¨æˆ· {self.config.target_user_id} ä¸åœ¨æ•°æ®ä¸­!")
            print(f"  ğŸ’¡ å¯ç”¨ç”¨æˆ·: {sorted(user_mapping.keys())}")

        return user_mapping
    
    def train_classifier(self) -> bool:
        """è®­ç»ƒç”¨æˆ·åˆ†ç±»å™¨"""
        print(f"\nğŸ¤– è®­ç»ƒç”¨æˆ· {self.config.target_user_id} çš„åˆ†ç±»å™¨")
        print(f"  å‚æ•°: epochs={self.config.classifier_epochs}, batch_size={self.config.classifier_batch_size}")
        
        try:
            # å‡†å¤‡æ•°æ®
            image_paths, labels = self._prepare_classifier_data()
            
            if len(image_paths) == 0:
                print(f"âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ•°æ®")
                return False
            
            # è®­ç»ƒåˆ†ç±»å™¨
            history = self.validation_system.train_user_classifier(
                user_id=self.config.target_user_id,
                image_paths=image_paths,
                labels=labels,
                epochs=self.config.classifier_epochs,
                batch_size=self.config.classifier_batch_size,
                learning_rate=self.config.classifier_lr
            )
            
            # ä¿å­˜è®­ç»ƒæ›²çº¿
            plot_path = self.output_path / f"user_{self.config.target_user_id:02d}_training.png"
            self.validation_system.plot_training_history(history, str(plot_path))
            
            # æ£€æŸ¥è®­ç»ƒæ•ˆæœ
            best_val_acc = max(history['val_acc'])
            print(f"  ğŸ“Š æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3f}")
            
            return best_val_acc > 0.7  # è®¾å®šæœ€ä½å‡†ç¡®ç‡è¦æ±‚
            
        except Exception as e:
            print(f"âŒ åˆ†ç±»å™¨è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _prepare_classifier_data(self) -> Tuple[List[str], List[int]]:
        """å‡†å¤‡åˆ†ç±»å™¨è®­ç»ƒæ•°æ®"""
        data_path = Path(self.config.real_data_root)
        target_user_dir = None
        other_user_dirs = []
        
        # æŸ¥æ‰¾ç›®æ ‡ç”¨æˆ·å’Œå…¶ä»–ç”¨æˆ·ç›®å½•
        for item in data_path.iterdir():
            if item.is_dir():
                if item.name == f"ID_{self.config.target_user_id}":
                    target_user_dir = item
                elif item.name.startswith("ID_"):
                    try:
                        other_user_id = int(item.name.split("_")[1])
                        if other_user_id != self.config.target_user_id:
                            other_user_dirs.append(item)
                    except ValueError:
                        continue
        
        if target_user_dir is None:
            print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ· {self.config.target_user_id} çš„æ•°æ®ç›®å½•")
            return [], []

        # ä½¿ç”¨æ”¹è¿›çš„æ•°æ®å‡†å¤‡æ–¹æ³•
        return self.validation_system.prepare_user_data(
            user_id=self.config.target_user_id,
            real_images_dir=str(target_user_dir),
            other_users_dirs=[str(d) for d in other_user_dirs],
            max_samples_per_class=self.config.max_samples_per_class,
            negative_ratio=3.0  # è´Ÿæ ·æœ¬æ˜¯æ­£æ ·æœ¬çš„3å€
        )

    def generate_images(self) -> Optional[str]:
        """ç”ŸæˆæŒ‡å®šç”¨æˆ·çš„å›¾åƒ"""
        if not all([self.vae, self.unet, self.condition_encoder, self.scheduler]):
            print("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå›¾åƒ")
            return None

        if self.config.target_user_id not in self.user_id_mapping:
            print(f"âŒ ç”¨æˆ· {self.config.target_user_id} ä¸åœ¨æ˜ å°„ä¸­")
            return None

        print(f"\nğŸ¨ ç”Ÿæˆç”¨æˆ· {self.config.target_user_id} çš„å›¾åƒ")
        print(f"  å‚æ•°: çº¯æ¡ä»¶ç”Ÿæˆ, steps={self.config.num_inference_steps}")

        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            gen_output_dir = self.output_path / "generated_images" / f"user_{self.config.target_user_id:02d}"
            gen_output_dir.mkdir(parents=True, exist_ok=True)

            # è·å–ç”¨æˆ·ç´¢å¼•
            user_idx = self.user_id_mapping[self.config.target_user_id]

            # è®¾ç½®è°ƒåº¦å™¨
            self.scheduler.set_timesteps(self.config.num_inference_steps)

            # ç”Ÿæˆå›¾åƒ
            self.vae.eval()
            self.unet.eval()
            self.condition_encoder.eval()

            with torch.no_grad():
                # æ‰¹é‡ç”Ÿæˆé…ç½®
                batch_size = self.config.batch_size  # ä½¿ç”¨é…ç½®ä¸­çš„æ‰¹é‡å¤§å°
                total_images = self.config.num_images_to_generate
                num_batches = (total_images + batch_size - 1) // batch_size

                print(f"  ğŸ“Š æ‰¹é‡ç”Ÿæˆé…ç½®: {batch_size}å¼ /æ‰¹, å…±{num_batches}æ‰¹")

                image_count = 0
                for batch_idx in range(num_batches):
                    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å®é™…å¤§å°
                    current_batch_size = min(batch_size, total_images - batch_idx * batch_size)
                    print(f"  ğŸ¨ ç”Ÿæˆæ‰¹æ¬¡ {batch_idx+1}/{num_batches} ({current_batch_size}å¼ )...")

                    # æ‰¹é‡éšæœºå™ªå£°
                    latents = torch.randn(current_batch_size, 4, 32, 32, device=self.config.device)

                    # æ‰¹é‡ç”¨æˆ·æ¡ä»¶
                    user_tensor = torch.tensor([user_idx] * current_batch_size, device=self.config.device)
                    user_embedding = self.condition_encoder(user_tensor)

                    # ç¡®ä¿3Då¼ é‡æ ¼å¼
                    if user_embedding.dim() == 2:
                        user_embedding = user_embedding.unsqueeze(1)

                    # æ‰©æ•£è¿‡ç¨‹
                    latents = latents * self.scheduler.init_noise_sigma

                    for t in self.scheduler.timesteps:
                        # æ‰¹é‡çº¯æ¡ä»¶é¢„æµ‹ (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
                        noise_pred = self.unet(
                            latents,
                            t,
                            encoder_hidden_states=user_embedding
                        ).sample

                        # è°ƒåº¦å™¨æ­¥éª¤
                        latents = self.scheduler.step(noise_pred, t, latents).prev_sample

                    # æ‰¹é‡è§£ç ä¸ºå›¾åƒ
                    vae_model = self.vae.module if hasattr(self.vae, 'module') else self.vae
                    latents = latents / vae_model.config.scaling_factor
                    images = vae_model.decode(latents).sample
                    images = images.clamp(0, 1)

                    # æ‰¹é‡ä¿å­˜å›¾åƒ
                    from PIL import Image
                    batch_images = images.cpu().permute(0, 2, 3, 1).numpy()

                    for i in range(current_batch_size):
                        image = (batch_images[i] * 255).astype(np.uint8)
                        pil_image = Image.fromarray(image)

                        save_path = gen_output_dir / f"user_{self.config.target_user_id}_generated_{image_count+1:02d}.png"
                        pil_image.save(save_path)
                        image_count += 1

            print(f"  âœ… ç”Ÿæˆå®Œæˆï¼Œä¿å­˜åœ¨: {gen_output_dir}")
            return str(gen_output_dir)

        except Exception as e:
            print(f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def validate_generated_images(self, generated_images_dir: str) -> Dict:
        """éªŒè¯ç”Ÿæˆå›¾åƒ - æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒ…å«å¯¹æ¯”æ§åˆ¶å®éªŒ"""
        print(f"\nğŸ” éªŒè¯ç”Ÿæˆå›¾åƒ (æ”¹è¿›ç‰ˆæœ¬)")

        try:
            # 1. åŸæœ‰çš„åˆ†ç±»å™¨éªŒè¯
            basic_result = self.validation_system.validate_generated_images(
                user_id=self.config.target_user_id,
                generated_images_dir=generated_images_dir,
                confidence_threshold=self.config.confidence_threshold
            )

            # 2. å¯¹æ¯”æ§åˆ¶å®éªŒ
            control_result = self._controlled_validation_experiment(generated_images_dir)

            # 3. å…¨ç”¨æˆ·å¯¹æ¯”çŸ©é˜µéªŒè¯ï¼ˆå¯é€‰ï¼Œæ›´å…¨é¢ï¼‰
            matrix_result = self._full_user_matrix_validation()

            # 4. åˆå¹¶ç»“æœ
            result = {
                'basic_validation': basic_result,
                'control_experiment': control_result,
                'user_matrix_validation': matrix_result,
                'overall_success': self._evaluate_overall_success(basic_result, control_result, matrix_result)
            }

            # ä¿å­˜éªŒè¯ç»“æœ
            result_path = self.output_path / f"user_{self.config.target_user_id:02d}_validation.json"
            with open(result_path, 'w') as f:
                json.dump(result, f, indent=2)

            return result

        except Exception as e:
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}

    def _controlled_validation_experiment(self, generated_images_dir: str) -> Dict:
        """å¯¹æ¯”æ§åˆ¶å®éªŒ - éªŒè¯æ¡ä»¶ç”Ÿæˆçš„æœ‰æ•ˆæ€§"""
        print(f"  ğŸ§ª æ‰§è¡Œå¯¹æ¯”æ§åˆ¶å®éªŒ...")

        try:
            # 1. å¯¹æ¯”æ‰€æœ‰å…¶ä»–ç”¨æˆ·ï¼ˆæ›´ä¸¥æ ¼çš„éªŒè¯ï¼‰
            all_other_users = [uid for uid in self.user_id_mapping.keys()
                             if uid != self.config.target_user_id]

            # å¦‚æœç”¨æˆ·å¤ªå¤šï¼Œéšæœºé€‰æ‹©æœ€å¤š10ä¸ªè¿›è¡Œå¯¹æ¯”ï¼ˆå¹³è¡¡ä¸¥æ ¼æ€§å’Œæ•ˆç‡ï¼‰
            import random
            if len(all_other_users) > 10:
                wrong_user_ids = random.sample(all_other_users, 10)
                print(f"    ä»{len(all_other_users)}ä¸ªå…¶ä»–ç”¨æˆ·ä¸­éšæœºé€‰æ‹©10ä¸ªè¿›è¡Œå¯¹æ¯”")
            else:
                wrong_user_ids = all_other_users
                print(f"    å¯¹æ¯”æ‰€æœ‰{len(wrong_user_ids)}ä¸ªå…¶ä»–ç”¨æˆ·")

            control_results = {}

            # 2. ä¸ºæ¯ä¸ªé”™è¯¯ç”¨æˆ·IDç”Ÿæˆå›¾åƒå¹¶éªŒè¯
            for wrong_id in wrong_user_ids:
                print(f"    ç”Ÿæˆé”™è¯¯æ¡ä»¶å›¾åƒ (ç”¨æˆ·{wrong_id})...")

                # ç”Ÿæˆé”™è¯¯æ¡ä»¶çš„å›¾åƒ
                wrong_images_dir = self._generate_wrong_condition_images(wrong_id)

                if wrong_images_dir:
                    # ç”¨ç›®æ ‡ç”¨æˆ·çš„åˆ†ç±»å™¨éªŒè¯é”™è¯¯æ¡ä»¶å›¾åƒ
                    wrong_result = self.validation_system.validate_generated_images(
                        user_id=self.config.target_user_id,
                        generated_images_dir=wrong_images_dir,
                        confidence_threshold=self.config.confidence_threshold
                    )
                    control_results[f'wrong_user_{wrong_id}'] = wrong_result

            # 3. è®¡ç®—å¯¹æ¯”æŒ‡æ ‡
            if control_results:
                # æ­£ç¡®æ¡ä»¶çš„ç»“æœ
                correct_result = self.validation_system.validate_generated_images(
                    user_id=self.config.target_user_id,
                    generated_images_dir=generated_images_dir,
                    confidence_threshold=self.config.confidence_threshold
                )

                correct_success_rate = correct_result.get('success_rate', 0)
                wrong_success_rates = [r.get('success_rate', 0) for r in control_results.values()]
                avg_wrong_success_rate = sum(wrong_success_rates) / len(wrong_success_rates)

                # æ¡ä»¶æ§åˆ¶æ•ˆæœï¼šæ­£ç¡®æ¡ä»¶åº”è¯¥æ˜æ˜¾å¥½äºé”™è¯¯æ¡ä»¶
                condition_control_ratio = correct_success_rate / (avg_wrong_success_rate + 1e-6)

                return {
                    'correct_condition_success_rate': correct_success_rate,
                    'wrong_conditions_avg_success_rate': avg_wrong_success_rate,
                    'condition_control_ratio': condition_control_ratio,
                    'control_effective': condition_control_ratio > 2.0,  # æ­£ç¡®æ¡ä»¶åº”è¯¥è‡³å°‘å¥½2å€
                    'detailed_wrong_results': control_results
                }
            else:
                return {'error': 'Failed to generate control images'}

        except Exception as e:
            print(f"    âŒ å¯¹æ¯”å®éªŒå¤±è´¥: {e}")
            return {'error': str(e)}

    def _generate_wrong_condition_images(self, wrong_user_id: int, num_images: int = 4) -> str:
        """ç”Ÿæˆé”™è¯¯æ¡ä»¶çš„å›¾åƒç”¨äºå¯¹æ¯”"""
        try:
            # åˆ›å»ºé”™è¯¯æ¡ä»¶å›¾åƒçš„è¾“å‡ºç›®å½•
            wrong_dir = self.output_path / "control_images" / f"wrong_user_{wrong_user_id}"
            wrong_dir.mkdir(parents=True, exist_ok=True)

            # è·å–é”™è¯¯ç”¨æˆ·çš„ç´¢å¼•
            wrong_user_idx = self.user_id_mapping[wrong_user_id]

            # è®¾ç½®è°ƒåº¦å™¨
            self.scheduler.set_timesteps(self.config.num_inference_steps)

            # ç”Ÿæˆå›¾åƒ
            self.vae.eval()
            self.unet.eval()
            self.condition_encoder.eval()

            with torch.no_grad():
                # å¯¹æ¯”å®éªŒä¹Ÿä½¿ç”¨æ‰¹é‡ç”Ÿæˆï¼ˆæ•°é‡è¾ƒå°‘ï¼Œä¸€æ¬¡æ€§ç”Ÿæˆï¼‰
                print(f"    æ‰¹é‡ç”Ÿæˆ{num_images}å¼ å¯¹æ¯”å›¾åƒ...")

                # æ‰¹é‡éšæœºå™ªå£°
                latents = torch.randn(num_images, 4, 32, 32, device=self.config.device)

                # æ‰¹é‡é”™è¯¯ç”¨æˆ·æ¡ä»¶
                user_tensor = torch.tensor([wrong_user_idx] * num_images, device=self.config.device)
                user_embedding = self.condition_encoder(user_tensor)

                # ç¡®ä¿3Då¼ é‡æ ¼å¼
                if user_embedding.dim() == 2:
                    user_embedding = user_embedding.unsqueeze(1)

                # æ‰©æ•£è¿‡ç¨‹
                latents = latents * self.scheduler.init_noise_sigma

                for t in self.scheduler.timesteps:
                    # æ‰¹é‡çº¯æ¡ä»¶é¢„æµ‹
                    noise_pred = self.unet(
                        latents,
                        t,
                        encoder_hidden_states=user_embedding
                    ).sample

                    # è°ƒåº¦å™¨æ­¥éª¤
                    latents = self.scheduler.step(noise_pred, t, latents).prev_sample

                # æ‰¹é‡è§£ç ä¸ºå›¾åƒ
                vae_model = self.vae.module if hasattr(self.vae, 'module') else self.vae
                latents = latents / vae_model.config.scaling_factor
                images = vae_model.decode(latents).sample
                images = images.clamp(0, 1)

                # æ‰¹é‡ä¿å­˜å›¾åƒ
                from PIL import Image
                batch_images = images.cpu().permute(0, 2, 3, 1).numpy()

                for i in range(num_images):
                    image = (batch_images[i] * 255).astype(np.uint8)
                    pil_image = Image.fromarray(image)

                    save_path = wrong_dir / f"wrong_condition_{i+1:02d}.png"
                    pil_image.save(save_path)

            return str(wrong_dir)

        except Exception as e:
            print(f"    âŒ ç”Ÿæˆé”™è¯¯æ¡ä»¶å›¾åƒå¤±è´¥: {e}")
            return None

    def _full_user_matrix_validation(self) -> Dict:
        """å…¨ç”¨æˆ·å¯¹æ¯”çŸ©é˜µéªŒè¯ - æœ€ä¸¥æ ¼çš„éªŒè¯æ–¹æ³•"""
        print(f"  ğŸ“Š æ‰§è¡Œå…¨ç”¨æˆ·çŸ©é˜µéªŒè¯...")

        try:
            all_users = list(self.user_id_mapping.keys())
            target_user = self.config.target_user_id

            # é™åˆ¶éªŒè¯è§„æ¨¡ï¼ˆé¿å…è®¡ç®—é‡è¿‡å¤§ï¼‰
            if len(all_users) > 8:
                print(f"    ç”¨æˆ·æ•°é‡({len(all_users)})è¾ƒå¤šï¼Œè·³è¿‡çŸ©é˜µéªŒè¯ä»¥èŠ‚çœæ—¶é—´")
                return {'skipped': True, 'reason': 'too_many_users'}

            print(f"    ä¸ºæ‰€æœ‰{len(all_users)}ä¸ªç”¨æˆ·ç”Ÿæˆå›¾åƒå¹¶äº¤å‰éªŒè¯...")

            # ç”Ÿæˆæ‰€æœ‰ç”¨æˆ·çš„å›¾åƒ
            user_images = {}
            for user_id in all_users:
                print(f"      ç”Ÿæˆç”¨æˆ·{user_id}çš„å›¾åƒ...")
                images_dir = self._generate_wrong_condition_images(user_id, num_images=2)
                if images_dir:
                    user_images[user_id] = images_dir

            # ç”¨ç›®æ ‡ç”¨æˆ·çš„åˆ†ç±»å™¨éªŒè¯æ‰€æœ‰ç”Ÿæˆå›¾åƒ
            validation_matrix = {}
            for generated_user_id, images_dir in user_images.items():
                result = self.validation_system.validate_generated_images(
                    user_id=target_user,  # å§‹ç»ˆç”¨ç›®æ ‡ç”¨æˆ·çš„åˆ†ç±»å™¨
                    generated_images_dir=images_dir,
                    confidence_threshold=self.config.confidence_threshold
                )
                validation_matrix[generated_user_id] = result.get('success_rate', 0)

            # åˆ†æç»“æœ
            target_success_rate = validation_matrix.get(target_user, 0)
            other_success_rates = [rate for uid, rate in validation_matrix.items() if uid != target_user]

            if other_success_rates:
                avg_other_success_rate = sum(other_success_rates) / len(other_success_rates)
                max_other_success_rate = max(other_success_rates)

                # è®¡ç®—å„ç§å¯¹æ¯”æŒ‡æ ‡
                avg_ratio = target_success_rate / (avg_other_success_rate + 1e-6)
                max_ratio = target_success_rate / (max_other_success_rate + 1e-6)

                # ä¸¥æ ¼æ ‡å‡†ï¼šç›®æ ‡ç”¨æˆ·åº”è¯¥æ˜æ˜¾å¥½äºæ‰€æœ‰å…¶ä»–ç”¨æˆ·
                matrix_success = (
                    target_success_rate >= 0.6 and  # ç›®æ ‡ç”¨æˆ·æˆåŠŸç‡è¶³å¤Ÿé«˜
                    avg_ratio >= 2.0 and           # å¹³å‡æ¯”å…¶ä»–ç”¨æˆ·å¥½2å€
                    max_ratio >= 1.5                # æ¯”æœ€å¥½çš„å…¶ä»–ç”¨æˆ·ä¹Ÿè¦å¥½1.5å€
                )

                return {
                    'validation_matrix': validation_matrix,
                    'target_user_success_rate': target_success_rate,
                    'avg_other_success_rate': avg_other_success_rate,
                    'max_other_success_rate': max_other_success_rate,
                    'avg_ratio': avg_ratio,
                    'max_ratio': max_ratio,
                    'matrix_validation_success': matrix_success,
                    'criteria': {
                        'min_target_success_rate': 0.6,
                        'min_avg_ratio': 2.0,
                        'min_max_ratio': 1.5
                    }
                }
            else:
                return {'error': 'No other users to compare'}

        except Exception as e:
            print(f"    âŒ çŸ©é˜µéªŒè¯å¤±è´¥: {e}")
            return {'error': str(e)}

    def _evaluate_overall_success(self, basic_result: Dict, control_result: Dict, matrix_result: Dict = None) -> Dict:
        """è¯„ä¼°æ•´ä½“éªŒè¯æˆåŠŸæ€§"""
        # åŸºç¡€éªŒè¯æŒ‡æ ‡
        basic_success_rate = basic_result.get('success_rate', 0)
        basic_avg_confidence = basic_result.get('avg_confidence', 0)

        # å¯¹æ¯”æ§åˆ¶æŒ‡æ ‡
        control_effective = control_result.get('control_effective', False)
        condition_ratio = control_result.get('condition_control_ratio', 0)

        # çŸ©é˜µéªŒè¯æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        matrix_success = True  # é»˜è®¤é€šè¿‡
        if matrix_result and 'matrix_validation_success' in matrix_result:
            matrix_success = matrix_result.get('matrix_validation_success', False)

        # ç»¼åˆè¯„ä¼°ï¼ˆæ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼‰
        overall_success = (
            basic_success_rate >= 0.6 and
            basic_avg_confidence >= 0.7 and
            control_effective and
            matrix_success  # å¦‚æœæœ‰çŸ©é˜µéªŒè¯ï¼Œä¹Ÿå¿…é¡»é€šè¿‡
        )

        result = {
            'overall_success': overall_success,
            'basic_success_rate': basic_success_rate,
            'basic_avg_confidence': basic_avg_confidence,
            'condition_control_effective': control_effective,
            'condition_control_ratio': condition_ratio,
            'evaluation_criteria': {
                'min_success_rate': 0.6,
                'min_avg_confidence': 0.7,
                'min_control_ratio': 2.0
            }
        }

        # æ·»åŠ çŸ©é˜µéªŒè¯ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if matrix_result and 'matrix_validation_success' in matrix_result:
            result['matrix_validation_success'] = matrix_success
            result['evaluation_criteria']['matrix_validation_required'] = True

        return result

    def run_full_pipeline(self, generate_images: bool = True) -> Dict:
        """è¿è¡Œå®Œæ•´çš„éªŒè¯æµç¨‹"""
        print(f"ğŸš€ å¼€å§‹å®Œæ•´éªŒè¯æµç¨‹")
        print(f"ç›®æ ‡ç”¨æˆ·: {self.config.target_user_id}")
        print(f"è¾“å‡ºç›®å½•: {self.config.output_dir}")
        print("=" * 60)

        results = {
            "target_user_id": self.config.target_user_id,
            "config": self.config.__dict__,
            "classifier_trained": False,
            "images_generated": False,
            "validation_completed": False,
            "success": False
        }

        # æ­¥éª¤1: è®­ç»ƒåˆ†ç±»å™¨
        if not self.train_classifier():
            print("âŒ åˆ†ç±»å™¨è®­ç»ƒå¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return results

        results["classifier_trained"] = True

        # æ­¥éª¤2: ç”Ÿæˆå›¾åƒ (å¯é€‰)
        generated_dir = None
        if generate_images:
            if not self.load_models():
                print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡å›¾åƒç”Ÿæˆ")
            else:
                generated_dir = self.generate_images()
                if generated_dir:
                    results["images_generated"] = True
                    results["generated_images_dir"] = generated_dir

        # æ­¥éª¤3: éªŒè¯å›¾åƒ
        if generated_dir:
            validation_result = self.validate_generated_images(generated_dir)
            if validation_result:
                results["validation_completed"] = True
                results["validation_result"] = validation_result

                # åˆ¤æ–­æ•´ä½“æˆåŠŸ
                success_rate = validation_result.get('success_rate', 0)
                avg_confidence = validation_result.get('avg_confidence', 0)

                if success_rate >= 0.6 and avg_confidence >= 0.8:
                    results["success"] = True
                    print(f"ğŸ‰ éªŒè¯æˆåŠŸ! æˆåŠŸç‡: {success_rate:.2f}, å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                else:
                    print(f"âš ï¸  éªŒè¯ç»“æœä¸ç†æƒ³. æˆåŠŸç‡: {success_rate:.2f}, å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                    print(f"ğŸ’¡ å»ºè®®: å°è¯•æ›´å¤šæ¨ç†æ­¥æ•° (num_inference_steps > {self.config.num_inference_steps})")

        return results

def main():
    """ä¸»å‡½æ•° - ç°ä»£åŒ–çš„å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(
        description="ç°ä»£åŒ–çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹éªŒè¯ç³»ç»Ÿ",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument("--target_user_id", type=int, required=True,
                       help="ç›®æ ‡ç”¨æˆ·ID")
    parser.add_argument("--real_data_root", type=str, required=True,
                       help="çœŸå®æ•°æ®æ ¹ç›®å½•")

    # åŸºæœ¬é…ç½®
    parser.add_argument("--output_dir", type=str, default="./validation_results",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¡ç®—è®¾å¤‡ (auto/cuda/cpu)")

    # åˆ†ç±»å™¨é…ç½®
    parser.add_argument("--classifier_epochs", type=int, default=30,
                       help="åˆ†ç±»å™¨è®­ç»ƒè½®æ•°")
    parser.add_argument("--classifier_batch_size", type=int, default=32,
                       help="åˆ†ç±»å™¨æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--classifier_lr", type=float, default=5e-4,
                       help="åˆ†ç±»å™¨å­¦ä¹ ç‡")
    parser.add_argument("--max_samples_per_class", type=int, default=1000,
                       help="æ¯ç±»æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--confidence_threshold", type=float, default=0.8,
                       help="ç½®ä¿¡åº¦é˜ˆå€¼")


    # ç”Ÿæˆé…ç½®
    parser.add_argument("--generate_images", action="store_true",
                       help="æ˜¯å¦ç”Ÿæˆå›¾åƒ")
    parser.add_argument("--num_images_to_generate", type=int, default=100,
                       help="ç”Ÿæˆå›¾åƒæ•°é‡ (å»ºè®®100+å¼ è·å¾—å¯é ç»Ÿè®¡ç»“æœ)")
    parser.add_argument("--num_inference_steps", type=int, default=50,
                       help="DDIMæ¨ç†æ­¥æ•° (å»ºè®®50-200)")
    parser.add_argument("--batch_size", type=int, default=10,
                       help="æ‰¹é‡ç”Ÿæˆå¤§å° (æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œå»ºè®®8-16)")

    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--vae_path", type=str,
                       help="VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--unet_path", type=str,
                       help="UNetæ¨¡å‹è·¯å¾„")
    parser.add_argument("--condition_encoder_path", type=str,
                       help="æ¡ä»¶ç¼–ç å™¨è·¯å¾„")

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®
    config = ValidationConfig(
        target_user_id=args.target_user_id,
        real_data_root=args.real_data_root,
        output_dir=args.output_dir,
        classifier_epochs=args.classifier_epochs,
        classifier_batch_size=args.classifier_batch_size,
        classifier_lr=args.classifier_lr,
        max_samples_per_class=args.max_samples_per_class,
        confidence_threshold=args.confidence_threshold,
        num_images_to_generate=args.num_images_to_generate,
        num_inference_steps=args.num_inference_steps,
        batch_size=args.batch_size,
        vae_path=args.vae_path,
        unet_path=args.unet_path,
        condition_encoder_path=args.condition_encoder_path,
        device=args.device
    )

    # æ‰“å°é…ç½®
    print("ğŸ”§ éªŒè¯é…ç½®:")
    print(f"  ç›®æ ‡ç”¨æˆ·: {config.target_user_id}")
    print(f"  æ•°æ®ç›®å½•: {config.real_data_root}")
    print(f"  è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"  åˆ†ç±»å™¨: æ ‡å‡†ResNet-18, epochs={config.classifier_epochs}, batch_size={config.classifier_batch_size}")
    if args.generate_images:
        print(f"  ç”Ÿæˆ: çº¯æ¡ä»¶ç”Ÿæˆ, steps={config.num_inference_steps}, batch_size={config.batch_size}")
        print(f"  æ¨¡å‹: VAE={config.vae_path is not None}, UNet={config.unet_path is not None}")
    print("=" * 60)

    # åˆ›å»ºéªŒè¯å™¨å¹¶è¿è¡Œ
    validator = ConditionalDiffusionValidator(config)
    results = validator.run_full_pipeline(generate_images=args.generate_images)

    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“‹ éªŒè¯ç»“æœæ€»ç»“:")
    print(f"  åˆ†ç±»å™¨è®­ç»ƒ: {'âœ…' if results['classifier_trained'] else 'âŒ'}")
    if args.generate_images:
        print(f"  å›¾åƒç”Ÿæˆ: {'âœ…' if results['images_generated'] else 'âŒ'}")
        print(f"  éªŒè¯å®Œæˆ: {'âœ…' if results['validation_completed'] else 'âŒ'}")
        print(f"  æ•´ä½“æˆåŠŸ: {'ğŸ‰' if results['success'] else 'âš ï¸'}")

    # ä¿å­˜å®Œæ•´ç»“æœ
    result_file = Path(config.output_dir) / f"user_{config.target_user_id:02d}_complete_results.json"
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)

    print(f"\nğŸ“„ å®Œæ•´ç»“æœä¿å­˜åœ¨: {result_file}")

    if results.get('success'):
        print("ğŸ‰ éªŒè¯æˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("âš ï¸  éªŒè¯æœªå®Œå…¨æˆåŠŸï¼Œè¯·æ£€æŸ¥ç»“æœå¹¶è°ƒæ•´å‚æ•°")
        return 1

if __name__ == "__main__":
    exit(main())
