#!/usr/bin/env python3
"""
åŸºäºç»Ÿè®¡çš„éªŒè¯å™¨ - å½“ç‰¹å¾ç›¸ä¼¼ä¸”æ•°æ®é‡å°‘æ—¶çš„æ›¿ä»£æ–¹æ¡ˆ
ä¸ä¾èµ–æ·±åº¦å­¦ä¹ ï¼Œä½¿ç”¨ç»Ÿè®¡æ–¹æ³•éªŒè¯ç”Ÿæˆå›¾åƒçš„åˆç†æ€§
"""

import numpy as np
from PIL import Image
from pathlib import Path
from typing import List, Dict, Tuple
import cv2
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

class StatisticalValidator:
    """åŸºäºç»Ÿè®¡çš„éªŒè¯å™¨"""
    
    def __init__(self):
        self.user_statistics = {}
        self.pca = None
        self.user_distributions = {}
    
    def extract_statistical_features(self, image_path: str) -> np.ndarray:
        """æå–å›¾åƒçš„ç»Ÿè®¡ç‰¹å¾"""
        # åŠ è½½å›¾åƒ
        img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
        if img is None:
            img = np.array(Image.open(image_path).convert('L'))
        
        features = []
        
        # 1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features.extend([
            np.mean(img),           # å‡å€¼
            np.std(img),            # æ ‡å‡†å·®
            np.var(img),            # æ–¹å·®
            stats.skew(img.flatten()),  # ååº¦
            stats.kurtosis(img.flatten()),  # å³°åº¦
        ])
        
        # 2. ç›´æ–¹å›¾ç‰¹å¾
        hist, _ = np.histogram(img, bins=32, range=(0, 256))
        hist = hist / np.sum(hist)  # å½’ä¸€åŒ–
        features.extend(hist.tolist())
        
        # 3. çº¹ç†ç‰¹å¾ (GLCM)
        try:
            from skimage.feature import graycomatrix, graycoprops
            # è®¡ç®—ç°åº¦å…±ç”ŸçŸ©é˜µ
            glcm = graycomatrix(img, distances=[1], angles=[0, 45, 90, 135], 
                              levels=256, symmetric=True, normed=True)
            
            # æå–çº¹ç†ç‰¹å¾
            contrast = graycoprops(glcm, 'contrast').flatten()
            dissimilarity = graycoprops(glcm, 'dissimilarity').flatten()
            homogeneity = graycoprops(glcm, 'homogeneity').flatten()
            energy = graycoprops(glcm, 'energy').flatten()
            
            features.extend(contrast.tolist())
            features.extend(dissimilarity.tolist())
            features.extend(homogeneity.tolist())
            features.extend(energy.tolist())
        except ImportError:
            # å¦‚æœæ²¡æœ‰skimageï¼Œä½¿ç”¨ç®€å•çš„çº¹ç†ç‰¹å¾
            # è®¡ç®—æ¢¯åº¦ç‰¹å¾
            grad_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)
            
            features.extend([
                np.mean(np.abs(grad_x)),
                np.std(grad_x),
                np.mean(np.abs(grad_y)),
                np.std(grad_y),
            ])
        
        # 4. é¢‘åŸŸç‰¹å¾
        f_transform = np.fft.fft2(img)
        f_shift = np.fft.fftshift(f_transform)
        magnitude_spectrum = np.log(np.abs(f_shift) + 1)
        
        features.extend([
            np.mean(magnitude_spectrum),
            np.std(magnitude_spectrum),
            np.max(magnitude_spectrum),
            np.min(magnitude_spectrum),
        ])
        
        # 5. å½¢çŠ¶ç‰¹å¾ï¼ˆè½®å»“ï¼‰
        contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(largest_contour)
            perimeter = cv2.arcLength(largest_contour, True)
            
            if perimeter > 0:
                circularity = 4 * np.pi * area / (perimeter * perimeter)
                features.append(circularity)
            else:
                features.append(0)
        else:
            features.append(0)
        
        return np.array(features)
    
    def load_user_statistics(self, data_root: str) -> Dict[int, List[np.ndarray]]:
        """åŠ è½½æ‰€æœ‰ç”¨æˆ·çš„ç»Ÿè®¡ç‰¹å¾"""
        print("ğŸ“Š æå–ç”¨æˆ·ç»Ÿè®¡ç‰¹å¾...")
        
        data_path = Path(data_root)
        user_features = {}
        
        for user_dir in data_path.iterdir():
            if user_dir.is_dir() and user_dir.name.startswith('ID_'):
                try:
                    user_id = int(user_dir.name.split('_')[1])
                    images = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                    
                    if images:
                        features = []
                        for img_path in images:
                            try:
                                feat = self.extract_statistical_features(img_path)
                                features.append(feat)
                            except Exception as e:
                                print(f"    è­¦å‘Š: æ— æ³•å¤„ç†å›¾åƒ {img_path}: {e}")
                                continue
                        
                        if features:
                            user_features[user_id] = features
                            print(f"  ç”¨æˆ· {user_id}: {len(features)} å¼ å›¾åƒçš„ç‰¹å¾")
                
                except ValueError:
                    continue
        
        return user_features
    
    def compute_user_distributions(self, user_features: Dict[int, List[np.ndarray]]):
        """è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„ç‰¹å¾åˆ†å¸ƒ"""
        print("ğŸ“ˆ è®¡ç®—ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ...")
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾è¿›è¡ŒPCAé™ç»´
        all_features = []
        user_labels = []
        
        for user_id, features in user_features.items():
            all_features.extend(features)
            user_labels.extend([user_id] * len(features))
        
        all_features = np.array(all_features)
        
        # PCAé™ç»´åˆ°50ç»´ï¼ˆä¿ç•™ä¸»è¦ä¿¡æ¯ï¼‰
        self.pca = PCA(n_components=min(50, all_features.shape[1]))
        all_features_pca = self.pca.fit_transform(all_features)
        
        print(f"  PCAé™ç»´: {all_features.shape[1]} -> {all_features_pca.shape[1]}")
        print(f"  è§£é‡Šæ–¹å·®æ¯”: {self.pca.explained_variance_ratio_.sum():.3f}")
        
        # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„åˆ†å¸ƒå‚æ•°
        start_idx = 0
        for user_id, features in user_features.items():
            end_idx = start_idx + len(features)
            user_features_pca = all_features_pca[start_idx:end_idx]
            
            # è®¡ç®—å‡å€¼å’Œåæ–¹å·®
            mean = np.mean(user_features_pca, axis=0)
            cov = np.cov(user_features_pca.T)
            
            self.user_distributions[user_id] = {
                'mean': mean,
                'cov': cov,
                'features': user_features_pca
            }
            
            start_idx = end_idx
        
        # è®¡ç®—ç”¨æˆ·é—´çš„å¯åˆ†ç¦»æ€§
        self._analyze_user_separability(all_features_pca, user_labels)
    
    def _analyze_user_separability(self, features: np.ndarray, labels: List[int]):
        """åˆ†æç”¨æˆ·é—´çš„å¯åˆ†ç¦»æ€§"""
        print("ğŸ” åˆ†æç”¨æˆ·å¯åˆ†ç¦»æ€§...")
        
        # è®¡ç®—è½®å»“ç³»æ•°
        if len(np.unique(labels)) > 1:
            silhouette_avg = silhouette_score(features, labels)
            print(f"  è½®å»“ç³»æ•°: {silhouette_avg:.3f} (>0.5ä¸ºå¥½ï¼Œ>0.7ä¸ºå¾ˆå¥½)")
            
            if silhouette_avg < 0.3:
                print("  âš ï¸  ç”¨æˆ·é—´ç‰¹å¾ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œåˆ†ç±»å¯èƒ½å›°éš¾")
            elif silhouette_avg < 0.5:
                print("  âš ï¸  ç”¨æˆ·é—´ç‰¹å¾æœ‰ä¸€å®šç›¸ä¼¼æ€§ï¼Œéœ€è¦è°¨æ…éªŒè¯")
            else:
                print("  âœ… ç”¨æˆ·é—´ç‰¹å¾æœ‰è¾ƒå¥½çš„å¯åˆ†ç¦»æ€§")
        
        # ä½¿ç”¨t-SNEå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
        try:
            if len(features) < 1000:  # æ•°æ®é‡ä¸å¤§æ—¶æ‰åšt-SNE
                tsne = TSNE(n_components=2, random_state=42)
                features_2d = tsne.fit_transform(features)
                
                plt.figure(figsize=(10, 8))
                unique_labels = np.unique(labels)
                colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))
                
                for label, color in zip(unique_labels, colors):
                    mask = np.array(labels) == label
                    plt.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                              c=[color], label=f'User {label}', alpha=0.6)
                
                plt.title('ç”¨æˆ·ç‰¹å¾åˆ†å¸ƒ (t-SNE)')
                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
                plt.tight_layout()
                plt.savefig('user_feature_distribution.png', dpi=150, bbox_inches='tight')
                plt.close()
                print("  ğŸ“Š ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜: user_feature_distribution.png")
        except Exception as e:
            print(f"  æ³¨æ„: t-SNEå¯è§†åŒ–å¤±è´¥: {e}")
    
    def validate_generated_images(self, target_user_id: int, 
                                generated_images_dir: str) -> Dict:
        """éªŒè¯ç”Ÿæˆå›¾åƒçš„ç»Ÿè®¡åˆç†æ€§"""
        print(f"\nğŸ” ç»Ÿè®¡éªŒè¯ç”Ÿæˆå›¾åƒ (ç”¨æˆ· {target_user_id})")
        
        if target_user_id not in self.user_distributions:
            return {'error': f'ç”¨æˆ· {target_user_id} çš„åˆ†å¸ƒä¿¡æ¯ä¸å­˜åœ¨'}
        
        # åŠ è½½ç”Ÿæˆå›¾åƒ
        gen_dir = Path(generated_images_dir)
        image_files = list(gen_dir.glob("*.png")) + list(gen_dir.glob("*.jpg"))
        
        if not image_files:
            return {'error': 'No generated images found'}
        
        print(f"  æ‰¾åˆ° {len(image_files)} å¼ ç”Ÿæˆå›¾åƒ")
        
        # æå–ç”Ÿæˆå›¾åƒçš„ç‰¹å¾
        gen_features = []
        for img_path in image_files:
            try:
                feat = self.extract_statistical_features(img_path)
                gen_features.append(feat)
            except Exception as e:
                print(f"    è­¦å‘Š: æ— æ³•å¤„ç†ç”Ÿæˆå›¾åƒ {img_path}: {e}")
                continue
        
        if not gen_features:
            return {'error': 'No valid generated images'}
        
        gen_features = np.array(gen_features)
        
        # PCAå˜æ¢
        gen_features_pca = self.pca.transform(gen_features)
        
        # è·å–ç›®æ ‡ç”¨æˆ·çš„åˆ†å¸ƒ
        target_dist = self.user_distributions[target_user_id]
        target_mean = target_dist['mean']
        target_cov = target_dist['cov']
        
        # è®¡ç®—é©¬æ°è·ç¦»
        try:
            inv_cov = np.linalg.pinv(target_cov)  # ä½¿ç”¨ä¼ªé€†é¿å…å¥‡å¼‚çŸ©é˜µ
            mahalanobis_distances = []
            
            for gen_feat in gen_features_pca:
                diff = gen_feat - target_mean
                distance = np.sqrt(diff.T @ inv_cov @ diff)
                mahalanobis_distances.append(distance)
            
            mahalanobis_distances = np.array(mahalanobis_distances)
            
            # è®¡ç®—ç›®æ ‡ç”¨æˆ·çœŸå®å›¾åƒçš„é©¬æ°è·ç¦»åˆ†å¸ƒä½œä¸ºå‚è€ƒ
            real_distances = []
            for real_feat in target_dist['features']:
                diff = real_feat - target_mean
                distance = np.sqrt(diff.T @ inv_cov @ diff)
                real_distances.append(distance)
            
            real_distances = np.array(real_distances)
            
            # ç»Ÿè®¡åˆ†æ
            # ä½¿ç”¨Kolmogorov-Smirnovæ£€éªŒæ¯”è¾ƒåˆ†å¸ƒ
            ks_statistic, p_value = stats.ks_2samp(mahalanobis_distances, real_distances)
            
            # è®¡ç®—ç”Ÿæˆå›¾åƒåœ¨çœŸå®åˆ†å¸ƒä¸­çš„ç™¾åˆ†ä½æ•°
            percentiles = [stats.percentileofscore(real_distances, d) for d in mahalanobis_distances]
            
            # åˆç†æ€§è¯„ä¼°ï¼šç”Ÿæˆå›¾åƒåº”è¯¥åœ¨çœŸå®åˆ†å¸ƒçš„åˆç†èŒƒå›´å†…
            reasonable_count = sum(1 for p in percentiles if 5 <= p <= 95)  # åœ¨5%-95%èŒƒå›´å†…
            reasonable_rate = reasonable_count / len(percentiles)
            
            result = {
                'reasonable_rate': reasonable_rate,
                'avg_mahalanobis_distance': np.mean(mahalanobis_distances),
                'real_avg_mahalanobis_distance': np.mean(real_distances),
                'ks_statistic': ks_statistic,
                'ks_p_value': p_value,
                'distribution_similar': p_value > 0.05,  # p>0.05è¡¨ç¤ºåˆ†å¸ƒç›¸ä¼¼
                'percentiles': percentiles,
                'total_images': len(mahalanobis_distances)
            }
            
            print(f"  ğŸ“Š éªŒè¯ç»“æœ:")
            print(f"    åˆç†æ€§æ¯”ç‡: {reasonable_rate:.3f}")
            print(f"    åˆ†å¸ƒç›¸ä¼¼æ€§: {'æ˜¯' if result['distribution_similar'] else 'å¦'} (p={p_value:.3f})")
            print(f"    å¹³å‡é©¬æ°è·ç¦»: ç”Ÿæˆ={np.mean(mahalanobis_distances):.3f}, çœŸå®={np.mean(real_distances):.3f}")
            
            return result
            
        except Exception as e:
            print(f"    âŒ é©¬æ°è·ç¦»è®¡ç®—å¤±è´¥: {e}")
            return {'error': f'Statistical analysis failed: {e}'}

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    validator = StatisticalValidator()
    
    # åŠ è½½ç”¨æˆ·ç»Ÿè®¡ç‰¹å¾
    user_features = validator.load_user_statistics("data/processed")
    
    # è®¡ç®—ç”¨æˆ·åˆ†å¸ƒ
    validator.compute_user_distributions(user_features)
    
    # éªŒè¯ç”Ÿæˆå›¾åƒ
    result = validator.validate_generated_images(
        target_user_id=1,
        generated_images_dir="validation_results/generated_images"
    )
