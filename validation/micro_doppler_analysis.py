#!/usr/bin/env python3
"""
ä¸“é—¨é’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„åˆ†æ
åˆ†æç”¨æˆ·é—´çš„æ­¥æ€ç‰¹å¾å·®å¼‚
"""

import os
import sys
import numpy as np
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

def analyze_micro_doppler_characteristics():
    """åˆ†æå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„ç‰¹å¾"""
    print("ğŸ¯ å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ç‰¹å¾åˆ†æ")
    print("=" * 50)
    
    print("ğŸ“Š å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ç‰¹ç‚¹:")
    print("  1. æ—¶é—´-é¢‘ç‡åŸŸè¡¨ç¤º")
    print("  2. åæ˜ äººä½“æ­¥æ€çš„å¤šæ™®å‹’é¢‘ç§»")
    print("  3. ä¸»è¦å·®å¼‚åœ¨æ­¥æ€æ¨¡å¼çš„ç»†å¾®å˜åŒ–")
    print("  4. èº¯å¹²ã€æ‰‹è‡‚ã€è…¿éƒ¨è¿åŠ¨çš„é¢‘ç‡ç‰¹å¾")
    
    print("\nğŸ” ç”¨æˆ·é—´å·®å¼‚æ¥æº:")
    print("  1. æ­¥é¢‘å·®å¼‚ (walking cadence)")
    print("  2. æ­¥å¹…å·®å¼‚ (stride length)")
    print("  3. æ‰‹è‡‚æ‘†åŠ¨æ¨¡å¼")
    print("  4. èº«é«˜ä½“é‡å½±å“çš„é¢‘ç‡åˆ†å¸ƒ")
    print("  5. ä¸ªäººæ­¥æ€ä¹ æƒ¯")
    
    print("\nâš ï¸  å¾®å¤šæ™®å‹’æ•°æ®çš„æŒ‘æˆ˜:")
    print("  1. ç”¨æˆ·é—´å·®å¼‚ç¡®å®è¾ƒå°")
    print("  2. ä½†å·®å¼‚æ˜¯æœ‰æ„ä¹‰çš„ç”Ÿç‰©ç‰¹å¾")
    print("  3. éœ€è¦ç²¾ç»†çš„ç‰¹å¾å­¦ä¹ ")
    print("  4. å¯¹å™ªå£°æ•æ„Ÿ")
    
    return True

def analyze_frequency_domain_differences(data_root: str, user_ids: list = [1, 2]):
    """åˆ†æé¢‘åŸŸå·®å¼‚"""
    print(f"\nğŸ”¬ åˆ†æé¢‘åŸŸç‰¹å¾å·®å¼‚")
    
    def load_and_analyze_spectrogram(user_id):
        # æŸ¥æ‰¾ç”¨æˆ·ç›®å½•
        from validation.analyze_user_differences import find_user_directory, load_and_analyze_images
        
        user_dir = find_user_directory(data_root, user_id)
        if not user_dir:
            return None
        
        images = load_and_analyze_images(user_dir, max_samples=10)
        if len(images) == 0:
            return None

        print(f"    åŠ è½½äº† {len(images)} å¼ å›¾åƒï¼Œå½¢çŠ¶: {images.shape}")

        # åˆ†æé¢‘åŸŸç‰¹å¾
        mean_img = np.mean(images, axis=0)
        print(f"    å¹³å‡å›¾åƒå½¢çŠ¶: {mean_img.shape}")

        # ç¡®ä¿å›¾åƒæ˜¯2Dçš„ï¼ˆç°åº¦å›¾æˆ–RGBçš„ä¸€ä¸ªé€šé“ï¼‰
        if len(mean_img.shape) == 3:
            # å¦‚æœæ˜¯RGBå›¾åƒï¼Œè½¬æ¢ä¸ºç°åº¦
            mean_img = np.mean(mean_img, axis=2)
            print(f"    è½¬æ¢ä¸ºç°åº¦å›¾ï¼Œæ–°å½¢çŠ¶: {mean_img.shape}")

        # è®¡ç®—é¢‘ç‡è½´å’Œæ—¶é—´è½´çš„èƒ½é‡åˆ†å¸ƒ
        if len(mean_img.shape) == 2:
            freq_profile = np.mean(mean_img, axis=1)  # æ²¿æ—¶é—´è½´å¹³å‡
            time_profile = np.mean(mean_img, axis=0)  # æ²¿é¢‘ç‡è½´å¹³å‡
        else:
            print(f"    âš ï¸  å›¾åƒç»´åº¦å¼‚å¸¸: {mean_img.shape}")
            return None
        
        # è®¡ç®—ä¸»è¦é¢‘ç‡æˆåˆ†
        if len(freq_profile) > 0:
            freq_peak_idx = np.argmax(freq_profile)
            freq_peak_value = freq_profile[freq_peak_idx]
        else:
            freq_peak_idx = 0
            freq_peak_value = 0.0

        # è®¡ç®—é¢‘ç‡åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹å¾
        if len(freq_profile) > 0 and np.sum(freq_profile) > 0:
            freq_centroid = np.sum(freq_profile * np.arange(len(freq_profile))) / np.sum(freq_profile)
            freq_spread = np.sqrt(np.sum(((np.arange(len(freq_profile)) - freq_centroid) ** 2) * freq_profile) / np.sum(freq_profile))
        else:
            freq_centroid = 0.0
            freq_spread = 0.0
        
        return {
            'mean_img': mean_img,
            'freq_profile': freq_profile,
            'time_profile': time_profile,
            'freq_peak_idx': freq_peak_idx,
            'freq_peak_value': freq_peak_value,
            'freq_centroid': freq_centroid,
            'freq_spread': freq_spread,
            'num_images': len(images)
        }
    
    # åˆ†ææ¯ä¸ªç”¨æˆ·
    user_features = {}
    for user_id in user_ids:
        print(f"  åˆ†æç”¨æˆ· {user_id}...")
        features = load_and_analyze_spectrogram(user_id)
        if features:
            user_features[user_id] = features
            print(f"    âœ… é¢‘ç‡é‡å¿ƒ: {features['freq_centroid']:.2f}")
            print(f"    âœ… é¢‘ç‡æ‰©æ•£: {features['freq_spread']:.2f}")
            print(f"    âœ… å³°å€¼ä½ç½®: {features['freq_peak_idx']}")
        else:
            print(f"    âŒ æ— æ³•åŠ è½½ç”¨æˆ· {user_id} æ•°æ®")
    
    if len(user_features) < 2:
        print("âŒ æœ‰æ•ˆç”¨æˆ·æ•°æ®ä¸è¶³")
        return False
    
    # æ¯”è¾ƒç”¨æˆ·é—´å·®å¼‚
    print(f"\nğŸ“Š ç”¨æˆ·é—´é¢‘åŸŸå·®å¼‚:")
    user_list = list(user_features.keys())
    
    for i in range(len(user_list)):
        for j in range(i + 1, len(user_list)):
            user1, user2 = user_list[i], user_list[j]
            f1, f2 = user_features[user1], user_features[user2]
            
            # é¢‘ç‡é‡å¿ƒå·®å¼‚
            centroid_diff = abs(f1['freq_centroid'] - f2['freq_centroid'])
            
            # é¢‘ç‡æ‰©æ•£å·®å¼‚
            spread_diff = abs(f1['freq_spread'] - f2['freq_spread'])
            
            # å³°å€¼ä½ç½®å·®å¼‚
            peak_diff = abs(f1['freq_peak_idx'] - f2['freq_peak_idx'])
            
            # é¢‘ç‡åˆ†å¸ƒç›¸å…³æ€§
            correlation = np.corrcoef(f1['freq_profile'], f2['freq_profile'])[0, 1]
            
            print(f"  ç”¨æˆ· {user1} vs ç”¨æˆ· {user2}:")
            print(f"    é¢‘ç‡é‡å¿ƒå·®å¼‚: {centroid_diff:.3f}")
            print(f"    é¢‘ç‡æ‰©æ•£å·®å¼‚: {spread_diff:.3f}")
            print(f"    å³°å€¼ä½ç½®å·®å¼‚: {peak_diff} åƒç´ ")
            print(f"    é¢‘ç‡åˆ†å¸ƒç›¸å…³æ€§: {correlation:.4f}")
            
            # åˆ¤æ–­å·®å¼‚ç¨‹åº¦
            if centroid_diff < 2.0 and spread_diff < 1.0 and correlation > 0.95:
                print(f"    ğŸš¨ é¢‘åŸŸå·®å¼‚æå°ï¼Œæ‰©æ•£æ¨¡å‹å¾ˆéš¾å­¦ä¹ ")
            elif centroid_diff < 5.0 and spread_diff < 3.0 and correlation > 0.9:
                print(f"    âš ï¸  é¢‘åŸŸå·®å¼‚è¾ƒå°ï¼Œéœ€è¦å¼ºåŒ–å­¦ä¹ ")
            else:
                print(f"    âœ… é¢‘åŸŸå·®å¼‚å¯æ£€æµ‹")
    
    return True

def recommend_training_strategy():
    """æ¨èè®­ç»ƒç­–ç•¥"""
    print(f"\nğŸ’¡ å¾®å¤šæ™®å‹’æ•°æ®çš„è®­ç»ƒç­–ç•¥å»ºè®®")
    print("=" * 50)
    
    print("ğŸ¯ ä¸éœ€è¦é‡æ–°è®­ç»ƒVAEçš„æƒ…å†µ:")
    print("  1. VAEå·²ç»èƒ½å¾ˆå¥½åœ°é‡å»ºæ—¶é¢‘å›¾")
    print("  2. æ½œåœ¨ç©ºé—´ä¿æŒäº†é¢‘åŸŸä¿¡æ¯")
    print("  3. é‡å»ºè´¨é‡æ»¡è¶³è¦æ±‚")
    
    print("\nğŸ¯ ä¸éœ€è¦é‡æ–°è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æƒ…å†µ:")
    print("  1. æ¡ä»¶dropout â‰¤ 0.1")
    print("  2. è®­ç»ƒæ—¶é—´è¶³å¤Ÿé•¿")
    print("  3. æŸå¤±æ”¶æ•›è‰¯å¥½")
    
    print("\nğŸ”§ ä¼˜å…ˆå°è¯•çš„ä¼˜åŒ–ç­–ç•¥:")
    print("  1. æé«˜æŒ‡å¯¼å¼ºåº¦ (30-50)")
    print("  2. æ›´å¤šæ¨ç†æ­¥æ•° (100-200)")
    print("  3. æ›´ç²¾ç»†çš„åˆ†ç±»å™¨ (æ›´å¤šæ•°æ®ã€æ›´å¤šè½®æ•°)")
    print("  4. é›†æˆå¤šä¸ªåˆ†ç±»å™¨")
    
    print("\nâš ï¸  éœ€è¦é‡æ–°è®­ç»ƒçš„ä¿¡å·:")
    print("  1. æç«¯å‚æ•°ä¸‹éªŒè¯æˆåŠŸç‡ä» <30%")
    print("  2. æ¡ä»¶ç¼–ç å™¨åµŒå…¥ç›¸ä¼¼åº¦ >0.95")
    print("  3. ç”Ÿæˆå›¾åƒå®Œå…¨æ— å·®å¼‚")
    
    print("\nğŸš€ å»ºè®®çš„æµ‹è¯•é¡ºåº:")
    print("  1. å…ˆç”¨æç«¯å‚æ•°æµ‹è¯• (guidance_scale=35-50)")
    print("  2. åˆ†ææ¡ä»¶ç¼–ç å™¨åµŒå…¥å·®å¼‚")
    print("  3. å¦‚æœä»å¤±è´¥ï¼Œè€ƒè™‘é‡æ–°è®­ç»ƒ")
    print("  4. é‡æ–°è®­ç»ƒæ—¶é™ä½condition_dropoutåˆ°0.02")

def create_micro_doppler_test_config():
    """åˆ›å»ºå¾®å¤šæ™®å‹’ä¸“ç”¨æµ‹è¯•é…ç½®"""
    print(f"\nâš™ï¸  å¾®å¤šæ™®å‹’ä¸“ç”¨æµ‹è¯•é…ç½®")
    print("=" * 50)
    
    configs = {
        "conservative": {
            "guidance_scale": 25.0,
            "num_inference_steps": 100,
            "classifier_epochs": 40,
            "classifier_lr": 1e-4,
            "description": "ä¿å®ˆæµ‹è¯• - é€‚åˆåˆæ¬¡éªŒè¯"
        },
        "aggressive": {
            "guidance_scale": 40.0,
            "num_inference_steps": 150,
            "classifier_epochs": 60,
            "classifier_lr": 5e-5,
            "description": "æ¿€è¿›æµ‹è¯• - å¼ºåŒ–å¾®å°å·®å¼‚"
        },
        "extreme": {
            "guidance_scale": 50.0,
            "num_inference_steps": 200,
            "classifier_epochs": 80,
            "classifier_lr": 2e-5,
            "description": "æç«¯æµ‹è¯• - æœ€å¤§åŒ–ç‰¹å¾ä¿æŒ"
        }
    }
    
    for name, config in configs.items():
        print(f"\nğŸ“‹ {config['description']} ({name}):")
        for key, value in config.items():
            if key != 'description':
                print(f"  {key}: {value}")
    
    return configs

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åˆ†æ")
    parser.add_argument("--action", choices=["analyze", "frequency", "recommend"], required=True)
    parser.add_argument("--data_root", type=str, help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--user_ids", type=int, nargs='+', default=[1, 2], help="ç”¨æˆ·IDåˆ—è¡¨")
    
    args = parser.parse_args()
    
    if args.action == "analyze":
        analyze_micro_doppler_characteristics()
        create_micro_doppler_test_config()
        
    elif args.action == "frequency":
        if not args.data_root:
            print("âŒ éœ€è¦æä¾›æ•°æ®æ ¹ç›®å½•")
            return
        analyze_frequency_domain_differences(args.data_root, args.user_ids)
        
    elif args.action == "recommend":
        recommend_training_strategy()

if __name__ == "__main__":
    main()
