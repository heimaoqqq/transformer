#!/usr/bin/env python3
"""
é’ˆå¯¹çƒ­åŠ›å›¾æ•°æ®çš„ä¸“é—¨è§£å†³æ–¹æ¡ˆ
åŸºäºç”¨æˆ·å±•ç¤ºçš„ID_1å’ŒID_2çƒ­åŠ›å›¾ï¼Œæä¾›é’ˆå¯¹æ€§çš„éªŒè¯ç­–ç•¥
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

def analyze_heatmap_characteristics():
    """åˆ†æçƒ­åŠ›å›¾æ•°æ®çš„ç‰¹ç‚¹"""
    print("ğŸ”¥ çƒ­åŠ›å›¾æ•°æ®ç‰¹å¾åˆ†æ")
    print("=" * 50)
    
    print("ğŸ“Š è§‚å¯Ÿåˆ°çš„ç‰¹å¾:")
    print("  1. éƒ½æ˜¯è“-ç»¿-é»„-çº¢çš„çƒ­åŠ›å›¾")
    print("  2. ä¸»è¦å·®å¼‚åœ¨ç»†å¾®çš„æ¨¡å¼å˜åŒ–")
    print("  3. é¢œè‰²åˆ†å¸ƒå’Œæ•´ä½“ç»“æ„å¾ˆç›¸ä¼¼")
    print("  4. å·®å¼‚ä¸»è¦ä½“ç°åœ¨å±€éƒ¨ç»†èŠ‚")
    
    print("\nğŸš¨ è¿™ç§æ•°æ®çš„æŒ‘æˆ˜:")
    print("  1. ç”¨æˆ·é—´å·®å¼‚æå°ï¼Œè‚‰çœ¼éƒ½éš¾ä»¥åŒºåˆ†")
    print("  2. æ‰©æ•£æ¨¡å‹å€¾å‘äºç”Ÿæˆ'å¹³å‡'æ ·æœ¬")
    print("  3. å¾®å°å·®å¼‚å®¹æ˜“è¢«å™ªå£°æ©ç›–")
    print("  4. éœ€è¦æå¼ºçš„æ¡ä»¶æ§åˆ¶æ‰èƒ½ä¿æŒå·®å¼‚")
    
    return True

def create_extreme_guidance_config():
    """åˆ›å»ºæç«¯æŒ‡å¯¼å¼ºåº¦é…ç½®"""
    print("\nğŸš€ é’ˆå¯¹çƒ­åŠ›å›¾æ•°æ®çš„æç«¯é…ç½®")
    print("=" * 50)
    
    configs = {
        "conservative": {
            "guidance_scale": 20.0,
            "num_inference_steps": 100,
            "condition_dropout": 0.05,
            "description": "ä¿å®ˆæ–¹æ¡ˆ - é€‚åˆåˆæ¬¡å°è¯•"
        },
        "aggressive": {
            "guidance_scale": 35.0,
            "num_inference_steps": 150,
            "condition_dropout": 0.02,
            "description": "æ¿€è¿›æ–¹æ¡ˆ - å¼ºåŒ–ç”¨æˆ·ç‰¹å¾"
        },
        "extreme": {
            "guidance_scale": 50.0,
            "num_inference_steps": 200,
            "condition_dropout": 0.01,
            "description": "æç«¯æ–¹æ¡ˆ - æœ€å¤§åŒ–æ¡ä»¶æ§åˆ¶"
        }
    }
    
    for name, config in configs.items():
        print(f"\nğŸ“‹ {config['description']} ({name}):")
        print(f"  æŒ‡å¯¼å¼ºåº¦: {config['guidance_scale']}")
        print(f"  æ¨ç†æ­¥æ•°: {config['num_inference_steps']}")
        print(f"  æ¡ä»¶dropout: {config['condition_dropout']}")
    
    return configs

def generate_with_extreme_guidance(
    vae_path: str,
    unet_path: str, 
    condition_encoder_path: str,
    data_dir: str,
    user_id: int,
    guidance_scale: float = 35.0,
    num_inference_steps: int = 150,
    num_images: int = 8,
    output_dir: str = "extreme_guidance_test"
):
    """ä½¿ç”¨æç«¯æŒ‡å¯¼å¼ºåº¦ç”Ÿæˆå›¾åƒ"""
    print(f"\nğŸ¯ ä½¿ç”¨æç«¯æŒ‡å¯¼å¼ºåº¦ç”Ÿæˆç”¨æˆ· {user_id} çš„å›¾åƒ")
    print(f"æŒ‡å¯¼å¼ºåº¦: {guidance_scale}")
    print(f"æ¨ç†æ­¥æ•°: {num_inference_steps}")
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler
        from training.train_diffusion import UserConditionEncoder, create_user_id_mapping
        
        # è®¾å¤‡è®¾ç½®
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ“‚ åŠ è½½æ¨¡å‹...")
        vae = AutoencoderKL.from_pretrained(vae_path)
        unet = UNet2DConditionModel.from_pretrained(unet_path)
        
        # åˆ›å»ºç”¨æˆ·IDæ˜ å°„
        user_id_mapping = create_user_id_mapping(data_dir)
        num_users = len(user_id_mapping)
        
        # åŠ è½½æ¡ä»¶ç¼–ç å™¨
        condition_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=unet.config.cross_attention_dim
        )
        
        try:
            condition_encoder.load_state_dict(torch.load(condition_encoder_path, map_location='cpu'))
            print("âœ… æˆåŠŸåŠ è½½æ¡ä»¶ç¼–ç å™¨æƒé‡")
        except Exception as e:
            print(f"âš ï¸  ä½¿ç”¨éšæœºæƒé‡: {e}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        vae = vae.to(device)
        unet = unet.to(device)
        condition_encoder = condition_encoder.to(device)
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = DDPMScheduler(
            num_train_timesteps=1000,
            beta_start=0.00085,
            beta_end=0.012,
            beta_schedule="scaled_linear",
            clip_sample=False,
            set_alpha_to_one=False,
        )
        scheduler.set_timesteps(num_inference_steps)
        
        # å‡†å¤‡ç”Ÿæˆ
        if user_id not in user_id_mapping:
            print(f"âŒ ç”¨æˆ· {user_id} ä¸åœ¨æ˜ å°„ä¸­: {list(user_id_mapping.keys())}")
            return False
        
        user_idx = user_id_mapping[user_id]
        print(f"ç”¨æˆ· {user_id} æ˜ å°„åˆ°ç´¢å¼• {user_idx}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå›¾åƒ
        print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆ {num_images} å¼ å›¾åƒ...")
        
        with torch.no_grad():
            for i in range(num_images):
                print(f"  ç”Ÿæˆç¬¬ {i+1}/{num_images} å¼ ...")
                
                # éšæœºå™ªå£°
                latents = torch.randn(1, 4, 32, 32, device=device)
                
                # ç”¨æˆ·æ¡ä»¶
                user_tensor = torch.tensor([user_idx], device=device)
                user_embedding = condition_encoder(user_tensor)
                
                # æ‰©æ•£è¿‡ç¨‹
                latents = latents * scheduler.init_noise_sigma
                
                for t in scheduler.timesteps:
                    # æœ‰æ¡ä»¶é¢„æµ‹
                    noise_pred_cond = unet(
                        latents, 
                        t, 
                        encoder_hidden_states=user_embedding
                    ).sample
                    
                    # æ— æ¡ä»¶é¢„æµ‹ï¼ˆä½¿ç”¨é›¶åµŒå…¥ï¼‰
                    zero_embedding = torch.zeros_like(user_embedding)
                    noise_pred_uncond = unet(
                        latents,
                        t,
                        encoder_hidden_states=zero_embedding
                    ).sample
                    
                    # åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ - ä½¿ç”¨æé«˜çš„æŒ‡å¯¼å¼ºåº¦
                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
                    
                    # è°ƒåº¦å™¨æ­¥éª¤
                    latents = scheduler.step(noise_pred, t, latents).prev_sample
                
                # è§£ç ä¸ºå›¾åƒ
                latents = 1 / 0.18215 * latents
                images = vae.decode(latents).sample
                images = (images / 2 + 0.5).clamp(0, 1)
                
                # ä¿å­˜å›¾åƒ
                from torchvision.utils import save_image
                save_path = output_path / f"user_{user_id}_extreme_guidance_{i+1}.png"
                save_image(images, save_path)
                
                print(f"    ä¿å­˜åˆ°: {save_path}")
        
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå›¾åƒä¿å­˜åœ¨: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def validate_extreme_guidance_results(
    generated_dir: str,
    real_data_root: str,
    user_id: int,
    classifier_path: str = None
):
    """éªŒè¯æç«¯æŒ‡å¯¼å¼ºåº¦çš„ç»“æœ"""
    print(f"\nğŸ” éªŒè¯æç«¯æŒ‡å¯¼å¼ºåº¦çš„ç”Ÿæˆç»“æœ")
    
    # å¦‚æœæ²¡æœ‰åˆ†ç±»å™¨ï¼Œå…ˆè®­ç»ƒä¸€ä¸ª
    if not classifier_path or not Path(classifier_path).exists():
        print("ğŸ“š è®­ç»ƒåˆ†ç±»å™¨...")
        from validation.improved_single_user_validation import train_user_classifier
        
        classifier_path = f"extreme_guidance_classifier_user_{user_id}.pth"
        success = train_user_classifier(
            target_user_id=user_id,
            real_data_root=real_data_root,
            output_dir="extreme_guidance_validation",
            max_samples_per_class=500,  # ä½¿ç”¨æ›´å¤šæ•°æ®
            epochs=30,                  # æ›´å¤šè®­ç»ƒè½®æ•°
            batch_size=16
        )
        
        if not success:
            print("âŒ åˆ†ç±»å™¨è®­ç»ƒå¤±è´¥")
            return False
    
    # éªŒè¯ç”Ÿæˆå›¾åƒ
    print("ğŸ¯ éªŒè¯ç”Ÿæˆå›¾åƒ...")
    from validation.improved_single_user_validation import validate_generated_images
    
    results = validate_generated_images(
        generated_images_dir=generated_dir,
        classifier_path=classifier_path,
        target_user_id=user_id
    )
    
    return results

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="çƒ­åŠ›å›¾æ•°æ®ä¸“é—¨è§£å†³æ–¹æ¡ˆ")
    parser.add_argument("--action", choices=["analyze", "generate", "validate"], required=True)
    parser.add_argument("--user_id", type=int, default=1)
    parser.add_argument("--data_dir", type=str, help="æ•°æ®ç›®å½•")
    parser.add_argument("--vae_path", type=str, help="VAEè·¯å¾„")
    parser.add_argument("--unet_path", type=str, help="UNetè·¯å¾„")
    parser.add_argument("--condition_encoder_path", type=str, help="æ¡ä»¶ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--guidance_scale", type=float, default=35.0, help="æŒ‡å¯¼å¼ºåº¦")
    parser.add_argument("--num_inference_steps", type=int, default=150, help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--num_images", type=int, default=8, help="ç”Ÿæˆå›¾åƒæ•°é‡")
    parser.add_argument("--output_dir", type=str, default="extreme_guidance_test")
    
    args = parser.parse_args()
    
    if args.action == "analyze":
        analyze_heatmap_characteristics()
        create_extreme_guidance_config()
        
    elif args.action == "generate":
        if not all([args.vae_path, args.unet_path, args.condition_encoder_path, args.data_dir]):
            print("âŒ ç”Ÿæˆéœ€è¦æä¾›æ‰€æœ‰æ¨¡å‹è·¯å¾„å’Œæ•°æ®ç›®å½•")
            return
        
        success = generate_with_extreme_guidance(
            vae_path=args.vae_path,
            unet_path=args.unet_path,
            condition_encoder_path=args.condition_encoder_path,
            data_dir=args.data_dir,
            user_id=args.user_id,
            guidance_scale=args.guidance_scale,
            num_inference_steps=args.num_inference_steps,
            num_images=args.num_images,
            output_dir=args.output_dir
        )
        
        if success:
            print(f"\nğŸ¯ ä¸‹ä¸€æ­¥: éªŒè¯ç”Ÿæˆç»“æœ")
            print(f"python validation/heatmap_specific_solution.py --action validate --user_id {args.user_id} --data_dir {args.data_dir}")
    
    elif args.action == "validate":
        if not all([args.data_dir]):
            print("âŒ éªŒè¯éœ€è¦æä¾›æ•°æ®ç›®å½•")
            return
        
        validate_extreme_guidance_results(
            generated_dir=args.output_dir,
            real_data_root=args.data_dir,
            user_id=args.user_id
        )

if __name__ == "__main__":
    main()
