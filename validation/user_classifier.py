#!/usr/bin/env python3
"""
ç”¨æˆ·éªŒè¯åˆ†ç±»å™¨
ä½¿ç”¨ResNet-18ä¸ºæ¯ä¸ªç”¨æˆ·è®­ç»ƒç‹¬ç«‹çš„äºŒåˆ†ç±»å™¨
éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦åŒ…å«å¯¹åº”ç”¨æˆ·çš„ç‰¹å¾ä¿¡æ¯
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import resnet18
import numpy as np
from PIL import Image
from pathlib import Path
import json
from typing import List, Tuple, Dict
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime

class UserImageDataset(Dataset):
    """ç”¨æˆ·å›¾åƒæ•°æ®é›†"""
    
    def __init__(self, image_paths: List[str], labels: List[int], transform=None):
        """
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ (0: è´Ÿæ ·æœ¬, 1: æ­£æ ·æœ¬)
            transform: å›¾åƒå˜æ¢
        """
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        
        assert len(image_paths) == len(labels), "å›¾åƒæ•°é‡ä¸æ ‡ç­¾æ•°é‡ä¸åŒ¹é…"
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_path = self.image_paths[idx]
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åŠ è½½å›¾åƒ {image_path}: {e}")
            # åˆ›å»ºä¸€ä¸ªé»‘è‰²å›¾åƒä½œä¸ºå¤‡ç”¨
            image = Image.new('RGB', (64, 64), (0, 0, 0))
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        
        label = self.labels[idx]
        return image, label

class UserClassifier(nn.Module):
    """æ ‡å‡†çš„ç”¨æˆ·åˆ†ç±»å™¨ - ä½¿ç”¨æˆç†Ÿçš„ResNetæ¶æ„"""

    def __init__(self, num_classes=2, pretrained=True):
        """
        Args:
            num_classes: åˆ†ç±»æ•°é‡ (2: æ˜¯/ä¸æ˜¯è¯¥ç”¨æˆ·)
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        """
        super(UserClassifier, self).__init__()

        # ä½¿ç”¨æ ‡å‡†çš„ResNet-18
        self.backbone = resnet18(pretrained=pretrained)

        # æ›¿æ¢æœ€åçš„åˆ†ç±»å±‚ä¸ºæ ‡å‡†åšæ³•
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_features, num_classes)

    def forward(self, x):
        # æ ‡å‡†çš„ResNetå‰å‘ä¼ æ’­
        return self.backbone(x)

class MicroDopplerCNN(nn.Module):
    """ä¸“é—¨ä¸ºå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾è®¾è®¡çš„è½»é‡çº§CNN"""

    def __init__(self, num_classes=2, dropout_rate=0.5):
        """
        ä¸“é—¨é’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„ç‰¹å¾è®¾è®¡
        å…³æ³¨æ—¶é—´-é¢‘ç‡åŸŸçš„å±€éƒ¨æ¨¡å¼
        """
        super(MicroDopplerCNN, self).__init__()

        # ç‰¹å¾æå–å±‚ - ä¸“é—¨æ•è·æ—¶é¢‘å›¾ç‰¹å¾
        self.features = nn.Sequential(
            # ç¬¬ä¸€ç»„ï¼šæ•è·ç²—ç²’åº¦æ—¶é¢‘ç‰¹å¾
            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),

            # ç¬¬äºŒç»„ï¼šæ•è·ä¸­ç­‰ç²’åº¦ç‰¹å¾
            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # ç¬¬ä¸‰ç»„ï¼šæ•è·ç»†ç²’åº¦ç‰¹å¾
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # ç¬¬å››ç»„ï¼šé«˜çº§ç‰¹å¾
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((4, 4))  # è‡ªé€‚åº”æ± åŒ–åˆ°å›ºå®šå°ºå¯¸
        )

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(512, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate * 0.25),
            nn.Linear(128, num_classes)
        )

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # ç‰¹å¾æå–
        features = self.features(x)  # [batch_size, 256, 4, 4]

        # å±•å¹³
        features = features.view(features.size(0), -1)  # [batch_size, 256*4*4]

        # åˆ†ç±»
        output = self.classifier(features)

        return output

class UserValidationSystem:
    """ç”¨æˆ·éªŒè¯ç³»ç»Ÿ"""
    
    def __init__(self, device="auto"):
        """
        Args:
            device: è®¡ç®—è®¾å¤‡
        """
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ ‡å‡†çš„å›¾åƒé¢„å¤„ç† - ä¸ä½¿ç”¨æ•°æ®å¢å¼º
        self.transform = transforms.Compose([
            transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),  # æ ‡å‡†ImageNetå°ºå¯¸
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNetæ ‡å‡†åŒ–
        ])

        # éªŒè¯æ—¶ä½¿ç”¨ç›¸åŒçš„å˜æ¢
        self.val_transform = self.transform
        
        # å­˜å‚¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨
        self.classifiers = {}
    
    def prepare_user_data(self, user_id: int, real_images_dir: str, other_users_dirs: List[str],
                         max_samples_per_class: int = 500, negative_ratio: float = 3.0) -> Tuple[List[str], List[int]]:
        """
        ä¸ºæŒ‡å®šç”¨æˆ·å‡†å¤‡è®­ç»ƒæ•°æ®

        Args:
            user_id: ç”¨æˆ·ID
            real_images_dir: è¯¥ç”¨æˆ·çœŸå®å›¾åƒç›®å½•
            other_users_dirs: å…¶ä»–ç”¨æˆ·å›¾åƒç›®å½•åˆ—è¡¨
            max_samples_per_class: æ­£æ ·æœ¬æœ€å¤§æ•°é‡
            negative_ratio: è´Ÿæ ·æœ¬ä¸æ­£æ ·æœ¬çš„æ¯”ä¾‹ (é»˜è®¤3:1)

        Returns:
            (image_paths, labels): å›¾åƒè·¯å¾„åˆ—è¡¨å’Œæ ‡ç­¾åˆ—è¡¨
        """
        image_paths = []
        labels = []
        
        # æ­£æ ·æœ¬: è¯¥ç”¨æˆ·çš„çœŸå®å›¾åƒ
        real_dir = Path(real_images_dir)
        if real_dir.exists():
            real_images = list(real_dir.glob("*.png")) + list(real_dir.glob("*.jpg"))
            real_images = real_images[:max_samples_per_class]  # é™åˆ¶æ ·æœ¬æ•°é‡
            
            image_paths.extend([str(p) for p in real_images])
            labels.extend([1] * len(real_images))
            
            print(f"  ç”¨æˆ· {user_id} æ­£æ ·æœ¬: {len(real_images)} å¼ ")
        else:
            print(f"  è­¦å‘Š: ç”¨æˆ· {user_id} çœŸå®å›¾åƒç›®å½•ä¸å­˜åœ¨: {real_dir}")
        
        # è´Ÿæ ·æœ¬: å…¶ä»–ç”¨æˆ·çš„å›¾åƒ (æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥)
        max_negative_samples = int(len(real_images) * negative_ratio)
        print(f"  ç›®æ ‡è´Ÿæ ·æœ¬æ•°é‡: {max_negative_samples} å¼  (æ¯”ä¾‹ {negative_ratio}:1)")

        # æ”¶é›†æ‰€æœ‰å…¶ä»–ç”¨æˆ·çš„å›¾åƒ
        all_negative_images = []
        for other_dir in other_users_dirs:
            other_path = Path(other_dir)
            if other_path.exists():
                other_images = list(other_path.glob("*.png")) + list(other_path.glob("*.jpg"))
                all_negative_images.extend(other_images)

        # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬ï¼Œç¡®ä¿ä»£è¡¨æ€§
        import random
        if len(all_negative_images) > max_negative_samples:
            selected_negative = random.sample(all_negative_images, max_negative_samples)
        else:
            selected_negative = all_negative_images
            print(f"  è­¦å‘Š: å¯ç”¨è´Ÿæ ·æœ¬({len(all_negative_images)})å°‘äºç›®æ ‡æ•°é‡({max_negative_samples})")

        # æ·»åŠ è´Ÿæ ·æœ¬
        image_paths.extend([str(p) for p in selected_negative])
        labels.extend([0] * len(selected_negative))
        negative_count = len(selected_negative)
        
        print(f"  ç”¨æˆ· {user_id} è´Ÿæ ·æœ¬: {negative_count} å¼ ")
        print(f"  ç”¨æˆ· {user_id} æ€»æ ·æœ¬: {len(image_paths)} å¼ ")
        
        return image_paths, labels
    
    def train_user_classifier(self, user_id: int, image_paths: List[str], labels: List[int],
                            epochs: int = 20, batch_size: int = 32, learning_rate: float = 1e-3,
                            validation_split: float = 0.2) -> Dict:
        """
        è®­ç»ƒç”¨æˆ·åˆ†ç±»å™¨
        
        Args:
            user_id: ç”¨æˆ·ID
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            è®­ç»ƒå†å²å­—å…¸
        """
        print(f"\nğŸ¯ è®­ç»ƒç”¨æˆ· {user_id} çš„åˆ†ç±»å™¨...")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        total_samples = len(image_paths)
        val_size = int(total_samples * validation_split)
        train_size = total_samples - val_size
        
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = np.random.permutation(total_samples)
        train_indices = indices[:train_size]
        val_indices = indices[train_size:]
        
        train_paths = [image_paths[i] for i in train_indices]
        train_labels = [labels[i] for i in train_indices]
        val_paths = [image_paths[i] for i in val_indices]
        val_labels = [labels[i] for i in val_indices]
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = UserImageDataset(train_paths, train_labels, self.transform)
        val_dataset = UserImageDataset(val_paths, val_labels, self.transform)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        
        # ä½¿ç”¨æ ‡å‡†çš„ResNet-18åˆ†ç±»å™¨
        model = UserClassifier(num_classes=2, pretrained=True)
        print(f"  ğŸ—ï¸  ä½¿ç”¨æ ‡å‡†ResNet-18åˆ†ç±»å™¨")

        model.to(self.device)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"  ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        
        # æ ‡å‡†çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)

        # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼ˆæ›´ç°ä»£çš„è°ƒåº¦ç­–ç•¥ï¼‰
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
        
        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        best_val_acc = 0.0
        best_model_state = None

        # æ—©åœæœºåˆ¶
        patience = 10  # 10ä¸ªepochæ²¡æœ‰æ”¹å–„å°±åœæ­¢
        patience_counter = 0
        min_improvement = 0.001  # æœ€å°æ”¹å–„é˜ˆå€¼ï¼Œé¿å…å¾®å°æ³¢åŠ¨è§¦å‘æ—©åœ
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for images, labels_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
                images = images.to(self.device)
                labels_batch = labels_batch.to(self.device)
                
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels_batch)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels_batch.size(0)
                train_correct += (predicted == labels_batch).sum().item()
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for images, labels_batch in val_loader:
                    images = images.to(self.device)
                    labels_batch = labels_batch.to(self.device)
                    
                    outputs = model(images)
                    loss = criterion(outputs, labels_batch)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += labels_batch.size(0)
                    val_correct += (predicted == labels_batch).sum().item()
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            train_loss_avg = train_loss / len(train_loader)
            train_acc = train_correct / train_total
            val_loss_avg = val_loss / len(val_loader)
            val_acc = val_correct / val_total
            
            # ä¿å­˜å†å²
            history['train_loss'].append(train_loss_avg)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss_avg)
            history['val_acc'].append(val_acc)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            # æ—©åœå’Œæœ€ä½³æ¨¡å‹ä¿å­˜ï¼ˆä¿®å¤é€»è¾‘ï¼‰
            if val_acc > best_val_acc + min_improvement:
                # æ˜¾è‘—æ”¹å–„ï¼šæ›´æ–°æœ€ä½³æ¨¡å‹å¹¶é‡ç½®è®¡æ•°å™¨
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
                patience_counter = 0
                print(f"    âœ… éªŒè¯å‡†ç¡®ç‡æ”¹å–„: {val_acc:.4f} (æœ€ä½³: {best_val_acc:.4f})")
            else:
                # æ²¡æœ‰æ˜¾è‘—æ”¹å–„ï¼šå¢åŠ è®¡æ•°å™¨
                patience_counter += 1
                print(f"    â³ æ— æ”¹å–„è®¡æ•°: {patience_counter}/{patience}")

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

            print(f"  Epoch {epoch+1}: Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}, Val Loss: {val_loss_avg:.4f}")

            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"  ğŸ›‘ æ—©åœè§¦å‘ï¼šè¿ç»­{patience}ä¸ªepochéªŒè¯å‡†ç¡®ç‡æ— æ˜¾è‘—æ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
                print(f"  ğŸ“Š æœ€ç»ˆæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
            print(f"  ğŸ“¥ å·²åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€")
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨æœ€ç»ˆçŠ¶æ€")

        # ä¿å­˜åˆ†ç±»å™¨
        self.classifiers[user_id] = model

        # è®­ç»ƒå®Œæˆæ€»ç»“
        final_epoch = len(history['val_acc'])
        print(f"âœ… ç”¨æˆ· {user_id} åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")
        print(f"  ğŸ“Š è®­ç»ƒè½®æ•°: {final_epoch}/{epochs}")
        print(f"  ğŸ¯ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        print(f"  ğŸ“ˆ æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {history['train_acc'][-1]:.4f}")

        # åˆ¤æ–­è®­ç»ƒè´¨é‡
        if best_val_acc >= 0.85:
            print(f"  ğŸŒŸ è®­ç»ƒè´¨é‡: ä¼˜ç§€")
        elif best_val_acc >= 0.75:
            print(f"  âœ… è®­ç»ƒè´¨é‡: è‰¯å¥½")
        elif best_val_acc >= 0.65:
            print(f"  âš ï¸  è®­ç»ƒè´¨é‡: ä¸€èˆ¬")
        else:
            print(f"  âŒ è®­ç»ƒè´¨é‡: è¾ƒå·®ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æˆ–å¢åŠ è®­ç»ƒè½®æ•°")

        return history

    def validate_generated_images(self, user_id: int, generated_images_dir: str,
                                confidence_threshold: float = 0.8) -> Dict:
        """
        éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦åŒ…å«ç”¨æˆ·ç‰¹å¾

        Args:
            user_id: ç”¨æˆ·ID
            generated_images_dir: ç”Ÿæˆå›¾åƒç›®å½•
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ (>0.8ç®—æˆåŠŸ)

        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        if user_id not in self.classifiers:
            raise ValueError(f"ç”¨æˆ· {user_id} çš„åˆ†ç±»å™¨å°šæœªè®­ç»ƒ")

        print(f"\nğŸ” éªŒè¯ç”¨æˆ· {user_id} çš„ç”Ÿæˆå›¾åƒ...")

        # åŠ è½½ç”Ÿæˆå›¾åƒ
        gen_dir = Path(generated_images_dir)
        if not gen_dir.exists():
            raise FileNotFoundError(f"ç”Ÿæˆå›¾åƒç›®å½•ä¸å­˜åœ¨: {gen_dir}")

        image_files = list(gen_dir.glob("*.png")) + list(gen_dir.glob("*.jpg"))
        if not image_files:
            print(f"  è­¦å‘Š: æœªæ‰¾åˆ°ç”Ÿæˆå›¾åƒ")
            return {}

        print(f"  æ‰¾åˆ° {len(image_files)} å¼ ç”Ÿæˆå›¾åƒ")

        # å‡†å¤‡æ•°æ®
        image_paths = [str(p) for p in image_files]
        dataset = UserImageDataset(image_paths, [0] * len(image_paths), self.transform)
        dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)

        # è·å–åˆ†ç±»å™¨
        model = self.classifiers[user_id]
        model.eval()

        # é¢„æµ‹
        all_predictions = []
        all_confidences = []

        with torch.no_grad():
            for images, _ in tqdm(dataloader, desc="éªŒè¯ç”Ÿæˆå›¾åƒ"):
                images = images.to(self.device)
                outputs = model(images)

                # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
                probabilities = torch.softmax(outputs, dim=1)
                confidences = probabilities[:, 1]  # æ­£ç±»(è¯¥ç”¨æˆ·)çš„ç½®ä¿¡åº¦
                predictions = (confidences > confidence_threshold).int()

                all_predictions.extend(predictions.cpu().numpy())
                all_confidences.extend(confidences.cpu().numpy())

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        all_predictions = np.array(all_predictions)
        all_confidences = np.array(all_confidences)

        success_count = np.sum(all_predictions)
        success_rate = success_count / len(all_predictions)
        avg_confidence = np.mean(all_confidences)
        max_confidence = np.max(all_confidences)
        min_confidence = np.min(all_confidences)

        results = {
            'user_id': user_id,
            'total_images': len(image_files),
            'success_count': int(success_count),
            'success_rate': float(success_rate),
            'avg_confidence': float(avg_confidence),
            'max_confidence': float(max_confidence),
            'min_confidence': float(min_confidence),
            'confidence_threshold': confidence_threshold,
            'individual_confidences': all_confidences.tolist(),
            'individual_predictions': all_predictions.tolist(),
            'image_files': [p.name for p in image_files]
        }

        print(f"  éªŒè¯ç»“æœ:")
        print(f"    æˆåŠŸå›¾åƒ: {success_count}/{len(image_files)} ({success_rate:.1%})")
        print(f"    å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"    ç½®ä¿¡åº¦èŒƒå›´: [{min_confidence:.3f}, {max_confidence:.3f}]")

        return results

    def save_classifier(self, user_id: int, save_path: str):
        """ä¿å­˜åˆ†ç±»å™¨"""
        if user_id not in self.classifiers:
            raise ValueError(f"ç”¨æˆ· {user_id} çš„åˆ†ç±»å™¨å°šæœªè®­ç»ƒ")

        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        torch.save({
            'model_state_dict': self.classifiers[user_id].state_dict(),
            'user_id': user_id,
            'model_class': 'UserClassifier'
        }, save_path)

        print(f"âœ… ç”¨æˆ· {user_id} åˆ†ç±»å™¨å·²ä¿å­˜åˆ°: {save_path}")

    def load_classifier(self, user_id: int, load_path: str):
        """åŠ è½½åˆ†ç±»å™¨"""
        checkpoint = torch.load(load_path, map_location=self.device)

        model = UserClassifier(num_classes=2, pretrained=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()

        self.classifiers[user_id] = model

        print(f"âœ… ç”¨æˆ· {user_id} åˆ†ç±»å™¨å·²ä» {load_path} åŠ è½½")

    def plot_training_history(self, history: Dict, save_path: str = None):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # æŸå¤±æ›²çº¿
        ax1.plot(history['train_loss'], label='Train Loss')
        ax1.plot(history['val_loss'], label='Val Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)

        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(history['train_acc'], label='Train Acc')
        ax2.plot(history['val_acc'], label='Val Acc')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ“Š è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ°: {save_path}")

        plt.show()

    def generate_validation_report(self, results_list: List[Dict], save_path: str = None) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = []
        report.append("# ç”¨æˆ·ç”Ÿæˆå›¾åƒéªŒè¯æŠ¥å‘Š\n")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"ç½®ä¿¡åº¦é˜ˆå€¼: {results_list[0]['confidence_threshold']}\n\n")

        # æ€»ä½“ç»Ÿè®¡
        total_images = sum(r['total_images'] for r in results_list)
        total_success = sum(r['success_count'] for r in results_list)
        overall_success_rate = total_success / total_images if total_images > 0 else 0

        report.append("## æ€»ä½“ç»Ÿè®¡\n")
        report.append(f"- æ€»å›¾åƒæ•°: {total_images}\n")
        report.append(f"- æˆåŠŸå›¾åƒæ•°: {total_success}\n")
        report.append(f"- æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1%}\n\n")

        # å„ç”¨æˆ·è¯¦ç»†ç»“æœ
        report.append("## å„ç”¨æˆ·è¯¦ç»†ç»“æœ\n")
        for result in results_list:
            user_id = result['user_id']
            report.append(f"### ç”¨æˆ· {user_id}\n")
            report.append(f"- å›¾åƒæ•°é‡: {result['total_images']}\n")
            report.append(f"- æˆåŠŸæ•°é‡: {result['success_count']}\n")
            report.append(f"- æˆåŠŸç‡: {result['success_rate']:.1%}\n")
            report.append(f"- å¹³å‡ç½®ä¿¡åº¦: {result['avg_confidence']:.3f}\n")
            report.append(f"- ç½®ä¿¡åº¦èŒƒå›´: [{result['min_confidence']:.3f}, {result['max_confidence']:.3f}]\n\n")

        report_text = "".join(report)

        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")

        return report_text
