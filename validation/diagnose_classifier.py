#!/usr/bin/env python3
"""
è¯Šæ–­åˆ†ç±»å™¨é—®é¢˜
æ£€æŸ¥åˆ†ç±»å™¨æ˜¯å¦æ­£ç¡®è®­ç»ƒå’Œé¢„æµ‹
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from validation.user_classifier import UserValidationSystem

def test_classifier_on_training_data(
    user_id: int,
    classifier_path: str,
    real_data_root: str,
    num_test_samples: int = 20
):
    """
    æµ‹è¯•åˆ†ç±»å™¨åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°
    """
    print(f"ğŸ” æµ‹è¯•ç”¨æˆ· {user_id} åˆ†ç±»å™¨åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°")
    
    # åˆå§‹åŒ–éªŒè¯ç³»ç»Ÿ
    validation_system = UserValidationSystem()
    
    # åŠ è½½åˆ†ç±»å™¨
    try:
        validation_system.load_classifier(user_id, classifier_path)
        print(f"âœ… æˆåŠŸåŠ è½½åˆ†ç±»å™¨")
    except Exception as e:
        print(f"âŒ åŠ è½½åˆ†ç±»å™¨å¤±è´¥: {e}")
        return
    
    # æŸ¥æ‰¾ç”¨æˆ·æ•°æ®
    data_root = Path(real_data_root)
    target_user_dir = None
    other_user_dirs = []
    
    for user_dir in data_root.iterdir():
        if user_dir.is_dir():
            dir_name = user_dir.name
            if dir_name == f"ID_{user_id}":
                target_user_dir = str(user_dir)
            elif dir_name.startswith('ID_'):
                try:
                    other_user_id = int(dir_name.split('_')[1])
                    if other_user_id != user_id:
                        other_user_dirs.append(str(user_dir))
                except ValueError:
                    continue
    
    if target_user_dir is None:
        print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ· {user_id} çš„æ•°æ®ç›®å½•")
        return
    
    print(f"âœ… æ‰¾åˆ°ç”¨æˆ·æ•°æ®ç›®å½•: {target_user_dir}")
    print(f"âœ… æ‰¾åˆ° {len(other_user_dirs)} ä¸ªå…¶ä»–ç”¨æˆ·ç›®å½•")
    
    # æµ‹è¯•æ­£æ ·æœ¬ (è¯¥ç”¨æˆ·çš„çœŸå®å›¾åƒ)
    print(f"\nğŸ“Š æµ‹è¯•æ­£æ ·æœ¬ (ç”¨æˆ· {user_id} çš„çœŸå®å›¾åƒ):")
    
    user_images = list(Path(target_user_dir).glob("*.png")) + list(Path(target_user_dir).glob("*.jpg"))
    test_positive = user_images[:num_test_samples]
    
    positive_confidences = []
    for img_path in test_positive:
        try:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            image_tensor = validation_system.transform(image).unsqueeze(0).to(validation_system.device)
            
            # é¢„æµ‹
            with torch.no_grad():
                model = validation_system.classifiers[user_id]
                outputs = model(image_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence = probabilities[0, 1].item()  # æ­£ç±»ç½®ä¿¡åº¦
                positive_confidences.append(confidence)
                
        except Exception as e:
            print(f"  âš ï¸  å¤„ç†å›¾åƒå¤±è´¥ {img_path}: {e}")
    
    if positive_confidences:
        avg_pos_conf = np.mean(positive_confidences)
        print(f"  æ­£æ ·æœ¬æ•°é‡: {len(positive_confidences)}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_pos_conf:.3f}")
        print(f"  ç½®ä¿¡åº¦èŒƒå›´: [{min(positive_confidences):.3f}, {max(positive_confidences):.3f}]")
        print(f"  é«˜ç½®ä¿¡åº¦(>0.8)æ¯”ä¾‹: {sum(1 for c in positive_confidences if c > 0.8)/len(positive_confidences):.1%}")
    
    # æµ‹è¯•è´Ÿæ ·æœ¬ (å…¶ä»–ç”¨æˆ·çš„å›¾åƒ)
    print(f"\nğŸ“Š æµ‹è¯•è´Ÿæ ·æœ¬ (å…¶ä»–ç”¨æˆ·çš„å›¾åƒ):")
    
    negative_confidences = []
    for other_dir in other_user_dirs[:3]:  # åªæµ‹è¯•å‰3ä¸ªå…¶ä»–ç”¨æˆ·
        other_images = list(Path(other_dir).glob("*.png")) + list(Path(other_dir).glob("*.jpg"))
        test_negative = other_images[:5]  # æ¯ä¸ªç”¨æˆ·æµ‹è¯•5å¼ 
        
        for img_path in test_negative:
            try:
                # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                image = Image.open(img_path).convert('RGB')
                image_tensor = validation_system.transform(image).unsqueeze(0).to(validation_system.device)
                
                # é¢„æµ‹
                with torch.no_grad():
                    model = validation_system.classifiers[user_id]
                    outputs = model(image_tensor)
                    probabilities = torch.softmax(outputs, dim=1)
                    confidence = probabilities[0, 1].item()  # æ­£ç±»ç½®ä¿¡åº¦
                    negative_confidences.append(confidence)
                    
            except Exception as e:
                print(f"  âš ï¸  å¤„ç†å›¾åƒå¤±è´¥ {img_path}: {e}")
    
    if negative_confidences:
        avg_neg_conf = np.mean(negative_confidences)
        print(f"  è´Ÿæ ·æœ¬æ•°é‡: {len(negative_confidences)}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_neg_conf:.3f}")
        print(f"  ç½®ä¿¡åº¦èŒƒå›´: [{min(negative_confidences):.3f}, {max(negative_confidences):.3f}]")
        print(f"  ä½ç½®ä¿¡åº¦(<0.2)æ¯”ä¾‹: {sum(1 for c in negative_confidences if c < 0.2)/len(negative_confidences):.1%}")
    
    # åˆ†æåˆ†ç±»å™¨æ€§èƒ½
    print(f"\nğŸ“ˆ åˆ†ç±»å™¨æ€§èƒ½åˆ†æ:")
    if positive_confidences and negative_confidences:
        separation = avg_pos_conf - avg_neg_conf
        print(f"  æ­£è´Ÿæ ·æœ¬ç½®ä¿¡åº¦å·®å¼‚: {separation:.3f}")
        
        if separation > 0.5:
            print(f"  âœ… åˆ†ç±»å™¨åŒºåˆ†åº¦è‰¯å¥½")
        elif separation > 0.2:
            print(f"  âš ï¸  åˆ†ç±»å™¨åŒºåˆ†åº¦ä¸€èˆ¬")
        else:
            print(f"  âŒ åˆ†ç±»å™¨åŒºåˆ†åº¦å¾ˆå·®")
            
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„åˆ†ç±»è¾¹ç•Œ
        pos_high = sum(1 for c in positive_confidences if c > 0.8)
        neg_low = sum(1 for c in negative_confidences if c < 0.2)
        
        print(f"  æ­£æ ·æœ¬é«˜ç½®ä¿¡åº¦(>0.8): {pos_high}/{len(positive_confidences)} ({pos_high/len(positive_confidences):.1%})")
        print(f"  è´Ÿæ ·æœ¬ä½ç½®ä¿¡åº¦(<0.2): {neg_low}/{len(negative_confidences)} ({neg_low/len(negative_confidences):.1%})")
        
        if pos_high/len(positive_confidences) > 0.7 and neg_low/len(negative_confidences) > 0.7:
            print(f"  âœ… åˆ†ç±»å™¨è®­ç»ƒè‰¯å¥½")
        else:
            print(f"  âŒ åˆ†ç±»å™¨å¯èƒ½è®­ç»ƒä¸è¶³æˆ–è¿‡æ‹Ÿåˆ")

def test_classifier_on_generated_images(
    user_id: int,
    classifier_path: str,
    generated_images_dir: str
):
    """
    æµ‹è¯•åˆ†ç±»å™¨åœ¨ç”Ÿæˆå›¾åƒä¸Šçš„è¡¨ç°ï¼Œå¹¶æ˜¾ç¤ºä¸€äº›å›¾åƒ
    """
    print(f"\nğŸ¨ æµ‹è¯•ç”¨æˆ· {user_id} åˆ†ç±»å™¨åœ¨ç”Ÿæˆå›¾åƒä¸Šçš„è¡¨ç°")
    
    # åˆå§‹åŒ–éªŒè¯ç³»ç»Ÿ
    validation_system = UserValidationSystem()
    
    # åŠ è½½åˆ†ç±»å™¨
    validation_system.load_classifier(user_id, classifier_path)
    
    # æŸ¥æ‰¾ç”Ÿæˆå›¾åƒ
    gen_dir = Path(generated_images_dir)
    if not gen_dir.exists():
        print(f"âŒ ç”Ÿæˆå›¾åƒç›®å½•ä¸å­˜åœ¨: {gen_dir}")
        return
    
    image_files = list(gen_dir.glob("*.png")) + list(gen_dir.glob("*.jpg"))
    if not image_files:
        print(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆå›¾åƒ")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(image_files)} å¼ ç”Ÿæˆå›¾åƒ")
    
    # æµ‹è¯•æ‰€æœ‰ç”Ÿæˆå›¾åƒ
    generated_confidences = []
    image_details = []
    
    for img_path in image_files:
        try:
            # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
            image = Image.open(img_path).convert('RGB')
            image_tensor = validation_system.transform(image).unsqueeze(0).to(validation_system.device)
            
            # é¢„æµ‹
            with torch.no_grad():
                model = validation_system.classifiers[user_id]
                outputs = model(image_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence = probabilities[0, 1].item()  # æ­£ç±»ç½®ä¿¡åº¦
                generated_confidences.append(confidence)
                image_details.append((img_path.name, confidence))
                
        except Exception as e:
            print(f"  âš ï¸  å¤„ç†å›¾åƒå¤±è´¥ {img_path}: {e}")
    
    if generated_confidences:
        avg_gen_conf = np.mean(generated_confidences)
        print(f"  ç”Ÿæˆå›¾åƒæ•°é‡: {len(generated_confidences)}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_gen_conf:.3f}")
        print(f"  ç½®ä¿¡åº¦èŒƒå›´: [{min(generated_confidences):.3f}, {max(generated_confidences):.3f}]")
        
        # æ˜¾ç¤ºæ¯å¼ å›¾åƒçš„ç½®ä¿¡åº¦
        print(f"\nğŸ“‹ å„å›¾åƒç½®ä¿¡åº¦è¯¦æƒ…:")
        for name, conf in sorted(image_details, key=lambda x: x[1], reverse=True):
            status = "âœ…" if conf > 0.8 else "âš ï¸" if conf > 0.3 else "âŒ"
            print(f"    {status} {name}: {conf:.3f}")
        
        # åˆ†æç½®ä¿¡åº¦åˆ†å¸ƒ
        high_conf = sum(1 for c in generated_confidences if c > 0.8)
        medium_conf = sum(1 for c in generated_confidences if 0.3 < c <= 0.8)
        low_conf = sum(1 for c in generated_confidences if c <= 0.3)
        
        print(f"\nğŸ“Š ç½®ä¿¡åº¦åˆ†å¸ƒ:")
        print(f"  é«˜ç½®ä¿¡åº¦ (>0.8): {high_conf}/{len(generated_confidences)} ({high_conf/len(generated_confidences):.1%})")
        print(f"  ä¸­ç½®ä¿¡åº¦ (0.3-0.8): {medium_conf}/{len(generated_confidences)} ({medium_conf/len(generated_confidences):.1%})")
        print(f"  ä½ç½®ä¿¡åº¦ (â‰¤0.3): {low_conf}/{len(generated_confidences)} ({low_conf/len(generated_confidences):.1%})")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯Šæ–­åˆ†ç±»å™¨é—®é¢˜")
    parser.add_argument("--user_id", type=int, required=True, help="ç”¨æˆ·ID")
    parser.add_argument("--classifier_path", type=str, required=True, help="åˆ†ç±»å™¨è·¯å¾„")
    parser.add_argument("--real_data_root", type=str, required=True, help="çœŸå®æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--generated_images_dir", type=str, help="ç”Ÿæˆå›¾åƒç›®å½•")
    parser.add_argument("--num_test_samples", type=int, default=20, help="æµ‹è¯•æ ·æœ¬æ•°é‡")
    
    args = parser.parse_args()
    
    print("ğŸ” åˆ†ç±»å™¨è¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    # æµ‹è¯•åˆ†ç±»å™¨åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¡¨ç°
    test_classifier_on_training_data(
        user_id=args.user_id,
        classifier_path=args.classifier_path,
        real_data_root=args.real_data_root,
        num_test_samples=args.num_test_samples
    )
    
    # å¦‚æœæä¾›äº†ç”Ÿæˆå›¾åƒç›®å½•ï¼Œä¹Ÿæµ‹è¯•ç”Ÿæˆå›¾åƒ
    if args.generated_images_dir:
        test_classifier_on_generated_images(
            user_id=args.user_id,
            classifier_path=args.classifier_path,
            generated_images_dir=args.generated_images_dir
        )
    
    print("\n" + "=" * 50)
    print("ğŸ¯ è¯Šæ–­å»ºè®®:")
    print("1. å¦‚æœåˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šè¡¨ç°å·®ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ")
    print("2. å¦‚æœåˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šè¡¨ç°å¥½ï¼Œä½†ç”Ÿæˆå›¾åƒä¸Šè¡¨ç°å·®ï¼Œè¯´æ˜ç”Ÿæˆå›¾åƒè´¨é‡æœ‰é—®é¢˜")
    print("3. æ£€æŸ¥å›¾åƒé¢„å¤„ç†æ˜¯å¦ä¸€è‡´")
    print("4. æ£€æŸ¥ç”¨æˆ·IDæ˜ å°„æ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    main()
