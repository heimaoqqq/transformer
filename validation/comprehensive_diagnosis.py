#!/usr/bin/env python3
"""
å…¨é¢è¯Šæ–­éªŒè¯å¤±è´¥çš„æ ¹æœ¬åŸå› 
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from validation.user_classifier import UserValidationSystem

def analyze_real_data_diversity(data_root: str, target_user_id: int, num_samples: int = 10):
    """
    åˆ†æçœŸå®æ•°æ®çš„å¤šæ ·æ€§å’Œç”¨æˆ·é—´å·®å¼‚
    """
    print(f"ğŸ“Š åˆ†æçœŸå®æ•°æ®çš„ç”¨æˆ·é—´å·®å¼‚")
    
    data_path = Path(data_root)
    
    # æ”¶é›†æ‰€æœ‰ç”¨æˆ·çš„æ ·æœ¬
    user_samples = {}
    for user_dir in data_path.iterdir():
        if user_dir.is_dir() and user_dir.name.startswith('ID_'):
            try:
                user_id = int(user_dir.name.split('_')[1])
                images = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                if images:
                    user_samples[user_id] = images[:num_samples]
            except ValueError:
                continue
    
    print(f"  æ‰¾åˆ° {len(user_samples)} ä¸ªç”¨æˆ·")
    
    # åˆ†æç›®æ ‡ç”¨æˆ·ä¸å…¶ä»–ç”¨æˆ·çš„å·®å¼‚
    if target_user_id not in user_samples:
        print(f"âŒ ç›®æ ‡ç”¨æˆ· {target_user_id} ä¸å­˜åœ¨")
        return False
    
    target_images = user_samples[target_user_id]
    print(f"  ç›®æ ‡ç”¨æˆ· {target_user_id}: {len(target_images)} å¼ å›¾åƒ")
    
    # ç®€å•çš„åƒç´ çº§å·®å¼‚åˆ†æ
    validation_system = UserValidationSystem()
    
    def load_and_process(img_path):
        try:
            image = Image.open(img_path).convert('RGB')
            tensor = validation_system.transform(image)
            return tensor.numpy().flatten()
        except:
            return None
    
    # åŠ è½½ç›®æ ‡ç”¨æˆ·å›¾åƒ
    target_vectors = []
    for img_path in target_images:
        vec = load_and_process(img_path)
        if vec is not None:
            target_vectors.append(vec)
    
    if not target_vectors:
        print(f"âŒ æ— æ³•åŠ è½½ç›®æ ‡ç”¨æˆ·å›¾åƒ")
        return False
    
    target_vectors = np.array(target_vectors)
    target_mean = np.mean(target_vectors, axis=0)
    
    # è®¡ç®—ä¸å…¶ä»–ç”¨æˆ·çš„å·®å¼‚
    user_distances = {}
    for other_user_id, other_images in user_samples.items():
        if other_user_id == target_user_id:
            continue
            
        other_vectors = []
        for img_path in other_images[:5]:  # æ¯ä¸ªç”¨æˆ·å–5å¼ 
            vec = load_and_process(img_path)
            if vec is not None:
                other_vectors.append(vec)
        
        if other_vectors:
            other_vectors = np.array(other_vectors)
            other_mean = np.mean(other_vectors, axis=0)
            distance = np.linalg.norm(target_mean - other_mean)
            user_distances[other_user_id] = distance
    
    if user_distances:
        avg_distance = np.mean(list(user_distances.values()))
        min_distance = min(user_distances.values())
        max_distance = max(user_distances.values())
        
        print(f"  ç”¨æˆ·é—´åƒç´ çº§å·®å¼‚:")
        print(f"    å¹³å‡è·ç¦»: {avg_distance:.2f}")
        print(f"    æœ€å°è·ç¦»: {min_distance:.2f}")
        print(f"    æœ€å¤§è·ç¦»: {max_distance:.2f}")
        
        if avg_distance < 50:
            print(f"  âš ï¸  ç”¨æˆ·é—´å·®å¼‚å¾ˆå°ï¼Œå¯èƒ½éš¾ä»¥åŒºåˆ†")
        elif avg_distance < 100:
            print(f"  ğŸ“Š ç”¨æˆ·é—´å·®å¼‚ä¸­ç­‰")
        else:
            print(f"  âœ… ç”¨æˆ·é—´å·®å¼‚æ˜æ˜¾")
    
    return True

def test_classifier_on_real_data(
    user_id: int,
    classifier_path: str,
    data_root: str,
    num_test_per_user: int = 20
):
    """
    è¯¦ç»†æµ‹è¯•åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°
    """
    print(f"\nğŸ¯ è¯¦ç»†æµ‹è¯•åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°")
    
    # åŠ è½½åˆ†ç±»å™¨
    validation_system = UserValidationSystem()
    try:
        validation_system.load_classifier(user_id, classifier_path)
        model = validation_system.classifiers[user_id]
        print(f"âœ… æˆåŠŸåŠ è½½åˆ†ç±»å™¨")
    except Exception as e:
        print(f"âŒ åŠ è½½åˆ†ç±»å™¨å¤±è´¥: {e}")
        return False
    
    # æ”¶é›†æµ‹è¯•æ•°æ®
    data_path = Path(data_root)
    positive_samples = []
    negative_samples = []
    
    # æ­£æ ·æœ¬ï¼šç›®æ ‡ç”¨æˆ·
    target_dir = data_path / f"ID_{user_id}"
    if target_dir.exists():
        images = list(target_dir.glob("*.png")) + list(target_dir.glob("*.jpg"))
        positive_samples = images[:num_test_per_user]
    
    # è´Ÿæ ·æœ¬ï¼šå…¶ä»–ç”¨æˆ·
    for user_dir in data_path.iterdir():
        if user_dir.is_dir() and user_dir.name.startswith('ID_'):
            try:
                other_user_id = int(user_dir.name.split('_')[1])
                if other_user_id != user_id:
                    images = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                    negative_samples.extend(images[:5])  # æ¯ä¸ªå…¶ä»–ç”¨æˆ·å–5å¼ 
            except ValueError:
                continue
    
    print(f"  æ­£æ ·æœ¬: {len(positive_samples)} å¼ ")
    print(f"  è´Ÿæ ·æœ¬: {len(negative_samples)} å¼ ")
    
    if len(positive_samples) < 5 or len(negative_samples) < 5:
        print(f"âŒ æµ‹è¯•æ ·æœ¬ä¸è¶³")
        return False
    
    # æµ‹è¯•åˆ†ç±»å™¨
    def predict_batch(image_paths, label):
        predictions = []
        confidences = []
        labels = []
        
        for img_path in image_paths:
            try:
                image = Image.open(img_path).convert('RGB')
                image_tensor = validation_system.transform(image).unsqueeze(0).to(validation_system.device)
                
                with torch.no_grad():
                    output = model(image_tensor)
                    probabilities = torch.softmax(output, dim=1)
                    confidence = probabilities[0, 1].item()
                    prediction = 1 if confidence > 0.5 else 0
                    
                    predictions.append(prediction)
                    confidences.append(confidence)
                    labels.append(label)
                    
            except Exception as e:
                print(f"    å¤„ç†å¤±è´¥ {img_path}: {e}")
        
        return predictions, confidences, labels
    
    # æµ‹è¯•æ­£æ ·æœ¬
    print(f"  æµ‹è¯•æ­£æ ·æœ¬...")
    pos_preds, pos_confs, pos_labels = predict_batch(positive_samples, 1)
    
    # æµ‹è¯•è´Ÿæ ·æœ¬
    print(f"  æµ‹è¯•è´Ÿæ ·æœ¬...")
    neg_preds, neg_confs, neg_labels = predict_batch(negative_samples, 0)
    
    # åˆå¹¶ç»“æœ
    all_preds = pos_preds + neg_preds
    all_confs = pos_confs + neg_confs
    all_labels = pos_labels + neg_labels
    
    if len(all_preds) < 10:
        print(f"âŒ æœ‰æ•ˆé¢„æµ‹å¤ªå°‘")
        return False
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_preds)
    
    pos_acc = accuracy_score(pos_labels, pos_preds) if pos_preds else 0
    neg_acc = accuracy_score(neg_labels, neg_preds) if neg_preds else 0
    
    avg_pos_conf = np.mean(pos_confs) if pos_confs else 0
    avg_neg_conf = np.mean(neg_confs) if neg_confs else 0
    
    print(f"\nğŸ“Š åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°:")
    print(f"  æ€»ä½“å‡†ç¡®ç‡: {accuracy:.3f}")
    print(f"  æ­£æ ·æœ¬å‡†ç¡®ç‡: {pos_acc:.3f}")
    print(f"  è´Ÿæ ·æœ¬å‡†ç¡®ç‡: {neg_acc:.3f}")
    print(f"  æ­£æ ·æœ¬å¹³å‡ç½®ä¿¡åº¦: {avg_pos_conf:.3f}")
    print(f"  è´Ÿæ ·æœ¬å¹³å‡ç½®ä¿¡åº¦: {avg_neg_conf:.3f}")
    print(f"  ç½®ä¿¡åº¦å·®å¼‚: {avg_pos_conf - avg_neg_conf:.3f}")
    
    # åˆ¤æ–­åˆ†ç±»å™¨è´¨é‡
    if accuracy > 0.8 and abs(avg_pos_conf - avg_neg_conf) > 0.3:
        print(f"  âœ… åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½")
        return True
    elif accuracy > 0.6:
        print(f"  âš ï¸  åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šè¡¨ç°ä¸€èˆ¬")
        return True
    else:
        print(f"  âŒ åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šè¡¨ç°å¾ˆå·®")
        return False

def analyze_generated_vs_real(
    user_id: int,
    real_data_root: str,
    generated_images_dir: str
):
    """
    åˆ†æç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒçš„å·®å¼‚
    """
    print(f"\nğŸ” åˆ†æç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒçš„å·®å¼‚")
    
    validation_system = UserValidationSystem()
    
    # åŠ è½½çœŸå®å›¾åƒ
    real_dir = Path(real_data_root) / f"ID_{user_id}"
    if not real_dir.exists():
        print(f"âŒ çœŸå®å›¾åƒç›®å½•ä¸å­˜åœ¨")
        return False
    
    real_images = list(real_dir.glob("*.png")) + list(real_dir.glob("*.jpg"))
    
    # åŠ è½½ç”Ÿæˆå›¾åƒ
    gen_dir = Path(generated_images_dir)
    if not gen_dir.exists():
        print(f"âŒ ç”Ÿæˆå›¾åƒç›®å½•ä¸å­˜åœ¨")
        return False
    
    gen_images = list(gen_dir.glob("*.png")) + list(gen_dir.glob("*.jpg"))
    
    print(f"  çœŸå®å›¾åƒ: {len(real_images)} å¼ ")
    print(f"  ç”Ÿæˆå›¾åƒ: {len(gen_images)} å¼ ")
    
    def load_and_process(img_path):
        try:
            image = Image.open(img_path).convert('RGB')
            tensor = validation_system.transform(image)
            return tensor.numpy().flatten()
        except:
            return None
    
    # å¤„ç†çœŸå®å›¾åƒ
    real_vectors = []
    for img_path in real_images[:20]:  # å–å‰20å¼ 
        vec = load_and_process(img_path)
        if vec is not None:
            real_vectors.append(vec)
    
    # å¤„ç†ç”Ÿæˆå›¾åƒ
    gen_vectors = []
    for img_path in gen_images:
        vec = load_and_process(img_path)
        if vec is not None:
            gen_vectors.append(vec)
    
    if not real_vectors or not gen_vectors:
        print(f"âŒ æ— æ³•åŠ è½½å›¾åƒ")
        return False
    
    real_vectors = np.array(real_vectors)
    gen_vectors = np.array(gen_vectors)
    
    # è®¡ç®—ç»Ÿè®¡å·®å¼‚
    real_mean = np.mean(real_vectors, axis=0)
    gen_mean = np.mean(gen_vectors, axis=0)
    
    mean_distance = np.linalg.norm(real_mean - gen_mean)
    
    # è®¡ç®—æ–¹å·®
    real_var = np.var(real_vectors)
    gen_var = np.var(gen_vectors)
    
    print(f"  åƒç´ çº§åˆ†æ:")
    print(f"    çœŸå®å›¾åƒå‡å€¼è·ç¦»ç”Ÿæˆå›¾åƒå‡å€¼: {mean_distance:.2f}")
    print(f"    çœŸå®å›¾åƒæ–¹å·®: {real_var:.2f}")
    print(f"    ç”Ÿæˆå›¾åƒæ–¹å·®: {gen_var:.2f}")
    
    if mean_distance > 100:
        print(f"  âŒ ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒå·®å¼‚å¾ˆå¤§")
    elif mean_distance > 50:
        print(f"  âš ï¸  ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒæœ‰ä¸€å®šå·®å¼‚")
    else:
        print(f"  âœ… ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒç›¸ä¼¼")
    
    return True

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å…¨é¢è¯Šæ–­éªŒè¯å¤±è´¥åŸå› ")
    parser.add_argument("--user_id", type=int, required=True, help="ç”¨æˆ·ID")
    parser.add_argument("--real_data_root", type=str, required=True, help="çœŸå®æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--classifier_path", type=str, required=True, help="åˆ†ç±»å™¨è·¯å¾„")
    parser.add_argument("--generated_images_dir", type=str, help="ç”Ÿæˆå›¾åƒç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸ” å…¨é¢è¯Šæ–­éªŒè¯å¤±è´¥åŸå› ")
    print("=" * 60)
    
    # 1. åˆ†æçœŸå®æ•°æ®å¤šæ ·æ€§
    diversity_ok = analyze_real_data_diversity(args.real_data_root, args.user_id)
    
    # 2. æµ‹è¯•åˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°
    classifier_ok = test_classifier_on_real_data(
        args.user_id, args.classifier_path, args.real_data_root
    )
    
    # 3. åˆ†æç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒçš„å·®å¼‚
    if args.generated_images_dir:
        similarity_ok = analyze_generated_vs_real(
            args.user_id, args.real_data_root, args.generated_images_dir
        )
    
    print("\n" + "=" * 60)
    print("ğŸ¯ è¯Šæ–­ç»“è®º:")
    
    if not diversity_ok:
        print("âŒ æ•°æ®é—®é¢˜ï¼šç”¨æˆ·é—´å·®å¼‚å¤ªå°æˆ–æ•°æ®è´¨é‡æœ‰é—®é¢˜")
    elif not classifier_ok:
        print("âŒ åˆ†ç±»å™¨é—®é¢˜ï¼šåˆ†ç±»å™¨åœ¨çœŸå®æ•°æ®ä¸Šè¡¨ç°å·®")
    elif args.generated_images_dir and not similarity_ok:
        print("âŒ ç”Ÿæˆé—®é¢˜ï¼šç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒå·®å¼‚å¤ªå¤§")
    else:
        print("âš ï¸  éœ€è¦è¿›ä¸€æ­¥åˆ†æï¼Œå¯èƒ½æ˜¯æ¡ä»¶æ‰©æ•£æ¨¡å‹æ²¡æœ‰å­¦åˆ°ç”¨æˆ·ç‰¹å¾")

if __name__ == "__main__":
    main()
