#!/usr/bin/env python3
"""
åŸºäºåº¦é‡å­¦ä¹ çš„éªŒè¯å™¨ - è§£å†³å°æ•°æ®é‡é—®é¢˜
ä½¿ç”¨Siamese Networkå­¦ä¹ ç”¨æˆ·ç›¸ä¼¼æ€§ï¼Œè€Œä¸æ˜¯è®­ç»ƒç‹¬ç«‹åˆ†ç±»å™¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import resnet18
import numpy as np
from PIL import Image
from pathlib import Path
from typing import List, Tuple, Dict
import random
from tqdm import tqdm

class SiameseDataset(Dataset):
    """Siameseç½‘ç»œæ•°æ®é›† - ç”Ÿæˆå›¾åƒå¯¹"""
    
    def __init__(self, user_images_dict: Dict[int, List[str]], transform=None):
        """
        Args:
            user_images_dict: {user_id: [image_paths]}
            transform: å›¾åƒå˜æ¢
        """
        self.user_images = user_images_dict
        self.user_ids = list(user_images_dict.keys())
        self.transform = transform
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å›¾åƒå¯¹
        self.pairs = []
        self.labels = []
        
        # æ­£æ ·æœ¬å¯¹ï¼šåŒä¸€ç”¨æˆ·çš„ä¸åŒå›¾åƒ
        for user_id, images in user_images_dict.items():
            if len(images) >= 2:
                for i in range(len(images)):
                    for j in range(i+1, min(i+10, len(images))):  # é™åˆ¶æ­£æ ·æœ¬å¯¹æ•°é‡
                        self.pairs.append((images[i], images[j]))
                        self.labels.append(1)  # åŒä¸€ç”¨æˆ·
        
        # è´Ÿæ ·æœ¬å¯¹ï¼šä¸åŒç”¨æˆ·çš„å›¾åƒ
        num_negative = len(self.pairs)  # ä¸æ­£æ ·æœ¬å¯¹æ•°é‡ç›¸ç­‰
        for _ in range(num_negative):
            user1, user2 = random.sample(self.user_ids, 2)
            img1 = random.choice(user_images_dict[user1])
            img2 = random.choice(user_images_dict[user2])
            self.pairs.append((img1, img2))
            self.labels.append(0)  # ä¸åŒç”¨æˆ·
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        img1_path, img2_path = self.pairs[idx]
        label = self.labels[idx]
        
        # åŠ è½½å›¾åƒ
        img1 = Image.open(img1_path).convert('RGB')
        img2 = Image.open(img2_path).convert('RGB')
        
        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)
        
        return img1, img2, label

class SimplifiedSiameseNetwork(nn.Module):
    """ç®€åŒ–çš„Siameseç½‘ç»œ - ç¨³å®šå¯é çš„å®ç°"""

    def __init__(self, embedding_dim=128):
        super(SimplifiedSiameseNetwork, self).__init__()

        # ä½¿ç”¨ResNet18ï¼Œæ›´ç¨³å®š
        self.backbone = resnet18(pretrained=True)

        # æ›¿æ¢æœ€åçš„åˆ†ç±»å±‚
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(in_features, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(embedding_dim, embedding_dim)
        )

        # ç›¸ä¼¼æ€§è®¡ç®—
        self.similarity = nn.CosineSimilarity(dim=1)
        
    def forward_one(self, x):
        """å•ä¸ªå›¾åƒçš„å‰å‘ä¼ æ’­"""
        return self.backbone(x)

    def forward(self, img1, img2):
        """è®¡ç®—ä¸¤ä¸ªå›¾åƒçš„ç›¸ä¼¼æ€§"""
        emb1 = self.forward_one(img1)
        emb2 = self.forward_one(img2)

        # L2å½’ä¸€åŒ–
        emb1 = F.normalize(emb1, p=2, dim=1)
        emb2 = F.normalize(emb2, p=2, dim=1)

        # ä½™å¼¦ç›¸ä¼¼æ€§
        similarity = self.similarity(emb1, emb2)

        return similarity, emb1, emb2

class MetricLearningValidator:
    """åŸºäºåº¦é‡å­¦ä¹ çš„éªŒè¯å™¨"""
    
    def __init__(self, device="auto"):
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.model = None
        self.user_prototypes = {}  # å­˜å‚¨æ¯ä¸ªç”¨æˆ·çš„åŸå‹ç‰¹å¾
    
    def load_user_images(self, data_root: str) -> Dict[int, List[str]]:
        """åŠ è½½æ‰€æœ‰ç”¨æˆ·çš„å›¾åƒè·¯å¾„"""
        data_path = Path(data_root)
        user_images = {}
        
        for user_dir in data_path.iterdir():
            if user_dir.is_dir() and user_dir.name.startswith('ID_'):
                try:
                    user_id = int(user_dir.name.split('_')[1])
                    images = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                    if images:
                        user_images[user_id] = [str(p) for p in images]
                        print(f"  ç”¨æˆ· {user_id}: {len(images)} å¼ å›¾åƒ")
                except ValueError:
                    continue
        
        return user_images
    
    def train_siamese_network(self, user_images: Dict[int, List[str]], 
                            epochs: int = 50, batch_size: int = 32) -> Dict:
        """è®­ç»ƒSiameseç½‘ç»œ"""
        print(f"\nğŸ¯ è®­ç»ƒSiameseç½‘ç»œ...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = SiameseDataset(user_images, self.transform)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        
        print(f"  ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªå›¾åƒå¯¹")
        print(f"  ğŸ“Š æ­£æ ·æœ¬å¯¹: {sum(dataset.labels)} ä¸ª")
        print(f"  ğŸ“Š è´Ÿæ ·æœ¬å¯¹: {len(dataset.labels) - sum(dataset.labels)} ä¸ª")
        
        # åˆ›å»ºç®€åŒ–çš„æ¨¡å‹
        self.model = SimplifiedSiameseNetwork().to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3, weight_decay=1e-4)
        criterion = nn.MSELoss()  # ä½¿ç”¨MSEæŸå¤±ï¼Œå› ä¸ºç›¸ä¼¼åº¦åœ¨[-1,1]èŒƒå›´
        
        # è®­ç»ƒå¾ªç¯
        history = {'train_loss': [], 'train_acc': []}
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for img1, img2, labels in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
                img1, img2, labels = img1.to(self.device), img2.to(self.device), labels.float().to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                similarity, _, _ = self.model(img1, img2)

                # å°†æ ‡ç­¾è½¬æ¢ä¸ºç›¸ä¼¼åº¦ç›®æ ‡ï¼š1->1.0, 0->-1.0
                target_similarity = labels * 2.0 - 1.0  # [0,1] -> [-1,1]

                # æŸå¤±è®¡ç®—
                loss = criterion(similarity, target_similarity)
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

                # å‡†ç¡®ç‡è®¡ç®—ï¼šç›¸ä¼¼åº¦>0è®¤ä¸ºæ˜¯åŒä¸€ç”¨æˆ·
                predicted = (similarity > 0).float()
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
            
            avg_loss = total_loss / len(dataloader)
            accuracy = correct / total
            
            history['train_loss'].append(avg_loss)
            history['train_acc'].append(accuracy)
            
            print(f"  Epoch {epoch+1}: Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}")
        
        print(f"âœ… Siameseç½‘ç»œè®­ç»ƒå®Œæˆ")
        return history
    
    def compute_user_prototypes(self, user_images: Dict[int, List[str]]):
        """è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„åŸå‹ç‰¹å¾"""
        print(f"\nğŸ“Š è®¡ç®—ç”¨æˆ·åŸå‹ç‰¹å¾...")
        
        self.model.eval()
        self.user_prototypes = {}
        
        with torch.no_grad():
            for user_id, image_paths in user_images.items():
                embeddings = []
                
                for img_path in image_paths:
                    img = Image.open(img_path).convert('RGB')
                    img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                    
                    embedding = self.model.forward_one(img_tensor)
                    embedding = F.normalize(embedding, p=2, dim=1)
                    embeddings.append(embedding.cpu())
                
                # è®¡ç®—åŸå‹ï¼ˆå¹³å‡ç‰¹å¾ï¼‰
                prototype = torch.stack(embeddings).mean(dim=0)
                self.user_prototypes[user_id] = prototype
                
                print(f"  ç”¨æˆ· {user_id}: åŸå‹ç‰¹å¾è®¡ç®—å®Œæˆ ({len(embeddings)} å¼ å›¾åƒ)")
    
    def validate_generated_images(self, target_user_id: int, generated_images_dir: str,
                                threshold: float = 0.5) -> Dict:
        """éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦å±äºç›®æ ‡ç”¨æˆ·"""
        print(f"\nğŸ” éªŒè¯ç”Ÿæˆå›¾åƒ (åº¦é‡å­¦ä¹ æ–¹æ³•)")
        
        if target_user_id not in self.user_prototypes:
            raise ValueError(f"ç”¨æˆ· {target_user_id} çš„åŸå‹ç‰¹å¾ä¸å­˜åœ¨")
        
        # åŠ è½½ç”Ÿæˆå›¾åƒ
        gen_dir = Path(generated_images_dir)
        image_files = list(gen_dir.glob("*.png")) + list(gen_dir.glob("*.jpg"))
        
        if not image_files:
            return {'error': 'No generated images found'}
        
        print(f"  æ‰¾åˆ° {len(image_files)} å¼ ç”Ÿæˆå›¾åƒ")
        
        # è·å–ç›®æ ‡ç”¨æˆ·åŸå‹
        target_prototype = self.user_prototypes[target_user_id].to(self.device)
        
        # è®¡ç®—ç›¸ä¼¼æ€§
        similarities = []
        self.model.eval()
        
        with torch.no_grad():
            for img_path in image_files:
                img = Image.open(img_path).convert('RGB')
                img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                
                # æå–ç‰¹å¾
                embedding = self.model.forward_one(img_tensor)
                embedding = F.normalize(embedding, p=2, dim=1)
                
                # è®¡ç®—ä¸ç›®æ ‡ç”¨æˆ·åŸå‹çš„ç›¸ä¼¼æ€§
                similarity = F.cosine_similarity(embedding, target_prototype, dim=1)
                similarities.append(similarity.item())
        
        # ç»Ÿè®¡ç»“æœ
        similarities = np.array(similarities)
        success_count = (similarities > threshold).sum()
        success_rate = success_count / len(similarities)
        avg_similarity = similarities.mean()
        
        result = {
            'success_rate': success_rate,
            'avg_similarity': avg_similarity,
            'threshold': threshold,
            'total_images': len(similarities),
            'successful_images': int(success_count),
            'similarities': similarities.tolist()
        }
        
        print(f"  ğŸ“Š éªŒè¯ç»“æœ:")
        print(f"    æˆåŠŸç‡: {success_rate:.3f}")
        print(f"    å¹³å‡ç›¸ä¼¼æ€§: {avg_similarity:.3f}")
        print(f"    é˜ˆå€¼: {threshold}")
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="åº¦é‡å­¦ä¹ éªŒè¯å™¨ - ä¸éœ€è¦é¢„è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--data_root", type=str, default="/kaggle/input/dataset",
                       help="çœŸå®æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--target_user_id", type=int, default=1,
                       help="ç›®æ ‡ç”¨æˆ·ID")
    parser.add_argument("--generated_images_dir", type=str,
                       default="/kaggle/working/validation_results/generated_images",
                       help="ç”Ÿæˆå›¾åƒç›®å½•è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=30,
                       help="Siameseç½‘ç»œè®­ç»ƒè½®æ•°")
    parser.add_argument("--threshold", type=float, default=0.7,
                       help="ç›¸ä¼¼æ€§é˜ˆå€¼")

    args = parser.parse_args()

    print("ğŸ§  åº¦é‡å­¦ä¹ éªŒè¯å™¨ - é’ˆå¯¹ç›¸ä¼¼ç‰¹å¾ä¼˜åŒ–")
    print(f"ğŸ”§ é…ç½®:")
    print(f"  æ•°æ®ç›®å½•: {args.data_root}")
    print(f"  ç›®æ ‡ç”¨æˆ·: {args.target_user_id}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  ç›¸ä¼¼æ€§é˜ˆå€¼: {args.threshold}")

    validator = MetricLearningValidator()

    # åŠ è½½æ•°æ®
    user_images = validator.load_user_images(args.data_root)

    if not user_images:
        print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•è·¯å¾„")
        exit(1)

    # è®­ç»ƒSiameseç½‘ç»œ
    history = validator.train_siamese_network(user_images, epochs=args.epochs)

    # è®¡ç®—ç”¨æˆ·åŸå‹
    validator.compute_user_prototypes(user_images)

    # éªŒè¯ç”Ÿæˆå›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    from pathlib import Path
    if Path(args.generated_images_dir).exists():
        result = validator.validate_generated_images(
            target_user_id=args.target_user_id,
            generated_images_dir=args.generated_images_dir,
            threshold=args.threshold
        )
        print(f"\nâœ… åº¦é‡å­¦ä¹ éªŒè¯å®Œæˆ")
    else:
        print(f"\nğŸ“‹ Siameseç½‘ç»œè®­ç»ƒå®Œæˆï¼Œç”Ÿæˆå›¾åƒç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡éªŒè¯æ­¥éª¤")
        print(f"ğŸ’¡ æç¤º: å…ˆè¿è¡Œä¼ ç»ŸéªŒè¯å™¨ç”Ÿæˆå›¾åƒï¼Œå†è¿è¡Œåº¦é‡å­¦ä¹ éªŒè¯")
