#!/usr/bin/env python3
"""
åŸºäºåº¦é‡å­¦ä¹ çš„éªŒè¯å™¨ - è§£å†³å°æ•°æ®é‡é—®é¢˜
ä½¿ç”¨Siamese Networkå­¦ä¹ ç”¨æˆ·ç›¸ä¼¼æ€§ï¼Œè€Œä¸æ˜¯è®­ç»ƒç‹¬ç«‹åˆ†ç±»å™¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import resnet18
import numpy as np
from PIL import Image
from pathlib import Path
from typing import List, Tuple, Dict
import random
from tqdm import tqdm

class SiameseDataset(Dataset):
    """Siameseç½‘ç»œæ•°æ®é›† - ç”Ÿæˆå›¾åƒå¯¹"""
    
    def __init__(self, user_images_dict: Dict[int, List[str]], transform=None):
        """
        Args:
            user_images_dict: {user_id: [image_paths]}
            transform: å›¾åƒå˜æ¢
        """
        self.user_images = user_images_dict
        self.user_ids = list(user_images_dict.keys())
        self.transform = transform
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å›¾åƒå¯¹
        self.pairs = []
        self.labels = []
        
        # æ­£æ ·æœ¬å¯¹ï¼šåŒä¸€ç”¨æˆ·çš„ä¸åŒå›¾åƒ
        for user_id, images in user_images_dict.items():
            if len(images) >= 2:
                for i in range(len(images)):
                    for j in range(i+1, min(i+10, len(images))):  # é™åˆ¶æ­£æ ·æœ¬å¯¹æ•°é‡
                        self.pairs.append((images[i], images[j]))
                        self.labels.append(1)  # åŒä¸€ç”¨æˆ·
        
        # è´Ÿæ ·æœ¬å¯¹ï¼šä¸åŒç”¨æˆ·çš„å›¾åƒ
        num_negative = len(self.pairs)  # ä¸æ­£æ ·æœ¬å¯¹æ•°é‡ç›¸ç­‰
        for _ in range(num_negative):
            user1, user2 = random.sample(self.user_ids, 2)
            img1 = random.choice(user_images_dict[user1])
            img2 = random.choice(user_images_dict[user2])
            self.pairs.append((img1, img2))
            self.labels.append(0)  # ä¸åŒç”¨æˆ·
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        img1_path, img2_path = self.pairs[idx]
        label = self.labels[idx]
        
        # åŠ è½½å›¾åƒ
        img1 = Image.open(img1_path).convert('RGB')
        img2 = Image.open(img2_path).convert('RGB')
        
        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)
        
        return img1, img2, label

class ImprovedSiameseNetwork(nn.Module):
    """æ”¹è¿›çš„Siameseç½‘ç»œ - ä¸“é—¨å¤„ç†ç›¸ä¼¼ç‰¹å¾çš„å°æ•°æ®é—®é¢˜"""

    def __init__(self, embedding_dim=256):
        super(ImprovedSiameseNetwork, self).__init__()

        # ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œæå–ç»†å¾®ç‰¹å¾
        from torchvision.models import resnet50
        self.backbone = resnet50(pretrained=True)

        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.backbone.fc = nn.Identity()

        # å¤šå°ºåº¦ç‰¹å¾æå–
        self.multi_scale = nn.ModuleList([
            nn.AdaptiveAvgPool2d((1, 1)),  # å…¨å±€ç‰¹å¾
            nn.AdaptiveAvgPool2d((2, 2)),  # ä¸­ç­‰å°ºåº¦
            nn.AdaptiveAvgPool2d((4, 4)),  # ç»†ç²’åº¦ç‰¹å¾
        ])

        # æ³¨æ„åŠ›æœºåˆ¶ - èšç„¦å…³é”®ç‰¹å¾
        self.attention = nn.MultiheadAttention(embed_dim=2048, num_heads=8, batch_first=True)

        # ç‰¹å¾èåˆå’Œé™ç»´
        self.feature_fusion = nn.Sequential(
            nn.Linear(2048 * (1 + 4 + 16), 1024),  # å¤šå°ºåº¦ç‰¹å¾èåˆ
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(embedding_dim, embedding_dim)
        )

        # å­¦ä¹ å¦‚ä½•æ¯”è¾ƒç‰¹å¾ï¼ˆè€Œä¸æ˜¯ç®€å•çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.relation_module = nn.Sequential(
            nn.Linear(embedding_dim * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
        
    def extract_deep_features(self, x):
        """æå–æ·±å±‚å¤šå°ºåº¦ç‰¹å¾"""
        # é€šè¿‡backboneæå–åŸºç¡€ç‰¹å¾
        features = self.backbone(x)  # [batch, 2048, 7, 7]

        # å¤šå°ºåº¦ç‰¹å¾æå–
        multi_scale_features = []
        for pool in self.multi_scale:
            pooled = pool(features)  # [batch, 2048, scale, scale]
            flattened = pooled.view(pooled.size(0), -1)  # [batch, 2048*scale*scale]
            multi_scale_features.append(flattened)

        # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾
        combined_features = torch.cat(multi_scale_features, dim=1)  # [batch, 2048*(1+4+16)]

        # ç‰¹å¾èåˆ
        fused_features = self.feature_fusion(combined_features)  # [batch, embedding_dim]

        return fused_features

    def forward_one(self, x):
        """å•ä¸ªå›¾åƒçš„å‰å‘ä¼ æ’­"""
        return self.extract_deep_features(x)

    def forward(self, img1, img2):
        """è®¡ç®—ä¸¤ä¸ªå›¾åƒçš„ç›¸ä¼¼æ€§ - ä½¿ç”¨å­¦ä¹ çš„å…³ç³»æ¨¡å—"""
        emb1 = self.forward_one(img1)
        emb2 = self.forward_one(img2)

        # L2å½’ä¸€åŒ–
        emb1 = F.normalize(emb1, p=2, dim=1)
        emb2 = F.normalize(emb2, p=2, dim=1)

        # ä½¿ç”¨å…³ç³»æ¨¡å—å­¦ä¹ å¦‚ä½•æ¯”è¾ƒç‰¹å¾
        combined = torch.cat([emb1, emb2], dim=1)
        similarity = self.relation_module(combined).squeeze()

        return similarity, emb1, emb2

class MetricLearningValidator:
    """åŸºäºåº¦é‡å­¦ä¹ çš„éªŒè¯å™¨"""
    
    def __init__(self, device="auto"):
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.model = None
        self.user_prototypes = {}  # å­˜å‚¨æ¯ä¸ªç”¨æˆ·çš„åŸå‹ç‰¹å¾
    
    def load_user_images(self, data_root: str) -> Dict[int, List[str]]:
        """åŠ è½½æ‰€æœ‰ç”¨æˆ·çš„å›¾åƒè·¯å¾„"""
        data_path = Path(data_root)
        user_images = {}
        
        for user_dir in data_path.iterdir():
            if user_dir.is_dir() and user_dir.name.startswith('ID_'):
                try:
                    user_id = int(user_dir.name.split('_')[1])
                    images = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                    if images:
                        user_images[user_id] = [str(p) for p in images]
                        print(f"  ç”¨æˆ· {user_id}: {len(images)} å¼ å›¾åƒ")
                except ValueError:
                    continue
        
        return user_images
    
    def train_siamese_network(self, user_images: Dict[int, List[str]], 
                            epochs: int = 50, batch_size: int = 32) -> Dict:
        """è®­ç»ƒSiameseç½‘ç»œ"""
        print(f"\nğŸ¯ è®­ç»ƒSiameseç½‘ç»œ...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = SiameseDataset(user_images, self.transform)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)
        
        print(f"  ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªå›¾åƒå¯¹")
        print(f"  ğŸ“Š æ­£æ ·æœ¬å¯¹: {sum(dataset.labels)} ä¸ª")
        print(f"  ğŸ“Š è´Ÿæ ·æœ¬å¯¹: {len(dataset.labels) - sum(dataset.labels)} ä¸ª")
        
        # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
        self.model = ImprovedSiameseNetwork().to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3, weight_decay=1e-4)
        criterion = nn.BCEWithLogitsLoss()
        
        # è®­ç»ƒå¾ªç¯
        history = {'train_loss': [], 'train_acc': []}
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for img1, img2, labels in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
                img1, img2, labels = img1.to(self.device), img2.to(self.device), labels.float().to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                similarity, _, _ = self.model(img1, img2)
                
                # æŸå¤±è®¡ç®—
                loss = criterion(similarity, labels)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                
                # å‡†ç¡®ç‡è®¡ç®—
                predicted = (torch.sigmoid(similarity) > 0.5).float()
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
            
            avg_loss = total_loss / len(dataloader)
            accuracy = correct / total
            
            history['train_loss'].append(avg_loss)
            history['train_acc'].append(accuracy)
            
            print(f"  Epoch {epoch+1}: Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}")
        
        print(f"âœ… Siameseç½‘ç»œè®­ç»ƒå®Œæˆ")
        return history
    
    def compute_user_prototypes(self, user_images: Dict[int, List[str]]):
        """è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„åŸå‹ç‰¹å¾"""
        print(f"\nğŸ“Š è®¡ç®—ç”¨æˆ·åŸå‹ç‰¹å¾...")
        
        self.model.eval()
        self.user_prototypes = {}
        
        with torch.no_grad():
            for user_id, image_paths in user_images.items():
                embeddings = []
                
                for img_path in image_paths:
                    img = Image.open(img_path).convert('RGB')
                    img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                    
                    embedding = self.model.forward_one(img_tensor)
                    embedding = F.normalize(embedding, p=2, dim=1)
                    embeddings.append(embedding.cpu())
                
                # è®¡ç®—åŸå‹ï¼ˆå¹³å‡ç‰¹å¾ï¼‰
                prototype = torch.stack(embeddings).mean(dim=0)
                self.user_prototypes[user_id] = prototype
                
                print(f"  ç”¨æˆ· {user_id}: åŸå‹ç‰¹å¾è®¡ç®—å®Œæˆ ({len(embeddings)} å¼ å›¾åƒ)")
    
    def validate_generated_images(self, target_user_id: int, generated_images_dir: str,
                                threshold: float = 0.5) -> Dict:
        """éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦å±äºç›®æ ‡ç”¨æˆ·"""
        print(f"\nğŸ” éªŒè¯ç”Ÿæˆå›¾åƒ (åº¦é‡å­¦ä¹ æ–¹æ³•)")
        
        if target_user_id not in self.user_prototypes:
            raise ValueError(f"ç”¨æˆ· {target_user_id} çš„åŸå‹ç‰¹å¾ä¸å­˜åœ¨")
        
        # åŠ è½½ç”Ÿæˆå›¾åƒ
        gen_dir = Path(generated_images_dir)
        image_files = list(gen_dir.glob("*.png")) + list(gen_dir.glob("*.jpg"))
        
        if not image_files:
            return {'error': 'No generated images found'}
        
        print(f"  æ‰¾åˆ° {len(image_files)} å¼ ç”Ÿæˆå›¾åƒ")
        
        # è·å–ç›®æ ‡ç”¨æˆ·åŸå‹
        target_prototype = self.user_prototypes[target_user_id].to(self.device)
        
        # è®¡ç®—ç›¸ä¼¼æ€§
        similarities = []
        self.model.eval()
        
        with torch.no_grad():
            for img_path in image_files:
                img = Image.open(img_path).convert('RGB')
                img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                
                # æå–ç‰¹å¾
                embedding = self.model.forward_one(img_tensor)
                embedding = F.normalize(embedding, p=2, dim=1)
                
                # è®¡ç®—ä¸ç›®æ ‡ç”¨æˆ·åŸå‹çš„ç›¸ä¼¼æ€§
                similarity = F.cosine_similarity(embedding, target_prototype, dim=1)
                similarities.append(similarity.item())
        
        # ç»Ÿè®¡ç»“æœ
        similarities = np.array(similarities)
        success_count = (similarities > threshold).sum()
        success_rate = success_count / len(similarities)
        avg_similarity = similarities.mean()
        
        result = {
            'success_rate': success_rate,
            'avg_similarity': avg_similarity,
            'threshold': threshold,
            'total_images': len(similarities),
            'successful_images': int(success_count),
            'similarities': similarities.tolist()
        }
        
        print(f"  ğŸ“Š éªŒè¯ç»“æœ:")
        print(f"    æˆåŠŸç‡: {success_rate:.3f}")
        print(f"    å¹³å‡ç›¸ä¼¼æ€§: {avg_similarity:.3f}")
        print(f"    é˜ˆå€¼: {threshold}")
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    validator = MetricLearningValidator()
    
    # åŠ è½½æ•°æ®
    user_images = validator.load_user_images("data/processed")
    
    # è®­ç»ƒSiameseç½‘ç»œ
    history = validator.train_siamese_network(user_images, epochs=30)
    
    # è®¡ç®—ç”¨æˆ·åŸå‹
    validator.compute_user_prototypes(user_images)
    
    # éªŒè¯ç”Ÿæˆå›¾åƒ
    result = validator.validate_generated_images(
        target_user_id=1,
        generated_images_dir="validation_results/generated_images",
        threshold=0.7
    )
