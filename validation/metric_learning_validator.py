#!/usr/bin/env python3
"""
åŸºäºåº¦é‡å­¦ä¹ çš„éªŒè¯å™¨ - è§£å†³å°æ•°æ®é‡é—®é¢˜
ä½¿ç”¨Siamese Networkå­¦ä¹ ç”¨æˆ·ç›¸ä¼¼æ€§ï¼Œè€Œä¸æ˜¯è®­ç»ƒç‹¬ç«‹åˆ†ç±»å™¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import resnet18
import numpy as np
from PIL import Image
from pathlib import Path
from typing import List, Tuple, Dict
import random
from tqdm import tqdm

class SiameseDataset(Dataset):
    """Siameseç½‘ç»œæ•°æ®é›† - ç”Ÿæˆå›¾åƒå¯¹"""
    
    def __init__(self, user_images_dict: Dict[int, List[str]], transform=None):
        """
        Args:
            user_images_dict: {user_id: [image_paths]}
            transform: å›¾åƒå˜æ¢
        """
        self.user_images = user_images_dict
        self.user_ids = list(user_images_dict.keys())
        self.transform = transform
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å›¾åƒå¯¹
        self.pairs = []
        self.labels = []
        
        # æ­£æ ·æœ¬å¯¹ï¼šåŒä¸€ç”¨æˆ·çš„ä¸åŒå›¾åƒ
        for user_id, images in user_images_dict.items():
            if len(images) >= 2:
                for i in range(len(images)):
                    for j in range(i+1, min(i+10, len(images))):  # é™åˆ¶æ­£æ ·æœ¬å¯¹æ•°é‡
                        self.pairs.append((images[i], images[j]))
                        self.labels.append(1)  # åŒä¸€ç”¨æˆ·
        
        # è´Ÿæ ·æœ¬å¯¹ï¼šä¸åŒç”¨æˆ·çš„å›¾åƒ
        num_negative = len(self.pairs)  # ä¸æ­£æ ·æœ¬å¯¹æ•°é‡ç›¸ç­‰
        for _ in range(num_negative):
            user1, user2 = random.sample(self.user_ids, 2)
            img1 = random.choice(user_images_dict[user1])
            img2 = random.choice(user_images_dict[user2])
            self.pairs.append((img1, img2))
            self.labels.append(0)  # ä¸åŒç”¨æˆ·
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        img1_path, img2_path = self.pairs[idx]
        label = self.labels[idx]
        
        # åŠ è½½å›¾åƒ
        img1 = Image.open(img1_path).convert('RGB')
        img2 = Image.open(img2_path).convert('RGB')
        
        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)
        
        return img1, img2, label

class SmallDataSiameseNetwork(nn.Module):
    """å°æ•°æ®é›†ä¸“ç”¨Siameseç½‘ç»œ - é˜²æ­¢è¿‡æ‹Ÿåˆ"""

    def __init__(self, embedding_dim=64):
        super(SmallDataSiameseNetwork, self).__init__()

        # ä½¿ç”¨ResNet18ï¼Œå‚æ•°é‡é€‚ä¸­
        self.backbone = resnet18(pretrained=True)
        print("  ğŸ¯ å°æ•°æ®é›†ä¼˜åŒ–ï¼šä½¿ç”¨ResNet18 + ç®€åŒ–åˆ†ç±»å¤´")

        # ç®€åŒ–çš„ç‰¹å¾æå–å¤´ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        in_features = self.backbone.fc.in_features  # 512
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.5),  # æ›´å¼ºçš„dropout
            nn.Linear(in_features, embedding_dim),  # ç›´æ¥é™åˆ°64ç»´
        )

        # ç›¸ä¼¼æ€§è®¡ç®—
        self.similarity = nn.CosineSimilarity(dim=1)

        # æ‰“å°å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"  ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        
    def forward_one(self, x):
        """å•ä¸ªå›¾åƒçš„å‰å‘ä¼ æ’­"""
        return self.backbone(x)

    def forward(self, img1, img2):
        """è®¡ç®—ä¸¤ä¸ªå›¾åƒçš„ç›¸ä¼¼æ€§"""
        emb1 = self.forward_one(img1)
        emb2 = self.forward_one(img2)

        # L2å½’ä¸€åŒ–
        emb1 = F.normalize(emb1, p=2, dim=1)
        emb2 = F.normalize(emb2, p=2, dim=1)

        # ä½™å¼¦ç›¸ä¼¼æ€§
        similarity = self.similarity(emb1, emb2)

        return similarity, emb1, emb2

class MetricLearningValidator:
    """åŸºäºåº¦é‡å­¦ä¹ çš„éªŒè¯å™¨"""
    
    def __init__(self, device="auto"):
        if device == "auto":
            if torch.cuda.is_available():
                try:
                    # æµ‹è¯•CUDAæ˜¯å¦æ­£å¸¸å·¥ä½œ
                    test_tensor = torch.randn(10, 10).cuda()
                    _ = test_tensor + test_tensor
                    self.device = torch.device("cuda")
                    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
                except Exception as e:
                    print(f"âš ï¸ CUDAæµ‹è¯•å¤±è´¥: {e}")
                    print(f"ğŸ”„ å›é€€åˆ°CPUè®­ç»ƒ")
                    self.device = torch.device("cpu")
            else:
                self.device = torch.device("cpu")
                print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        else:
            self.device = torch.device(device)
            print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.model = None
        self.user_prototypes = {}  # å­˜å‚¨æ¯ä¸ªç”¨æˆ·çš„åŸå‹ç‰¹å¾
    
    def load_user_images(self, data_root: str) -> Dict[int, List[str]]:
        """åŠ è½½æ‰€æœ‰ç”¨æˆ·çš„å›¾åƒè·¯å¾„"""
        data_path = Path(data_root)
        user_images = {}
        
        for user_dir in data_path.iterdir():
            if user_dir.is_dir() and user_dir.name.startswith('ID_'):
                try:
                    user_id = int(user_dir.name.split('_')[1])
                    images = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                    if images:
                        user_images[user_id] = [str(p) for p in images]
                        print(f"  ç”¨æˆ· {user_id}: {len(images)} å¼ å›¾åƒ")
                except ValueError:
                    continue
        
        return user_images
    
    def train_siamese_network(self, user_images: Dict[int, List[str]],
                            epochs: int = 50, batch_size: int = 32) -> Dict:
        """è®­ç»ƒSiameseç½‘ç»œ"""
        print(f"\nğŸ¯ è®­ç»ƒSiameseç½‘ç»œ...")
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆä¼˜åŒ–æ•°æ®åŠ è½½ï¼‰
        dataset = SiameseDataset(user_images, self.transform)

        # å°æ•°æ®é›†è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if len(dataset) < 1000 and batch_size > 16:
            recommended_batch_size = max(8, len(dataset) // 50)  # ç¡®ä¿è‡³å°‘50ä¸ªbatch
            print(f"  ğŸ”§ å°æ•°æ®é›†è‡ªåŠ¨è°ƒæ•´: batch_size {batch_size} -> {recommended_batch_size}")
            batch_size = recommended_batch_size

        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=2,  # P100ç¯å¢ƒå‡å°‘workeræ•°é‡
            pin_memory=True,  # åŠ é€ŸGPUä¼ è¾“
            persistent_workers=False  # P100ç¯å¢ƒå…³é—­æŒä¹…worker
        )
        
        print(f"  ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªå›¾åƒå¯¹")
        print(f"  ğŸ“Š æ­£æ ·æœ¬å¯¹: {sum(dataset.labels)} ä¸ª")
        print(f"  ğŸ“Š è´Ÿæ ·æœ¬å¯¹: {len(dataset.labels) - sum(dataset.labels)} ä¸ª")

        # å°æ•°æ®é›†æ‰¹æ¬¡å¤§å°ä¼˜åŒ–
        total_batches = len(dataloader)
        if total_batches < 50:
            print(f"  âš ï¸  å°æ•°æ®é›†è­¦å‘Š: æ¯epochåªæœ‰{total_batches}ä¸ªbatch")
            print(f"  ğŸ’¡ å»ºè®®: è€ƒè™‘å‡å°batch_sizeä»¥å¢åŠ æ¢¯åº¦æ›´æ–°é¢‘ç‡")

        print(f"  ğŸ”§ è®­ç»ƒé…ç½®: batch_size={batch_size}, æ¯epoch {total_batches} ä¸ªbatch")
        
        # åˆ›å»ºå°æ•°æ®é›†ä¸“ç”¨æ¨¡å‹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        self.model = SmallDataSiameseNetwork(embedding_dim=64).to(self.device)
        
        # å°æ•°æ®é›†ä¼˜åŒ–é…ç½®ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=1e-3,  # é™ä½å­¦ä¹ ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            weight_decay=1e-2,  # å¢å¼ºæƒé‡è¡°å‡
            betas=(0.9, 0.999),
            eps=1e-8
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=epochs,
            eta_min=1e-6
        )

        criterion = nn.MSELoss()  # ä½¿ç”¨MSEæŸå¤±ï¼Œå› ä¸ºç›¸ä¼¼åº¦åœ¨[-1,1]èŒƒå›´

        # P100ä¸ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆPascalæ¶æ„æ”¯æŒæœ‰é™ï¼‰
        print(f"  ğŸ”§ P100ä¼˜åŒ–ï¼šä¸ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")

        # æ—©åœæœºåˆ¶ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        best_loss = float('inf')
        patience = 5  # å°æ•°æ®é›†ç”¨æ›´å°çš„patience
        patience_counter = 0

        # è®­ç»ƒå¾ªç¯
        history = {'train_loss': [], 'train_acc': []}
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for img1, img2, labels in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
                img1, img2, labels = img1.to(self.device, non_blocking=True), img2.to(self.device, non_blocking=True), labels.float().to(self.device, non_blocking=True)

                optimizer.zero_grad()

                # æ ‡å‡†å‰å‘ä¼ æ’­ï¼ˆP100ä¼˜åŒ–ï¼‰
                similarity, _, _ = self.model(img1, img2)
                target_similarity = labels * 2.0 - 1.0  # [0,1] -> [-1,1]
                loss = criterion(similarity, target_similarity)

                # æ ‡å‡†åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

                # å‡†ç¡®ç‡è®¡ç®—ï¼šç›¸ä¼¼åº¦>0è®¤ä¸ºæ˜¯åŒä¸€ç”¨æˆ·
                predicted = (similarity > 0).float()
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

            avg_loss = total_loss / len(dataloader)
            accuracy = correct / total

            history['train_loss'].append(avg_loss)
            history['train_acc'].append(accuracy)

            # æ—©åœæ£€æŸ¥
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
                print(f"  Epoch {epoch+1}: Loss: {avg_loss:.4f}, Acc: {accuracy:.4f} âœ…")
            else:
                patience_counter += 1
                print(f"  Epoch {epoch+1}: Loss: {avg_loss:.4f}, Acc: {accuracy:.4f} (æ— æ”¹å–„: {patience_counter}/{patience})")

                if patience_counter >= patience:
                    print(f"  ğŸ›‘ æ—©åœè§¦å‘ï¼š{patience}ä¸ªepochæ— æ”¹å–„ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ")
                    break
        
        print(f"âœ… Siameseç½‘ç»œè®­ç»ƒå®Œæˆ")
        return history
    
    def compute_user_prototypes(self, user_images: Dict[int, List[str]]):
        """è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„åŸå‹ç‰¹å¾"""
        print(f"\nğŸ“Š è®¡ç®—ç”¨æˆ·åŸå‹ç‰¹å¾...")
        
        self.model.eval()
        self.user_prototypes = {}
        
        with torch.no_grad():
            for user_id, image_paths in user_images.items():
                embeddings = []
                
                for img_path in image_paths:
                    img = Image.open(img_path).convert('RGB')
                    img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                    
                    embedding = self.model.forward_one(img_tensor)
                    embedding = F.normalize(embedding, p=2, dim=1)
                    embeddings.append(embedding.cpu())
                
                # è®¡ç®—åŸå‹ï¼ˆå¹³å‡ç‰¹å¾ï¼‰
                prototype = torch.stack(embeddings).mean(dim=0)
                self.user_prototypes[user_id] = prototype
                
                print(f"  ç”¨æˆ· {user_id}: åŸå‹ç‰¹å¾è®¡ç®—å®Œæˆ ({len(embeddings)} å¼ å›¾åƒ)")
    
    def validate_generated_images(self, target_user_id: int, generated_images_dir: str,
                                threshold: float = 0.5) -> Dict:
        """éªŒè¯ç”Ÿæˆå›¾åƒæ˜¯å¦å±äºç›®æ ‡ç”¨æˆ·"""
        print(f"\nğŸ” éªŒè¯ç”Ÿæˆå›¾åƒ (åº¦é‡å­¦ä¹ æ–¹æ³•)")
        
        if target_user_id not in self.user_prototypes:
            raise ValueError(f"ç”¨æˆ· {target_user_id} çš„åŸå‹ç‰¹å¾ä¸å­˜åœ¨")
        
        # åŠ è½½ç”Ÿæˆå›¾åƒ
        gen_dir = Path(generated_images_dir)
        image_files = list(gen_dir.glob("*.png")) + list(gen_dir.glob("*.jpg"))
        
        if not image_files:
            return {'error': 'No generated images found'}
        
        print(f"  æ‰¾åˆ° {len(image_files)} å¼ ç”Ÿæˆå›¾åƒ")
        
        # è·å–ç›®æ ‡ç”¨æˆ·åŸå‹
        target_prototype = self.user_prototypes[target_user_id].to(self.device)
        
        # è®¡ç®—ç›¸ä¼¼æ€§
        similarities = []
        self.model.eval()
        
        with torch.no_grad():
            for img_path in image_files:
                img = Image.open(img_path).convert('RGB')
                img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                
                # æå–ç‰¹å¾
                embedding = self.model.forward_one(img_tensor)
                embedding = F.normalize(embedding, p=2, dim=1)
                
                # è®¡ç®—ä¸ç›®æ ‡ç”¨æˆ·åŸå‹çš„ç›¸ä¼¼æ€§
                similarity = F.cosine_similarity(embedding, target_prototype, dim=1)
                similarities.append(similarity.item())
        
        # ç»Ÿè®¡ç»“æœ
        similarities = np.array(similarities)
        success_count = (similarities > threshold).sum()
        success_rate = success_count / len(similarities)
        avg_similarity = similarities.mean()
        
        result = {
            'success_rate': success_rate,
            'avg_similarity': avg_similarity,
            'threshold': threshold,
            'total_images': len(similarities),
            'successful_images': int(success_count),
            'similarities': similarities.tolist()
        }
        
        print(f"  ğŸ“Š éªŒè¯ç»“æœ:")
        print(f"    æˆåŠŸç‡: {success_rate:.3f}")
        print(f"    å¹³å‡ç›¸ä¼¼æ€§: {avg_similarity:.3f}")
        print(f"    é˜ˆå€¼: {threshold}")
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="åº¦é‡å­¦ä¹ éªŒè¯å™¨ - ä¸éœ€è¦é¢„è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--data_root", type=str, default="/kaggle/input/dataset",
                       help="çœŸå®æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--target_user_id", type=int, default=1,
                       help="ç›®æ ‡ç”¨æˆ·ID")
    parser.add_argument("--generated_images_dir", type=str,
                       default="/kaggle/working/validation_results/generated_images",
                       help="ç”Ÿæˆå›¾åƒç›®å½•è·¯å¾„")
    parser.add_argument("--epochs", type=int, default=30,
                       help="Siameseç½‘ç»œè®­ç»ƒè½®æ•°")
    parser.add_argument("--threshold", type=float, default=0.3,
                       help="ç›¸ä¼¼æ€§é˜ˆå€¼ï¼ˆé’ˆå¯¹é«˜ç›¸ä¼¼æ€§æ•°æ®é™ä½ï¼‰")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="æ‰¹æ¬¡å¤§å°ï¼ˆå°æ•°æ®é›†æ¨è16-32ï¼‰")
    parser.add_argument("--use_larger_model", action="store_true",
                       help="ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆResNet34ï¼‰")
    parser.add_argument("--force_cpu", action="store_true",
                       help="å¼ºåˆ¶ä½¿ç”¨CPUè®­ç»ƒï¼ˆGPUæœ‰é—®é¢˜æ—¶ï¼‰")

    args = parser.parse_args()

    print("ğŸ§  åº¦é‡å­¦ä¹ éªŒè¯å™¨ - é’ˆå¯¹ç›¸ä¼¼ç‰¹å¾ä¼˜åŒ–")
    print(f"ğŸ”§ é…ç½®:")
    print(f"  æ•°æ®ç›®å½•: {args.data_root}")
    print(f"  ç›®æ ‡ç”¨æˆ·: {args.target_user_id}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  ç›¸ä¼¼æ€§é˜ˆå€¼: {args.threshold}")

    validator = MetricLearningValidator()

    # åŠ è½½æ•°æ®
    user_images = validator.load_user_images(args.data_root)

    if not user_images:
        print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•è·¯å¾„")
        exit(1)

    # è®­ç»ƒSiameseç½‘ç»œï¼ˆä½¿ç”¨ä¼˜åŒ–å‚æ•°ï¼‰
    history = validator.train_siamese_network(
        user_images,
        epochs=args.epochs,
        batch_size=args.batch_size
    )

    # è®¡ç®—ç”¨æˆ·åŸå‹
    validator.compute_user_prototypes(user_images)

    # éªŒè¯ç”Ÿæˆå›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    from pathlib import Path
    if Path(args.generated_images_dir).exists():
        result = validator.validate_generated_images(
            target_user_id=args.target_user_id,
            generated_images_dir=args.generated_images_dir,
            threshold=args.threshold
        )
        print(f"\nâœ… åº¦é‡å­¦ä¹ éªŒè¯å®Œæˆ")
    else:
        print(f"\nğŸ“‹ Siameseç½‘ç»œè®­ç»ƒå®Œæˆï¼Œç”Ÿæˆå›¾åƒç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡éªŒè¯æ­¥éª¤")
        print(f"ğŸ’¡ æç¤º: å…ˆè¿è¡Œä¼ ç»ŸéªŒè¯å™¨ç”Ÿæˆå›¾åƒï¼Œå†è¿è¡Œåº¦é‡å­¦ä¹ éªŒè¯")
