#!/usr/bin/env python3
"""
è°ƒè¯•VAEæ¶æ„ - éªŒè¯ä¸‹é‡‡æ ·è¡Œä¸º
"""

import torch
from diffusers import AutoencoderKL

def test_vae_downsampling():
    """æµ‹è¯•ä¸åŒé…ç½®çš„VAEä¸‹é‡‡æ ·è¡Œä¸º"""
    print("ğŸ” è°ƒè¯•VAEæ¶æ„ä¸‹é‡‡æ ·è¡Œä¸º")
    print("=" * 60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æµ‹è¯•é…ç½®1: å½“å‰é…ç½® (2å±‚DownEncoderBlock2D)
    print("\nğŸ“Š æµ‹è¯•é…ç½®1: 2å±‚DownEncoderBlock2D")
    try:
        vae1 = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256],
            latent_channels=4,
            sample_size=128,
            layers_per_block=1,
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        ).to(device)
        
        test_input = torch.randn(1, 3, 128, 128).to(device)
        with torch.no_grad():
            latent = vae1.encode(test_input).latent_dist.sample()
            reconstructed = vae1.decode(latent).sample
        
        print(f"   è¾“å…¥: {test_input.shape}")
        print(f"   æ½œåœ¨: {latent.shape}")
        print(f"   é‡å»º: {reconstructed.shape}")
        print(f"   ä¸‹é‡‡æ ·å› å­: {128 // latent.shape[-1]}")
        
    except Exception as e:
        print(f"   âŒ é…ç½®1å¤±è´¥: {e}")
    
    # æµ‹è¯•é…ç½®2: 3å±‚DownEncoderBlock2D (æ ‡å‡†Stable Diffusion)
    print("\nğŸ“Š æµ‹è¯•é…ç½®2: 3å±‚DownEncoderBlock2D (æ ‡å‡†SD)")
    try:
        vae2 = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256, 512],
            latent_channels=4,
            sample_size=128,
            layers_per_block=1,
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        ).to(device)
        
        test_input = torch.randn(1, 3, 128, 128).to(device)
        with torch.no_grad():
            latent = vae2.encode(test_input).latent_dist.sample()
            reconstructed = vae2.decode(latent).sample
        
        print(f"   è¾“å…¥: {test_input.shape}")
        print(f"   æ½œåœ¨: {latent.shape}")
        print(f"   é‡å»º: {reconstructed.shape}")
        print(f"   ä¸‹é‡‡æ ·å› å­: {128 // latent.shape[-1]}")
        
    except Exception as e:
        print(f"   âŒ é…ç½®2å¤±è´¥: {e}")
    
    # æµ‹è¯•é…ç½®3: 4å±‚DownEncoderBlock2D
    print("\nğŸ“Š æµ‹è¯•é…ç½®3: 4å±‚DownEncoderBlock2D")
    try:
        vae3 = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256, 512, 512],
            latent_channels=4,
            sample_size=128,
            layers_per_block=1,
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        ).to(device)
        
        test_input = torch.randn(1, 3, 128, 128).to(device)
        with torch.no_grad():
            latent = vae3.encode(test_input).latent_dist.sample()
            reconstructed = vae3.decode(latent).sample
        
        print(f"   è¾“å…¥: {test_input.shape}")
        print(f"   æ½œåœ¨: {latent.shape}")
        print(f"   é‡å»º: {reconstructed.shape}")
        print(f"   ä¸‹é‡‡æ ·å› å­: {128 // latent.shape[-1]}")
        
    except Exception as e:
        print(f"   âŒ é…ç½®3å¤±è´¥: {e}")
    
    # æµ‹è¯•ä¸åŒçš„layers_per_block
    print("\nğŸ“Š æµ‹è¯•é…ç½®4: 2å±‚DownEncoderBlock2D + layers_per_block=2")
    try:
        vae4 = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256],
            latent_channels=4,
            sample_size=128,
            layers_per_block=2,  # å¢åŠ æ¯å±‚çš„ResNetå—æ•°
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        ).to(device)
        
        test_input = torch.randn(1, 3, 128, 128).to(device)
        with torch.no_grad():
            latent = vae4.encode(test_input).latent_dist.sample()
            reconstructed = vae4.decode(latent).sample
        
        print(f"   è¾“å…¥: {test_input.shape}")
        print(f"   æ½œåœ¨: {latent.shape}")
        print(f"   é‡å»º: {reconstructed.shape}")
        print(f"   ä¸‹é‡‡æ ·å› å­: {128 // latent.shape[-1]}")
        
    except Exception as e:
        print(f"   âŒ é…ç½®4å¤±è´¥: {e}")
    
    print("\nğŸ¯ ç»“è®º:")
    print("   - æ¯ä¸ªDownEncoderBlock2Dè¿›è¡Œ1æ¬¡ä¸‹é‡‡æ · (2å€)")
    print("   - è¦è¾¾åˆ°128â†’32éœ€è¦3å±‚DownEncoderBlock2D (2^3=8å€)")
    print("   - å½“å‰2å±‚é…ç½®åªèƒ½è¾¾åˆ°128â†’64 (2^2=4å€)")
    print("   - layers_per_blockä¸å½±å“ä¸‹é‡‡æ ·å€æ•°ï¼Œåªå½±å“ç‰¹å¾æå–èƒ½åŠ›")

if __name__ == "__main__":
    test_vae_downsampling()
