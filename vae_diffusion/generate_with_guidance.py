#!/usr/bin/env python3
"""
æ”¯æŒæŒ‡å¯¼å¼ºåº¦çš„æ¡ä»¶æ‰©æ•£å›¾åƒç”Ÿæˆè„šæœ¬
ä¸“é—¨é’ˆå¯¹256Ã—256å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åƒä¼˜åŒ–

æ–°å¢åŠŸèƒ½:
- æ”¯æŒåˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ (CFG)
- å¯é…ç½®æŒ‡å¯¼å¼ºåº¦ (guidance_scale)
- é’ˆå¯¹256Ã—256å›¾åƒä¼˜åŒ–
- è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†æ ¼å¼
"""

import os
import torch
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
from typing import List
from tqdm import tqdm

from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# ç›´æ¥å®šä¹‰UserConditionEncoderç±»ï¼Œé¿å…å¯¼å…¥é—®é¢˜
import torch.nn as nn

class UserConditionEncoder(nn.Module):
    """ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨ - å®Œå…¨åŒ¹é…è®­ç»ƒæ—¶çš„ç»“æ„"""
    def __init__(self, num_users: int, embed_dim: int = 512, dropout: float = 0.1):
        super().__init__()
        self.num_users = num_users
        self.embed_dim = embed_dim

        # ç”¨æˆ·åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(num_users, embed_dim)

        # MLPå±‚ - å®Œå…¨åŒ¹é…è®­ç»ƒä»£ç ç»“æ„
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),  # mlp.0
            nn.SiLU(),                        # mlp.1 (æ¿€æ´»å‡½æ•°)
            nn.Dropout(dropout),              # mlp.2 (Dropout)
            nn.Linear(embed_dim, embed_dim),  # mlp.3
        )

        # åˆå§‹åŒ– - åŒ¹é…è®­ç»ƒä»£ç 
        nn.init.normal_(self.user_embedding.weight, std=0.02)

    def forward(self, user_indices):
        """
        ç¼–ç ç”¨æˆ·ID
        Args:
            user_indices: ç”¨æˆ·ç´¢å¼• [B]
        Returns:
            ç”¨æˆ·åµŒå…¥ [B, embed_dim]
        """
        # è·å–ç”¨æˆ·åµŒå…¥
        user_embeds = self.user_embedding(user_indices)

        # é€šè¿‡MLP
        user_embeds = self.mlp(user_embeds)

        return user_embeds

def generate_with_guidance(
    vae_path: str,
    unet_path: str,
    condition_encoder_path: str,
    user_ids: List[int],
    data_dir: str,
    num_images_per_user: int = 50,
    num_inference_steps: int = 50,
    guidance_scale: float = 1.0,  # æ–°å¢: æŒ‡å¯¼å¼ºåº¦
    batch_size: int = 10,  # æ–°å¢: æ‰¹é‡ç”Ÿæˆå¤§å°
    output_dir: str = "./generated_images",
    device: str = "auto",
    seed: int = 42
):
    """
    ä½¿ç”¨æŒ‡å¯¼å¼ºåº¦ç”Ÿæˆæ¡ä»¶å›¾åƒ
    
    Args:
        guidance_scale: æŒ‡å¯¼å¼ºåº¦
            - 1.0: çº¯æ¡ä»¶ç”Ÿæˆ (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
            - >1.0: åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ (CFG), å¢å¼ºæ¡ä»¶æ§åˆ¶
            - æ¨èå€¼: 1.0-3.0 (è¿‡é«˜å¯èƒ½å¯¼è‡´è¿‡é¥±å’Œ)
    """
    
    # è®¾å¤‡æ£€æµ‹
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    if guidance_scale == 1.0:
        print(f"ğŸ¯ ç”Ÿæˆæ¨¡å¼: çº¯æ¡ä»¶ç”Ÿæˆ (ä¸è®­ç»ƒæ—¶ä¸€è‡´)")
    else:
        print(f"ğŸ¯ ç”Ÿæˆæ¨¡å¼: CFGå¢å¼º (guidance_scale={guidance_scale})")
        print(f"âš ï¸  æ³¨æ„: è®­ç»ƒæ—¶ä½¿ç”¨çº¯æ¡ä»¶ç”Ÿæˆï¼Œå»ºè®®ä½¿ç”¨ --guidance_scale 1.0")
    
    # è®¾ç½®éšæœºç§å­
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)

    # è·å–ç”¨æˆ·IDæ˜ å°„ - ä¸è®­ç»ƒæ—¶MicroDopplerDataModuleå®Œå…¨ä¸€è‡´
    print("ğŸ” æ‰«ææ•°æ®é›†...")
    data_path = Path(data_dir)

    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„_get_all_usersé€»è¾‘
    all_users = []
    for user_dir in data_path.iterdir():
        if user_dir.is_dir() and user_dir.name.startswith('ID_'):
            try:
                user_id = int(user_dir.name.split('_')[1])
                all_users.append(user_id)
                # æ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯
                image_files = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                print(f"  ç”¨æˆ· {user_id:2d}: {len(image_files):3d} å¼ å›¾åƒ")
            except ValueError:
                continue

    # ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼šç®€å•æ’åº
    all_users = sorted(all_users)
    num_users = len(all_users)

    print(f"ğŸ”§ ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„æ˜ å°„é€»è¾‘: all_users.index(user_id)")
    
    print(f"ğŸ“Š å‘ç° {num_users} ä¸ªç”¨æˆ·: {all_users}")

    # æ˜¾ç¤ºæ˜ å°„é¢„è§ˆ
    print(f"ğŸ—ºï¸  æ˜ å°„é¢„è§ˆ:")
    for user_id in user_ids:
        if user_id in all_users:
            idx = all_users.index(user_id)
            print(f"    ç”¨æˆ·{user_id} â†’ ç´¢å¼•{idx} (all_usersæ˜ å°„)")
        else:
            idx = user_id - 1 if user_id > 0 else user_id
            print(f"    ç”¨æˆ·{user_id} â†’ ç´¢å¼•{idx} (å›é€€æ˜ å°„)")

    # éªŒè¯ç›®æ ‡ç”¨æˆ·å­˜åœ¨ï¼ˆå…è®¸å›é€€æ˜ å°„ï¼‰
    for user_id in user_ids:
        if user_id not in all_users and user_id <= 0:
            print(f"âŒ ç”¨æˆ· {user_id} æ— æ•ˆ")
            return False
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    
    # 1. VAE
    print("  åŠ è½½VAE...")
    vae = AutoencoderKL.from_pretrained(vae_path)
    vae.to(device)
    vae.eval()
    
    # 2. UNet
    print("  åŠ è½½UNet...")
    unet = UNet2DConditionModel.from_pretrained(unet_path)
    unet.to(device)
    unet.eval()
    
    # 3. æ¡ä»¶ç¼–ç å™¨ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
    print("  åŠ è½½æ¡ä»¶ç¼–ç å™¨...")
    condition_encoder = UserConditionEncoder(
        num_users=num_users,
        embed_dim=unet.config.cross_attention_dim,  # é»˜è®¤512
        dropout=0.1  # ä¸è®­ç»ƒæ—¶é»˜è®¤å€¼ä¸€è‡´
    )
    condition_encoder_state = torch.load(condition_encoder_path, map_location='cpu')
    condition_encoder.load_state_dict(condition_encoder_state)
    condition_encoder.to(device)
    condition_encoder.eval()
    
    # 4. è°ƒåº¦å™¨ - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
    print("  åˆ›å»ºè°ƒåº¦å™¨...")
    noise_scheduler = DDPMScheduler(
        num_train_timesteps=1000,  # ä¸è®­ç»ƒæ—¶é»˜è®¤å€¼ä¸€è‡´
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        trained_betas=None,
        variance_type="fixed_small",  # è®­ç»ƒæ—¶å‚æ•°
        clip_sample=False,
        prediction_type="epsilon",
        thresholding=False,  # è®­ç»ƒæ—¶å‚æ•°
        dynamic_thresholding_ratio=0.995,  # è®­ç»ƒæ—¶å‚æ•°
        clip_sample_range=1.0,  # è®­ç»ƒæ—¶å‚æ•°
        sample_max_value=1.0,  # è®­ç»ƒæ—¶å‚æ•°
    )
    ddim_scheduler = DDIMScheduler.from_config(noise_scheduler.config)
    ddim_scheduler.set_timesteps(num_inference_steps)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # ç”Ÿæˆå›¾åƒ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
    print(f"  è¾“å‡ºç›®å½•: {output_path}")
    print(f"  æ¨ç†æ­¥æ•°: {num_inference_steps}")
    print(f"  æ¯ç”¨æˆ·å›¾åƒæ•°: {num_images_per_user}")
    print(f"  æ‰¹é‡å¤§å°: {batch_size}")

    with torch.no_grad():
        for user_id in user_ids:
            print(f"\nğŸ‘¤ ç”Ÿæˆç”¨æˆ· {user_id} çš„å›¾åƒ...")

            user_dir = output_path / f"user_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)

            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„æ˜ å°„æ–¹å¼ï¼ˆåŒ…æ‹¬å›é€€é€»è¾‘ï¼‰
            try:
                user_idx = all_users.index(user_id)
                print(f"  ç”¨æˆ· {user_id} â†’ ç´¢å¼• {user_idx} (all_usersæ˜ å°„)")
            except ValueError:
                # å›é€€æ–¹æ¡ˆï¼šä¸è®­ç»ƒæ—¶ä¸€è‡´
                user_idx = user_id - 1 if user_id > 0 else user_id
                print(f"  ç”¨æˆ· {user_id} â†’ ç´¢å¼• {user_idx} (å›é€€æ˜ å°„: user_id-1)")
                print(f"  âš ï¸  è­¦å‘Š: ç”¨æˆ· {user_id} ä¸åœ¨all_usersä¸­ï¼Œä½¿ç”¨å›é€€æ˜ å°„")

            # æ‰¹é‡ç”Ÿæˆå›¾åƒ
            total_batches = (num_images_per_user + batch_size - 1) // batch_size
            img_counter = 0

            for batch_idx in range(total_batches):
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å®é™…å¤§å°
                current_batch_size = min(batch_size, num_images_per_user - batch_idx * batch_size)
                if current_batch_size <= 0:
                    break

                print(f"  ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: ç”Ÿæˆ {current_batch_size} å¼ å›¾åƒ...")

                # æ‰¹é‡åˆå§‹å™ªå£° (256Ã—256 â†’ 32Ã—32æ½œåœ¨ç©ºé—´)
                latents = torch.randn(current_batch_size, 4, 32, 32, device=device)

                # ç”¨æˆ·æ¡ä»¶ç¼–ç  (æ‰¹é‡)
                user_idx_tensor = torch.tensor([user_idx] * current_batch_size, device=device)
                encoder_hidden_states = condition_encoder(user_idx_tensor)
                encoder_hidden_states = encoder_hidden_states.unsqueeze(1)

                # æ‰¹é‡å»å™ªè¿‡ç¨‹
                for t in tqdm(ddim_scheduler.timesteps,
                            desc=f"ç”¨æˆ· {user_id}, æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}",
                            leave=False):
                    timestep = t.unsqueeze(0).to(device)

                    if guidance_scale == 1.0:
                        # çº¯æ¡ä»¶ç”Ÿæˆ (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
                        noise_pred = unet(
                            latents,
                            timestep,
                            encoder_hidden_states=encoder_hidden_states,
                            return_dict=False
                        )[0]
                    else:
                        # åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ (CFG)
                        # æ¡ä»¶é¢„æµ‹
                        noise_pred_cond = unet(
                            latents,
                            timestep,
                            encoder_hidden_states=encoder_hidden_states,
                            return_dict=False
                        )[0]

                        # æ— æ¡ä»¶é¢„æµ‹ (ç©ºæ¡ä»¶)
                        uncond_embeddings = torch.zeros_like(encoder_hidden_states)
                        noise_pred_uncond = unet(
                            latents,
                            timestep,
                            encoder_hidden_states=uncond_embeddings,
                            return_dict=False
                        )[0]

                        # CFGç»„åˆ
                        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)

                    # å»å™ªæ­¥éª¤
                    latents = ddim_scheduler.step(noise_pred, t, latents, return_dict=False)[0]

                # æ‰¹é‡VAEè§£ç 
                vae_model = vae.module if hasattr(vae, 'module') else vae
                latents = latents / vae_model.config.scaling_factor
                images = vae_model.decode(latents).sample

                # æ‰¹é‡è½¬æ¢ä¸ºPILå›¾åƒå¹¶ä¿å­˜
                images = images.clamp(0, 1)
                images = images.cpu().permute(0, 2, 3, 1).numpy()

                for i in range(current_batch_size):
                    image = (images[i] * 255).astype(np.uint8)
                    pil_image = Image.fromarray(image)

                    # æ·»åŠ ç”Ÿæˆä¿¡æ¯æ ‡ç­¾
                    try:
                        from PIL import ImageDraw, ImageFont
                        draw = ImageDraw.Draw(pil_image)
                        if guidance_scale == 1.0:
                            label_text = f"ID:{user_id} Idx:{user_idx} Pure"
                        else:
                            label_text = f"ID:{user_id} Idx:{user_idx} CFG:{guidance_scale}"
                        draw.text((5, 5), label_text, fill=(255, 255, 255))
                        draw.text((5, 20), f"Steps:{num_inference_steps} Batch:{current_batch_size}", fill=(255, 255, 255))
                    except:
                        pass

                    # ä¿å­˜å›¾åƒ
                    pil_image.save(user_dir / f"generated_{img_counter:03d}.png")
                    img_counter += 1

                # æ¸…ç†å†…å­˜
                del latents, images
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            print(f"  âœ… ç”¨æˆ· {user_id} å®Œæˆ: {user_dir} ({img_counter} å¼ å›¾åƒ)")
    
    print(f"\nğŸ‰ ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    return True

def main():
    parser = argparse.ArgumentParser(description="æ”¯æŒæŒ‡å¯¼å¼ºåº¦çš„æ¡ä»¶æ‰©æ•£å›¾åƒç”Ÿæˆ")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--vae_path", type=str, required=True, help="VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--unet_path", type=str, required=True, help="UNetæ¨¡å‹è·¯å¾„")
    parser.add_argument("--condition_encoder_path", type=str, required=True, help="æ¡ä»¶ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½• (ç”¨äºè·å–ç”¨æˆ·æ˜ å°„)")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--user_ids", type=int, nargs="+", required=True, help="è¦ç”Ÿæˆçš„ç”¨æˆ·IDåˆ—è¡¨")
    parser.add_argument("--num_images_per_user", type=int, default=50, help="æ¯ç”¨æˆ·ç”Ÿæˆå›¾åƒæ•°")
    parser.add_argument("--num_inference_steps", type=int, default=20, help="DDIMæ¨ç†æ­¥æ•° (è®­ç»ƒæ—¶é»˜è®¤20)")
    parser.add_argument("--guidance_scale", type=float, default=1.0,
                       help="æŒ‡å¯¼å¼ºåº¦ (1.0=çº¯æ¡ä»¶ç”Ÿæˆï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´)")
    parser.add_argument("--batch_size", type=int, default=10, help="æ‰¹é‡ç”Ÿæˆå¤§å° (å……åˆ†åˆ©ç”¨æ˜¾å­˜)")
    parser.add_argument("--output_dir", type=str, default="./generated_images", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="auto", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    print("ğŸ¨ æ”¯æŒæŒ‡å¯¼å¼ºåº¦çš„æ¡ä»¶æ‰©æ•£å›¾åƒç”Ÿæˆ")
    print("=" * 60)
    print(f"ğŸ“Š æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"ğŸ¯ ç›®æ ‡ç”¨æˆ·: {args.user_ids}")
    print(f"ğŸ›ï¸  æŒ‡å¯¼å¼ºåº¦: {args.guidance_scale}")
    print(f"ğŸ“ˆ æ¨ç†æ­¥æ•°: {args.num_inference_steps}")
    print(f"ğŸ“¦ æ‰¹é‡å¤§å°: {args.batch_size}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")

    success = generate_with_guidance(
        vae_path=args.vae_path,
        unet_path=args.unet_path,
        condition_encoder_path=args.condition_encoder_path,
        user_ids=args.user_ids,
        data_dir=args.data_dir,
        num_images_per_user=args.num_images_per_user,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        batch_size=args.batch_size,
        output_dir=args.output_dir,
        device=args.device,
        seed=args.seed
    )
    
    if success:
        print("\nğŸ‰ ç”ŸæˆæˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("\nâŒ ç”Ÿæˆå¤±è´¥!")
        return 1

if __name__ == "__main__":
    exit(main())
