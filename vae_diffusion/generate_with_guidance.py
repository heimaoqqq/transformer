#!/usr/bin/env python3
"""
æ”¯æŒæŒ‡å¯¼å¼ºåº¦çš„æ¡ä»¶æ‰©æ•£å›¾åƒç”Ÿæˆè„šæœ¬
ä¸“é—¨é’ˆå¯¹256Ã—256å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾åƒä¼˜åŒ–

æ–°å¢åŠŸèƒ½:
- æ”¯æŒåˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ (CFG)
- å¯é…ç½®æŒ‡å¯¼å¼ºåº¦ (guidance_scale)
- é’ˆå¯¹256Ã—256å›¾åƒä¼˜åŒ–
- è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†æ ¼å¼
"""

import os
import torch
import numpy as np
from PIL import Image
from pathlib import Path
import argparse
from typing import List
from tqdm import tqdm

from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# ç›´æ¥å®šä¹‰UserConditionEncoderç±»ï¼Œé¿å…å¯¼å…¥é—®é¢˜
import torch.nn as nn

class UserConditionEncoder(nn.Module):
    """ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨ - å®Œå…¨åŒ¹é…è®­ç»ƒæ—¶çš„ç»“æ„"""
    def __init__(self, num_users: int, embed_dim: int = 512, dropout: float = 0.1):
        super().__init__()
        self.num_users = num_users
        self.embed_dim = embed_dim

        # ç”¨æˆ·åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(num_users, embed_dim)

        # MLPå±‚ - å®Œå…¨åŒ¹é…è®­ç»ƒä»£ç ç»“æ„
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),  # mlp.0
            nn.SiLU(),                        # mlp.1 (æ¿€æ´»å‡½æ•°)
            nn.Dropout(dropout),              # mlp.2 (Dropout)
            nn.Linear(embed_dim, embed_dim),  # mlp.3
        )

        # åˆå§‹åŒ– - åŒ¹é…è®­ç»ƒä»£ç 
        nn.init.normal_(self.user_embedding.weight, std=0.02)

    def forward(self, user_indices):
        """
        ç¼–ç ç”¨æˆ·ID
        Args:
            user_indices: ç”¨æˆ·ç´¢å¼• [B]
        Returns:
            ç”¨æˆ·åµŒå…¥ [B, embed_dim]
        """
        # è·å–ç”¨æˆ·åµŒå…¥
        user_embeds = self.user_embedding(user_indices)

        # é€šè¿‡MLP
        user_embeds = self.mlp(user_embeds)

        return user_embeds

def generate_with_guidance(
    vae_path: str,
    unet_path: str,
    condition_encoder_path: str,
    user_ids: List[int],
    data_dir: str,
    num_images_per_user: int = 50,
    num_inference_steps: int = 50,
    guidance_scale: float = 1.0,  # æ–°å¢: æŒ‡å¯¼å¼ºåº¦
    output_dir: str = "./generated_images",
    device: str = "auto",
    seed: int = 42
):
    """
    ä½¿ç”¨æŒ‡å¯¼å¼ºåº¦ç”Ÿæˆæ¡ä»¶å›¾åƒ
    
    Args:
        guidance_scale: æŒ‡å¯¼å¼ºåº¦
            - 1.0: çº¯æ¡ä»¶ç”Ÿæˆ (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
            - >1.0: åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ (CFG), å¢å¼ºæ¡ä»¶æ§åˆ¶
            - æ¨èå€¼: 1.0-3.0 (è¿‡é«˜å¯èƒ½å¯¼è‡´è¿‡é¥±å’Œ)
    """
    
    # è®¾å¤‡æ£€æµ‹
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ¯ æŒ‡å¯¼å¼ºåº¦: {guidance_scale} ({'çº¯æ¡ä»¶' if guidance_scale == 1.0 else 'CFGå¢å¼º'})")
    
    # è®¾ç½®éšæœºç§å­
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)

    # è·å–ç”¨æˆ·IDæ˜ å°„
    print("ğŸ” æ‰«ææ•°æ®é›†...")
    data_path = Path(data_dir)
    user_labels = []  # æ”¶é›†æ‰€æœ‰ç”¨æˆ·æ ‡ç­¾ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰

    for user_dir in data_path.iterdir():
        if user_dir.is_dir() and user_dir.name.startswith('ID_'):
            try:
                user_id = int(user_dir.name.split('_')[1])
                image_files = list(user_dir.glob("*.png")) + list(user_dir.glob("*.jpg"))
                if len(image_files) > 0:
                    # ä¸ºæ¯ä¸ªå›¾åƒæ·»åŠ ç”¨æˆ·æ ‡ç­¾ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„æ•°æ®åŠ è½½ï¼‰
                    user_labels.extend([user_id] * len(image_files))
                    print(f"  ç”¨æˆ· {user_id:2d}: {len(image_files):3d} å¼ å›¾åƒ")
            except ValueError:
                continue

    # ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ˜ å°„é€»è¾‘
    unique_users = sorted(list(set(user_labels)))
    user_id_to_idx = {user: idx for idx, user in enumerate(unique_users)}
    num_users = len(unique_users)
    
    print(f"ğŸ“Š å‘ç° {num_users} ä¸ªç”¨æˆ·: {unique_users}")
    print(f"ğŸ—ºï¸  ç”¨æˆ·æ˜ å°„: {user_id_to_idx}")
    print(f"ğŸ”§ ä¿®å¤: ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„æ˜ å°„é€»è¾‘")
    
    # éªŒè¯ç›®æ ‡ç”¨æˆ·
    for user_id in user_ids:
        if user_id not in user_id_to_idx:
            print(f"âŒ ç”¨æˆ· {user_id} ä¸åœ¨æ•°æ®é›†ä¸­")
            return False
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    
    # 1. VAE
    print("  åŠ è½½VAE...")
    vae = AutoencoderKL.from_pretrained(vae_path)
    vae.to(device)
    vae.eval()
    
    # 2. UNet
    print("  åŠ è½½UNet...")
    unet = UNet2DConditionModel.from_pretrained(unet_path)
    unet.to(device)
    unet.eval()
    
    # 3. æ¡ä»¶ç¼–ç å™¨
    print("  åŠ è½½æ¡ä»¶ç¼–ç å™¨...")
    condition_encoder = UserConditionEncoder(
        num_users=num_users,
        embed_dim=unet.config.cross_attention_dim
    )
    condition_encoder_state = torch.load(condition_encoder_path, map_location='cpu')
    condition_encoder.load_state_dict(condition_encoder_state)
    condition_encoder.to(device)
    condition_encoder.eval()
    
    # 4. è°ƒåº¦å™¨
    print("  åˆ›å»ºè°ƒåº¦å™¨...")
    noise_scheduler = DDPMScheduler(
        num_train_timesteps=1000,
        beta_start=0.00085,
        beta_end=0.012,
        beta_schedule="scaled_linear",
        clip_sample=False,
        prediction_type="epsilon",
    )
    ddim_scheduler = DDIMScheduler.from_config(noise_scheduler.config)
    ddim_scheduler.set_timesteps(num_inference_steps)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # ç”Ÿæˆå›¾åƒ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
    print(f"  è¾“å‡ºç›®å½•: {output_path}")
    print(f"  æ¨ç†æ­¥æ•°: {num_inference_steps}")
    print(f"  æ¯ç”¨æˆ·å›¾åƒæ•°: {num_images_per_user}")
    
    with torch.no_grad():
        for user_id in user_ids:
            print(f"\nğŸ‘¤ ç”Ÿæˆç”¨æˆ· {user_id} çš„å›¾åƒ...")
            
            user_dir = output_path / f"user_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)
            
            user_idx = user_id_to_idx[user_id]
            print(f"  ç”¨æˆ· {user_id} â†’ ç´¢å¼• {user_idx}")
            
            for img_idx in range(num_images_per_user):
                # åˆå§‹å™ªå£° (256Ã—256 â†’ 32Ã—32æ½œåœ¨ç©ºé—´)
                latents = torch.randn(1, 4, 32, 32, device=device)
                
                # ç”¨æˆ·æ¡ä»¶ç¼–ç 
                user_idx_tensor = torch.tensor([user_idx], device=device)
                encoder_hidden_states = condition_encoder(user_idx_tensor)
                encoder_hidden_states = encoder_hidden_states.unsqueeze(1)
                
                # å»å™ªè¿‡ç¨‹
                for t in tqdm(ddim_scheduler.timesteps, 
                            desc=f"ç”¨æˆ· {user_id}, å›¾åƒ {img_idx+1}/{num_images_per_user}",
                            leave=False):
                    timestep = t.unsqueeze(0).to(device)
                    
                    if guidance_scale == 1.0:
                        # çº¯æ¡ä»¶ç”Ÿæˆ (ä¸è®­ç»ƒæ—¶ç›¸åŒ)
                        noise_pred = unet(
                            latents,
                            timestep,
                            encoder_hidden_states=encoder_hidden_states,
                            return_dict=False
                        )[0]
                    else:
                        # åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ (CFG)
                        # æ¡ä»¶é¢„æµ‹
                        noise_pred_cond = unet(
                            latents,
                            timestep,
                            encoder_hidden_states=encoder_hidden_states,
                            return_dict=False
                        )[0]
                        
                        # æ— æ¡ä»¶é¢„æµ‹ (ç©ºæ¡ä»¶)
                        uncond_embeddings = torch.zeros_like(encoder_hidden_states)
                        noise_pred_uncond = unet(
                            latents,
                            timestep,
                            encoder_hidden_states=uncond_embeddings,
                            return_dict=False
                        )[0]
                        
                        # CFGç»„åˆ
                        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
                    
                    # å»å™ªæ­¥éª¤
                    latents = ddim_scheduler.step(noise_pred, t, latents, return_dict=False)[0]
                
                # VAEè§£ç 
                vae_model = vae.module if hasattr(vae, 'module') else vae
                latents = latents / vae_model.config.scaling_factor
                image = vae_model.decode(latents).sample
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                image = image.clamp(0, 1)
                image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
                image = (image * 255).astype(np.uint8)
                
                # ä¿å­˜å›¾åƒ
                pil_image = Image.fromarray(image)
                
                # æ·»åŠ ç”Ÿæˆä¿¡æ¯æ ‡ç­¾
                try:
                    from PIL import ImageDraw, ImageFont
                    draw = ImageDraw.Draw(pil_image)
                    label_text = f"ID:{user_id} CFG:{guidance_scale} Steps:{num_inference_steps}"
                    draw.text((5, 5), label_text, fill=(255, 255, 255))
                    draw.text((5, 20), f"256x256 Generated", fill=(255, 255, 255))
                except:
                    pass
                
                pil_image.save(user_dir / f"generated_{img_idx:03d}.png")
                
                # æ¸…ç†å†…å­˜
                del latents
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            print(f"  âœ… ç”¨æˆ· {user_id} å®Œæˆ: {user_dir}")
    
    print(f"\nğŸ‰ ç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    return True

def main():
    parser = argparse.ArgumentParser(description="æ”¯æŒæŒ‡å¯¼å¼ºåº¦çš„æ¡ä»¶æ‰©æ•£å›¾åƒç”Ÿæˆ")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--vae_path", type=str, required=True, help="VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--unet_path", type=str, required=True, help="UNetæ¨¡å‹è·¯å¾„")
    parser.add_argument("--condition_encoder_path", type=str, required=True, help="æ¡ä»¶ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½• (ç”¨äºè·å–ç”¨æˆ·æ˜ å°„)")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--user_ids", type=int, nargs="+", required=True, help="è¦ç”Ÿæˆçš„ç”¨æˆ·IDåˆ—è¡¨")
    parser.add_argument("--num_images_per_user", type=int, default=50, help="æ¯ç”¨æˆ·ç”Ÿæˆå›¾åƒæ•°")
    parser.add_argument("--num_inference_steps", type=int, default=50, help="DDIMæ¨ç†æ­¥æ•°")
    parser.add_argument("--guidance_scale", type=float, default=1.0, 
                       help="æŒ‡å¯¼å¼ºåº¦ (1.0=çº¯æ¡ä»¶, >1.0=CFGå¢å¼º, æ¨è1.0-3.0)")
    parser.add_argument("--output_dir", type=str, default="./generated_images", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="auto", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    print("ğŸ¨ æ”¯æŒæŒ‡å¯¼å¼ºåº¦çš„æ¡ä»¶æ‰©æ•£å›¾åƒç”Ÿæˆ")
    print("=" * 60)
    print(f"ğŸ“Š æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"ğŸ¯ ç›®æ ‡ç”¨æˆ·: {args.user_ids}")
    print(f"ğŸ›ï¸  æŒ‡å¯¼å¼ºåº¦: {args.guidance_scale}")
    print(f"ğŸ“ˆ æ¨ç†æ­¥æ•°: {args.num_inference_steps}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    success = generate_with_guidance(
        vae_path=args.vae_path,
        unet_path=args.unet_path,
        condition_encoder_path=args.condition_encoder_path,
        user_ids=args.user_ids,
        data_dir=args.data_dir,
        num_images_per_user=args.num_images_per_user,
        num_inference_steps=args.num_inference_steps,
        guidance_scale=args.guidance_scale,
        output_dir=args.output_dir,
        device=args.device,
        seed=args.seed
    )
    
    if success:
        print("\nğŸ‰ ç”ŸæˆæˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("\nâŒ ç”Ÿæˆå¤±è´¥!")
        return 1

if __name__ == "__main__":
    exit(main())
