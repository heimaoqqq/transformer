#!/usr/bin/env python3
"""
ç»ˆæKaggleç¯å¢ƒä¿®å¤è„šæœ¬
å½»åº•æ¸…ç†å¹¶é‡å»ºæ‰€æœ‰ä¾èµ–ï¼Œè§£å†³æ‰€æœ‰ç‰ˆæœ¬å†²çª
"""

import subprocess
import sys
import os
import shutil

def run_command(cmd, description="", ignore_errors=False):
    """è¿è¡Œå‘½ä»¤"""
    print(f"ğŸ”„ {description}")
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        if result.returncode == 0 or ignore_errors:
            print(f"âœ… {description} - å®Œæˆ")
            return True
        else:
            print(f"âŒ {description} - å¤±è´¥")
            if result.stderr and not ignore_errors:
                print(f"é”™è¯¯: {result.stderr}")
            return False
    except Exception as e:
        print(f"âŒ {description} - å¼‚å¸¸: {e}")
        return False

def nuclear_cleanup():
    """æ ¸å¼¹çº§æ¸…ç† - å½»åº•æ¸…é™¤æ‰€æœ‰ç›¸å…³åŒ…"""
    print("ğŸ’¥ æ ¸å¼¹çº§æ¸…ç† - å½»åº•æ¸…é™¤æ‰€æœ‰ç›¸å…³åŒ…")
    print("=" * 50)
    
    # 1. æ¸…ç†Pythonç¼“å­˜
    print("\n1ï¸âƒ£ æ¸…ç†Pythonç¼“å­˜...")
    try:
        import sys
        for module in list(sys.modules.keys()):
            if any(pkg in module for pkg in ['numpy', 'torch', 'diffusers', 'transformers', 'scipy', 'sklearn']):
                del sys.modules[module]
        print("âœ… Pythonæ¨¡å—ç¼“å­˜å·²æ¸…ç†")
    except:
        pass
    
    # 2. å¸è½½æ‰€æœ‰ç›¸å…³åŒ…
    print("\n2ï¸âƒ£ å¸è½½æ‰€æœ‰ç›¸å…³åŒ…...")
    packages_to_remove = [
        # æ ¸å¿ƒåŒ…
        "torch", "torchvision", "torchaudio", "torchtext",
        "numpy", "scipy", "scikit-learn", "matplotlib",
        # AIåŒ…
        "diffusers", "transformers", "accelerate", "huggingface_hub",
        # ä¾èµ–åŒ…
        "pillow", "opencv-python", "opencv-contrib-python",
        "einops", "tqdm", "packaging", "filelock",
        # å¯èƒ½å†²çªçš„åŒ…
        "jax", "jaxlib", "flax", "optax",
        "tensorflow", "tensorflow-gpu",
        "pandas", "seaborn"
    ]
    
    for package in packages_to_remove:
        run_command(f"pip uninstall -y {package}", f"å¸è½½ {package}", ignore_errors=True)
    
    # 3. å¼ºåˆ¶æ¸…ç†pipç¼“å­˜
    print("\n3ï¸âƒ£ æ¸…ç†pipç¼“å­˜...")
    run_command("pip cache purge", "æ¸…ç†pipç¼“å­˜", ignore_errors=True)
    
    # 4. æ¸…ç†condaç¼“å­˜ (å¦‚æœå­˜åœ¨)
    print("\n4ï¸âƒ£ æ¸…ç†condaç¼“å­˜...")
    run_command("conda clean -a -y", "æ¸…ç†condaç¼“å­˜", ignore_errors=True)

def install_base_system():
    """å®‰è£…åŸºç¡€ç³»ç»ŸåŒ…"""
    print("\nğŸ—ï¸  å®‰è£…åŸºç¡€ç³»ç»ŸåŒ…")
    print("=" * 30)
    
    # 1. å‡çº§pipå’Œsetuptools
    print("\n1ï¸âƒ£ å‡çº§åŸºç¡€å·¥å…·...")
    run_command("pip install --upgrade pip setuptools wheel", "å‡çº§pipå’Œsetuptools")
    
    # 2. å®‰è£…NumPy (æœ€ç¨³å®šç‰ˆæœ¬)
    print("\n2ï¸âƒ£ å®‰è£…NumPy...")
    numpy_versions = ["1.24.4", "1.23.5", "1.21.6"]
    for version in numpy_versions:
        if run_command(f"pip install numpy=={version}", f"å®‰è£…NumPy {version}"):
            break
    
    # 3. å®‰è£…SciPy (å…¼å®¹NumPy)
    print("\n3ï¸âƒ£ å®‰è£…SciPy...")
    run_command("pip install scipy==1.10.1", "å®‰è£…SciPy")

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("\nğŸ” æ£€æŸ¥GPUç¯å¢ƒ")
    print("=" * 30)

    # 1. æ£€æŸ¥nvidia-smi
    print("\n1ï¸âƒ£ æ£€æŸ¥nvidia-smi...")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smiå¯ç”¨")
            # æå–GPUä¿¡æ¯
            lines = result.stdout.split('\n')
            gpu_found = False
            for line in lines:
                if any(gpu in line for gpu in ['Tesla', 'T4', 'P100', 'V100', 'A100']):
                    print(f"   ğŸ¯ æ£€æµ‹åˆ°GPU: {line.strip()}")
                    gpu_found = True

            if not gpu_found:
                print("âš ï¸  nvidia-smiè¿è¡Œä½†æœªæ£€æµ‹åˆ°GPU")
                return False
            return True
        else:
            print("âŒ nvidia-smiå¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ nvidia-smiå¼‚å¸¸: {e}")
        return False

def install_pytorch_stack():
    """å®‰è£…PyTorchæŠ€æœ¯æ ˆ"""
    print("\nğŸ”¥ å®‰è£…PyTorchæŠ€æœ¯æ ˆ")
    print("=" * 30)

    # æ£€æŸ¥GPUç¯å¢ƒ
    has_gpu = check_gpu_environment()

    if has_gpu:
        print("\nğŸ¯ æ£€æµ‹åˆ°GPUï¼Œå®‰è£…CUDAç‰ˆæœ¬PyTorch")
        # GPUç¯å¢ƒï¼šä¼˜å…ˆå®‰è£…CUDAç‰ˆæœ¬
        pytorch_options = [
            # é€‰é¡¹1: CUDA 12.1ç‰ˆæœ¬
            {
                "cmd": "pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu121",
                "desc": "PyTorch 2.1.0 CUDA 12.1ç‰ˆæœ¬"
            },
            # é€‰é¡¹2: CUDA 11.8ç‰ˆæœ¬
            {
                "cmd": "pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118",
                "desc": "PyTorch 2.0.1 CUDA 11.8ç‰ˆæœ¬"
            },
            # é€‰é¡¹3: é»˜è®¤ç‰ˆæœ¬
            {
                "cmd": "pip install torch==2.1.0 torchvision==0.16.0",
                "desc": "PyTorch 2.1.0 é»˜è®¤ç‰ˆæœ¬"
            }
        ]
    else:
        print("\nğŸ’» æœªæ£€æµ‹åˆ°GPUï¼Œå®‰è£…CPUç‰ˆæœ¬PyTorch")
        # CPUç¯å¢ƒï¼šå®‰è£…CPUç‰ˆæœ¬
        pytorch_options = [
            # é€‰é¡¹1: CPUç‰ˆæœ¬
            {
                "cmd": "pip install torch==2.1.0+cpu torchvision==0.16.0+cpu --index-url https://download.pytorch.org/whl/cpu",
                "desc": "PyTorch 2.1.0 CPUç‰ˆæœ¬"
            },
            # é€‰é¡¹2: è¾ƒæ—§ç‰ˆæœ¬
            {
                "cmd": "pip install torch==1.13.1 torchvision==0.14.1",
                "desc": "PyTorch 1.13.1 ä¿å®ˆç‰ˆæœ¬"
            }
        ]

    for i, option in enumerate(pytorch_options, 1):
        print(f"\nå°è¯•æ–¹æ¡ˆ {i}: {option['desc']}")
        if run_command(option["cmd"], option["desc"]):
            print(f"âœ… PyTorchæ–¹æ¡ˆ {i} å®‰è£…æˆåŠŸ")
            break
    else:
        print("âŒ æ‰€æœ‰PyTorchå®‰è£…æ–¹æ¡ˆéƒ½å¤±è´¥")
        return False

    return True

def install_ai_packages():
    """å®‰è£…AIç›¸å…³åŒ… - å¼ºåˆ¶ä½¿ç”¨å…¼å®¹ç‰ˆæœ¬ç»„åˆ"""
    print("\nğŸ¤– å®‰è£…AIç›¸å…³åŒ…")
    print("=" * 30)

    # å¼ºåˆ¶ä½¿ç”¨ç»è¿‡éªŒè¯çš„ç¨³å®šç‰ˆæœ¬ç»„åˆ - ç¡®ä¿ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´
    # è¿™äº›ç‰ˆæœ¬ç»è¿‡æµ‹è¯•ï¼Œè§£å†³äº† cached_download å…¼å®¹æ€§é—®é¢˜
    ai_packages = [
        ("huggingface_hub==0.16.4", "HuggingFace Hub"),  # åŒ…å« cached_downloadï¼Œä¸diffuserså…¼å®¹
        ("transformers==4.30.2", "Transformers"),        # ç¨³å®šç‰ˆæœ¬ï¼Œæ”¯æŒæ‰€æœ‰åŠŸèƒ½
        ("diffusers==0.21.4", "Diffusers"),              # ä¸ huggingface_hub 0.16.4 å®Œå…¨å…¼å®¹
        ("accelerate==0.20.3", "Accelerate")             # ç¨³å®šç‰ˆæœ¬ï¼Œæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
    ]

    print("ğŸ”§ å¼ºåˆ¶å®‰è£…å…¼å®¹ç‰ˆæœ¬ç»„åˆä»¥ç¡®ä¿ç¨³å®šæ€§...")

    success_count = 0
    for package, name in ai_packages:
        # å…ˆå°è¯•å¼ºåˆ¶é‡è£…ä»¥ç¡®ä¿ç‰ˆæœ¬æ­£ç¡®
        if run_command(f"pip install --force-reinstall {package}", f"å¼ºåˆ¶å®‰è£… {name}"):
            success_count += 1
        else:
            # å¦‚æœå¼ºåˆ¶é‡è£…å¤±è´¥ï¼Œå°è¯•æ™®é€šå®‰è£…
            print(f"   âš ï¸  {name} å¼ºåˆ¶å®‰è£…å¤±è´¥ï¼Œå°è¯•æ™®é€šå®‰è£…...")
            if run_command(f"pip install {package}", f"å®‰è£… {name}"):
                success_count += 1
            else:
                print(f"   âŒ {name} å®‰è£…å¤±è´¥")

    print(f"\nğŸ“Š AIåŒ…å®‰è£…ç»“æœ: {success_count}/{len(ai_packages)} æˆåŠŸ")

    # éªŒè¯å…³é”®å…¼å®¹æ€§
    print("\nğŸ” éªŒè¯å…³é”®å…¼å®¹æ€§...")
    try:
        from huggingface_hub import cached_download
        print("âœ… cached_download éªŒè¯æˆåŠŸ")
        return True
    except ImportError:
        print("âŒ cached_download ä»ç„¶ä¸å¯ç”¨")
        print("ğŸ”§ æ‰§è¡Œå¼ºåŠ›ä¿®å¤...")

        # å¼ºåŠ›ä¿®å¤ï¼šå®Œå…¨é‡è£…å…³é”®åŒ…
        critical_packages = [
            "huggingface_hub==0.16.4",
            "diffusers==0.21.4"
        ]

        for package in critical_packages:
            print(f"ğŸ”„ å¼ºåŠ›é‡è£… {package}...")
            # å…ˆå¸è½½
            package_name = package.split('==')[0]
            run_command(f"pip uninstall {package_name} -y", f"å¸è½½ {package_name}")
            # æ¸…ç†ç¼“å­˜
            run_command("pip cache purge", "æ¸…ç†ç¼“å­˜")
            # é‡è£…
            run_command(f"pip install --no-cache-dir {package}", f"é‡è£… {package}")

        # æœ€ç»ˆéªŒè¯
        try:
            # æ¸…ç†æ¨¡å—ç¼“å­˜
            import sys
            modules_to_clear = ['huggingface_hub', 'diffusers']
            for module in modules_to_clear:
                if module in sys.modules:
                    del sys.modules[module]

            from huggingface_hub import cached_download
            print("âœ… å¼ºåŠ›ä¿®å¤æˆåŠŸ")
            return True
        except ImportError:
            print("âŒ å¼ºåŠ›ä¿®å¤å¤±è´¥")
            print("ğŸ’¡ å»ºè®®: é‡å¯å†…æ ¸åé‡æ–°è¿è¡Œæ­¤è„šæœ¬")
            return False
    except Exception as e:
        print(f"âš ï¸  å…¶ä»–éªŒè¯é—®é¢˜: {e}")
        return success_count == len(ai_packages)

def install_utility_packages():
    """å®‰è£…å·¥å…·åŒ…"""
    print("\nğŸ› ï¸  å®‰è£…å·¥å…·åŒ…")
    print("=" * 30)
    
    utility_packages = [
        ("pillow==9.5.0", "Pillow"),
        ("opencv-python==4.8.0.76", "OpenCV"),
        ("matplotlib==3.7.2", "Matplotlib"),
        ("scikit-learn==1.3.0", "Scikit-learn"),
        ("tqdm==4.65.0", "TQDM"),
        ("einops==0.6.1", "Einops"),
        ("packaging>=20.0", "Packaging"),
        ("filelock>=3.0", "FileLock")
    ]
    
    for package, name in utility_packages:
        run_command(f"pip install {package}", f"å®‰è£… {name}")

def test_gpu_functionality():
    """æµ‹è¯•GPUåŠŸèƒ½"""
    print("\nğŸ® GPUåŠŸèƒ½æµ‹è¯•:")

    try:
        import torch

        # æ£€æŸ¥PyTorchç‰ˆæœ¬
        pytorch_version = torch.__version__
        print(f"âœ… PyTorchç‰ˆæœ¬: {pytorch_version}")

        # æ£€æŸ¥CUDAç¼–è¯‘æ”¯æŒ
        cuda_version = torch.version.cuda
        print(f"âœ… CUDAç¼–è¯‘ç‰ˆæœ¬: {cuda_version}")

        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        cuda_available = torch.cuda.is_available()
        print(f"{'âœ…' if cuda_available else 'âŒ'} CUDAå¯ç”¨: {cuda_available}")

        if cuda_available:
            # GPUè¯¦ç»†ä¿¡æ¯
            device_count = torch.cuda.device_count()
            print(f"âœ… GPUæ•°é‡: {device_count}")

            for i in range(device_count):
                gpu_name = torch.cuda.get_device_name(i)
                props = torch.cuda.get_device_properties(i)
                memory_gb = props.total_memory / 1024**3
                print(f"âœ… GPU {i}: {gpu_name}")
                print(f"   å†…å­˜: {memory_gb:.1f} GB")
                print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")

            # æµ‹è¯•GPUæ“ä½œ
            device = torch.device('cuda:0')
            test_tensor = torch.randn(100, 100, device=device)
            result = torch.mm(test_tensor, test_tensor.t())

            print("âœ… GPUå¼ é‡æ“ä½œæˆåŠŸ")
            print(f"   è®¾å¤‡: {test_tensor.device}")

            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            torch.cuda.empty_cache()
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
            print(f"   å·²åˆ†é…å†…å­˜: {memory_allocated:.1f} MB")

            return True
        else:
            # CPUæ¨¡å¼
            print("â„¹ï¸  ä½¿ç”¨CPUæ¨¡å¼")
            test_tensor = torch.randn(100, 100)
            result = torch.mm(test_tensor, test_tensor.t())
            print("âœ… CPUå¼ é‡æ“ä½œæˆåŠŸ")

            # æ£€æŸ¥æ˜¯å¦ä¸ºCPUç‰ˆæœ¬
            if '+cpu' in pytorch_version:
                print("âš ï¸  æ£€æµ‹åˆ°CPUç‰ˆæœ¬PyTorch")
                print("   å¦‚éœ€GPUæ”¯æŒï¼Œè¯·é‡æ–°å®‰è£…CUDAç‰ˆæœ¬")
                return False
            else:
                print("âš ï¸  CUDAä¸å¯ç”¨ä½†PyTorchæ”¯æŒCUDA")
                print("   å¯èƒ½æ˜¯é©±åŠ¨æˆ–ç¯å¢ƒé—®é¢˜")
                return False

    except Exception as e:
        print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def comprehensive_test():
    """å…¨é¢æµ‹è¯•"""
    print("\nğŸ§ª å…¨é¢åŠŸèƒ½æµ‹è¯•")
    print("=" * 30)

    # æ¸…ç†æ¨¡å—ç¼“å­˜
    modules_to_clear = ['numpy', 'torch', 'torchvision', 'diffusers', 'transformers', 'scipy', 'sklearn']
    for module in modules_to_clear:
        if module in sys.modules:
            del sys.modules[module]

    test_results = {}

    # æµ‹è¯•0: GPUåŠŸèƒ½
    print("\n0ï¸âƒ£ æµ‹è¯•GPUåŠŸèƒ½...")
    gpu_ok = test_gpu_functionality()
    test_results['gpu'] = gpu_ok
    
    # æµ‹è¯•1: NumPy
    print("\n1ï¸âƒ£ æµ‹è¯•NumPy...")
    try:
        import numpy as np
        test_array = np.random.randn(3, 3)
        result = np.mean(test_array)
        print(f"âœ… NumPy {np.__version__}: åŠŸèƒ½æ­£å¸¸")
        test_results['numpy'] = True
    except Exception as e:
        print(f"âŒ NumPyæµ‹è¯•å¤±è´¥: {e}")
        test_results['numpy'] = False
    
    # æµ‹è¯•2: PyTorch
    print("\n2ï¸âƒ£ æµ‹è¯•PyTorch...")
    try:
        import torch
        test_tensor = torch.randn(2, 3)
        result = torch.mean(test_tensor)
        print(f"âœ… PyTorch {torch.__version__}: åŠŸèƒ½æ­£å¸¸")
        
        # æ£€æŸ¥CUDA
        if torch.cuda.is_available():
            print(f"âœ… CUDAå¯ç”¨: {torch.cuda.get_device_name(0)}")
        else:
            print("â„¹ï¸  ä½¿ç”¨CPUæ¨¡å¼")
        
        test_results['torch'] = True
    except Exception as e:
        print(f"âŒ PyTorchæµ‹è¯•å¤±è´¥: {e}")
        test_results['torch'] = False
    
    # æµ‹è¯•3: TorchVision
    print("\n3ï¸âƒ£ æµ‹è¯•TorchVision...")
    try:
        import torchvision
        print(f"âœ… TorchVision {torchvision.__version__}: å¯¼å…¥æˆåŠŸ")
        test_results['torchvision'] = True
    except Exception as e:
        print(f"âŒ TorchVisionæµ‹è¯•å¤±è´¥: {e}")
        test_results['torchvision'] = False
    
    # æµ‹è¯•4: Diffusers (å…³é”®å…¼å®¹æ€§æµ‹è¯•)
    print("\n4ï¸âƒ£ æµ‹è¯•Diffusers...")
    try:
        # é¦–å…ˆæµ‹è¯• cached_download å…¼å®¹æ€§
        from huggingface_hub import cached_download
        print("âœ… cached_download å¯¼å…¥æˆåŠŸ")

        import diffusers
        from diffusers import AutoencoderKL, UNet2DConditionModel
        print(f"âœ… Diffusers {diffusers.__version__}: å¯¼å…¥æˆåŠŸ")
        test_results['diffusers'] = True
    except ImportError as e:
        if 'cached_download' in str(e):
            print(f"âŒ Diffusersæµ‹è¯•å¤±è´¥: cached_download å…¼å®¹æ€§é—®é¢˜")
            print("ğŸ”§ è¿™è¡¨æ˜éœ€è¦é‡æ–°è¿è¡Œç¯å¢ƒä¿®å¤")
            print("ğŸ’¡ å»ºè®®: é‡å¯å†…æ ¸åé‡æ–°è¿è¡Œ ultimate_fix_kaggle.py")
        else:
            print(f"âŒ Diffusersæµ‹è¯•å¤±è´¥: {e}")
        test_results['diffusers'] = False
    except Exception as e:
        print(f"âŒ Diffusersæµ‹è¯•å¤±è´¥: {e}")
        test_results['diffusers'] = False
    
    # æµ‹è¯•5: VAEåŠŸèƒ½ (ä¸é¡¹ç›®é…ç½®ä¸€è‡´)
    print("\n5ï¸âƒ£ æµ‹è¯•VAEåŠŸèƒ½...")
    try:
        from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler
        import torch

        # åˆ›å»ºä¸é¡¹ç›®ä¸€è‡´çš„VAE (128Ã—128 â†’ 32Ã—32)
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256, 512],
            latent_channels=4,
            sample_size=128,
        )

        # åˆ›å»ºä¸é¡¹ç›®ä¸€è‡´çš„UNet (sample_size=32)
        unet = UNet2DConditionModel(
            sample_size=32,
            in_channels=4,
            out_channels=4,
            cross_attention_dim=768,
        )

        scheduler = DDPMScheduler(num_train_timesteps=1000)

        # æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹
        with torch.no_grad():
            test_input = torch.randn(1, 3, 128, 128)
            test_conditions = torch.randn(1, 1, 768)

            # VAEç¼–ç  (128Ã—128 â†’ 32Ã—32)
            latents = vae.encode(test_input).latent_dist.sample()

            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, 1000, (1,))
            noisy_latents = scheduler.add_noise(latents, noise, timesteps)

            # UNeté¢„æµ‹
            pred = unet(noisy_latents, timesteps, encoder_hidden_states=test_conditions, return_dict=False)[0]

            # VAEè§£ç  (32Ã—32 â†’ 128Ã—128)
            reconstructed = vae.decode(latents).sample

        print("âœ… VAE+LDMå®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
        print(f"   è¾“å…¥: {test_input.shape}")
        print(f"   æ½œåœ¨: {latents.shape}")
        print(f"   é‡å»º: {reconstructed.shape}")
        print(f"   UNeté¢„æµ‹: {pred.shape}")
        print(f"   å‹ç¼©æ¯”: {test_input.shape[-1] // latents.shape[-1]}å€")
        test_results['vae'] = True

    except Exception as e:
        print(f"âŒ VAEåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        test_results['vae'] = False
    
    # æµ‹è¯•æ€»ç»“
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    passed = sum(test_results.values())
    total = len(test_results)

    for test_name, result in test_results.items():
        status = "âœ…" if result else "âŒ"
        print(f"   {status} {test_name}")

    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} é€šè¿‡")

    # å…³é”®æµ‹è¯•
    critical_tests = ['numpy', 'torch', 'diffusers', 'vae']
    critical_passed = all(test_results.get(test, False) for test in critical_tests)

    # GPUå»ºè®®
    if not test_results.get('gpu', False):
        print("\nğŸ’¡ GPUå»ºè®®:")
        print("   - æ£€æŸ¥Kaggle GPUè®¾ç½®")
        print("   - é‡å¯å†…æ ¸åé‡æ–°è¿è¡Œ")
        print("   - æˆ–ä½¿ç”¨CPUæ¨¡å¼è®­ç»ƒ")

    return critical_passed, test_results

def main():
    """ä¸»ä¿®å¤æµç¨‹"""
    print("ğŸš€ ç»ˆæKaggleç¯å¢ƒä¿®å¤å·¥å…·")
    print("å½»åº•æ¸…ç†å¹¶é‡å»ºæ‰€æœ‰ä¾èµ–")
    print("=" * 60)
    
    try:
        # é˜¶æ®µ1: æ ¸å¼¹çº§æ¸…ç†
        nuclear_cleanup()
        
        # é˜¶æ®µ2: å®‰è£…åŸºç¡€ç³»ç»Ÿ
        install_base_system()
        
        # é˜¶æ®µ3: å®‰è£…PyTorch
        if not install_pytorch_stack():
            print("âŒ PyTorchå®‰è£…å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        # é˜¶æ®µ4: å®‰è£…AIåŒ…
        install_ai_packages()
        
        # é˜¶æ®µ5: å®‰è£…å·¥å…·åŒ…
        install_utility_packages()
        
        # é˜¶æ®µ6: å…¨é¢æµ‹è¯•
        print("\n" + "=" * 50 + " æœ€ç»ˆæµ‹è¯• " + "=" * 50)
        
        success, test_results = comprehensive_test()

        if success:
            print("\nğŸ‰ ä¿®å¤æˆåŠŸï¼æ‰€æœ‰å…³é”®ç»„ä»¶æ­£å¸¸å·¥ä½œ")
            print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")

            # æ ¹æ®GPUçŠ¶æ€ç»™å‡ºå»ºè®®
            if test_results.get('gpu', False):
                print("   python train_kaggle.py --stage all")
                print("\nğŸ’¡ æç¤º:")
                print("   - ç¯å¢ƒå·²å®Œå…¨é‡å»º")
                print("   - GPUå¯ç”¨ï¼Œå¯ä»¥å…¨é€Ÿè®­ç»ƒ")
                print("   - æ‰€æœ‰ç‰ˆæœ¬å†²çªå·²è§£å†³")
            else:
                print("   python train_kaggle.py --stage all --device cpu")
                print("\nğŸ’¡ æç¤º:")
                print("   - ç¯å¢ƒå·²å®Œå…¨é‡å»º")
                print("   - ä½¿ç”¨CPUæ¨¡å¼è®­ç»ƒ")
                print("   - è®­ç»ƒæ—¶é—´ä¼šè¾ƒé•¿")

            return True
        else:
            print("\nâš ï¸  éƒ¨åˆ†ç»„ä»¶ä»æœ‰é—®é¢˜")
            print("\nğŸ”§ å»ºè®®:")

            # å…·ä½“å»ºè®®
            if not test_results.get('gpu', False):
                print("   1. æ£€æŸ¥Kaggle GPUè®¾ç½®")
                print("   2. é‡å¯å†…æ ¸å¹¶é‡æ–°è¿è¡Œ")
            if not test_results.get('torch', False):
                print("   3. PyTorchå®‰è£…é—®é¢˜")
            if not test_results.get('diffusers', False):
                print("   4. Diffusersç‰ˆæœ¬é—®é¢˜")

            print("   5. æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ")
            return False
            
    except Exception as e:
        print(f"\nğŸ’¥ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        print("\nğŸ”§ å»ºè®®:")
        print("   1. é‡å¯Kaggleå†…æ ¸")
        print("   2. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        return False

if __name__ == "__main__":
    main()
