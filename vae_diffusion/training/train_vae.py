#!/usr/bin/env python3
"""
å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ VAE è®­ç»ƒè„šæœ¬
åŸºäº Diffusers AutoencoderKL å®ç°
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from diffusers import AutoencoderKL
from accelerate import Accelerator
from tqdm import tqdm
import wandb
from pathlib import Path
import argparse
import json
from PIL import Image
import numpy as np

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from ..utils.data_loader import MicroDopplerDataset
from ..utils.metrics import calculate_psnr, calculate_ssim

class MicroDopplerVAELoss(nn.Module):
    """å¾®å¤šæ™®å‹’VAEæŸå¤±å‡½æ•°"""
    
    def __init__(self, kl_weight=1e-4, perceptual_weight=1.0, freq_weight=0.1):
        super().__init__()
        self.kl_weight = kl_weight
        self.perceptual_weight = perceptual_weight
        self.freq_weight = freq_weight
        
        # åŸºç¡€æŸå¤±
        self.mse_loss = nn.MSELoss()
        
        # æ„ŸçŸ¥æŸå¤± (å¯é€‰ï¼Œéœ€è¦å®‰è£… lpips)
        self.lpips_loss = None
        if self.perceptual_weight > 0:
            try:
                import lpips
                self.lpips_loss = lpips.LPIPS(net='vgg')
                print("âœ… LPIPSæ„ŸçŸ¥æŸå¤±å·²åŠ è½½")
            except ImportError:
                print("âš ï¸  LPIPS not available, disabling perceptual loss")
                self.perceptual_weight = 0.0
            except Exception as e:
                print(f"âš ï¸  LPIPSåŠ è½½å¤±è´¥: {e}, disabling perceptual loss")
                self.perceptual_weight = 0.0
                self.lpips_loss = None

    def to(self, device):
        """ç§»åŠ¨æŸå¤±å‡½æ•°åˆ°æŒ‡å®šè®¾å¤‡"""
        super().to(device)
        if self.lpips_loss is not None:
            self.lpips_loss = self.lpips_loss.to(device)
        return self
    
    def forward(self, reconstruction, target, posterior):
        """
        è®¡ç®—VAEæŸå¤±
        Args:
            reconstruction: é‡å»ºå›¾åƒ [B, C, H, W]
            target: ç›®æ ‡å›¾åƒ [B, C, H, W]
            posterior: VAEåéªŒåˆ†å¸ƒ
        """
        batch_size = target.shape[0]
        
        # 1. é‡å»ºæŸå¤±
        recon_loss = self.mse_loss(reconstruction, target)
        
        # 2. KLæ•£åº¦æŸå¤±
        kl_loss = posterior.kl().mean()
        
        # 3. æ„ŸçŸ¥æŸå¤±
        perceptual_loss = 0.0
        if self.lpips_loss is not None and self.perceptual_weight > 0:
            # å½’ä¸€åŒ–åˆ°[-1, 1]
            recon_norm = reconstruction * 2.0 - 1.0
            target_norm = target * 2.0 - 1.0
            perceptual_loss = self.lpips_loss(recon_norm, target_norm).mean()
        
        # 4. é¢‘åŸŸæŸå¤± (ä¿æŒæ—¶é¢‘ç‰¹æ€§)
        freq_loss = 0.0
        if self.freq_weight > 0:
            # FFTå˜æ¢
            recon_fft = torch.fft.fft2(reconstruction)
            target_fft = torch.fft.fft2(target)
            
            # å¹…åº¦è°±æŸå¤±
            recon_mag = torch.abs(recon_fft)
            target_mag = torch.abs(target_fft)
            freq_loss = self.mse_loss(recon_mag, target_mag)
        
        # æ€»æŸå¤±
        total_loss = (recon_loss + 
                     self.kl_weight * kl_loss + 
                     self.perceptual_weight * perceptual_loss + 
                     self.freq_weight * freq_loss)
        
        return {
            'total_loss': total_loss,
            'recon_loss': recon_loss,
            'kl_loss': kl_loss,
            'perceptual_loss': perceptual_loss,
            'freq_loss': freq_loss
        }

def train_vae(args):
    """VAEè®­ç»ƒä¸»å‡½æ•°"""

    # å†…å­˜ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True

    # æ£€æŸ¥GPUé…ç½®
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")

        for i in range(gpu_count):
            gpu_props = torch.cuda.get_device_properties(i)
            gpu_memory = gpu_props.total_memory / 1024**3
            print(f"   GPU {i}: {gpu_props.name} - {gpu_memory:.1f} GB")
            torch.cuda.empty_cache()

        # å¦‚æœæœ‰å¤šä¸ªGPUï¼Œç¡®ä¿ä½¿ç”¨æ‰€æœ‰GPU
        if gpu_count > 1:
            print(f"âœ… å°†ä½¿ç”¨æ‰€æœ‰ {gpu_count} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        else:
            print("âš ï¸  åªæ£€æµ‹åˆ°1ä¸ªGPUï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥GPUé…ç½®")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°GPU")

    # åˆå§‹åŒ–åŠ é€Ÿå™¨ (è‡ªåŠ¨æ£€æµ‹å¤šGPU)
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with="wandb" if args.use_wandb else None,
        project_dir=args.output_dir
    )

    # æ‰“å°å®é™…ä½¿ç”¨çš„è®¾å¤‡ä¿¡æ¯
    print(f"ğŸš€ è®­ç»ƒè®¾å¤‡: {accelerator.device}")
    print(f"ğŸ”¢ è¿›ç¨‹æ•°: {accelerator.num_processes}")
    print(f"ğŸ“Š æ˜¯å¦åˆ†å¸ƒå¼: {accelerator.distributed_type}")
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–wandb
    if args.use_wandb and accelerator.is_main_process:
        wandb.init(
            project="micro-doppler-vae",
            config=vars(args),
            name=f"vae-{args.experiment_name}"
        )
    
    # åˆ›å»ºVAEæ¨¡å‹ - CelebAæ ‡å‡†é…ç½®
    # è§£ææ¶æ„å‚æ•°
    down_blocks = args.down_block_types.split(',')
    up_blocks = args.up_block_types.split(',')
    channels = [int(c) for c in args.block_out_channels.split(',')]

    # è®¡ç®—å‹ç¼©æ¯”
    num_downsample = len(down_blocks)
    downsample_factor = 2 ** num_downsample
    latent_size = args.resolution // downsample_factor
    input_pixels = args.resolution * args.resolution * 3
    latent_pixels = latent_size * latent_size * args.latent_channels
    compression_ratio = input_pixels / latent_pixels

    print("ğŸ¨ ä½¿ç”¨å¯é…ç½®VAEæ¶æ„")
    print(f"   ğŸ“ è¾“å…¥: {args.resolution}Ã—{args.resolution}Ã—3")
    print(f"   ğŸ”½ ä¸‹é‡‡æ ·å±‚æ•°: {num_downsample}")
    print(f"   ğŸ¯ æ½œåœ¨ç©ºé—´: {latent_size}Ã—{latent_size}Ã—{args.latent_channels}")
    print(f"   ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
    print(f"   ğŸ§± æ¯å±‚å—æ•°: {args.layers_per_block}")
    print(f"   ğŸ“ˆ é€šé“æ•°: {channels}")

    # ç¡®å®šsample_sizeå‚æ•°
    sample_size = args.sample_size if args.sample_size is not None else args.resolution

    vae = AutoencoderKL(
        in_channels=3,
        out_channels=3,
        down_block_types=down_blocks,
        up_block_types=up_blocks,
        block_out_channels=channels,
        latent_channels=args.latent_channels,
        sample_size=sample_size,
        layers_per_block=args.layers_per_block,
        act_fn="silu",
        norm_num_groups=32,
        scaling_factor=0.18215,
    )
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = MicroDopplerDataset(
        data_dir=args.data_dir,
        resolution=args.resolution,
        augment=args.use_augmentation
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        vae.parameters(),
        lr=args.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay,
        eps=1e-8
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨æ›´ç¨³å®šçš„ç­–ç•¥
    total_steps = len(dataloader) * args.num_epochs
    warmup_steps = total_steps // 10  # 10% warmup

    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            progress = (step - warmup_steps) / (total_steps - warmup_steps)
            return 0.5 * (1 + np.cos(np.pi * progress))

    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_fn = MicroDopplerVAELoss(
        kl_weight=args.kl_weight,
        perceptual_weight=args.perceptual_weight,
        freq_weight=args.freq_weight
    )
    
    # ä½¿ç”¨acceleratorå‡†å¤‡æ¨¡å‹å’Œæ•°æ®
    vae, optimizer, dataloader, lr_scheduler = accelerator.prepare(
        vae, optimizer, dataloader, lr_scheduler
    )

    # ç§»åŠ¨æŸå¤±å‡½æ•°åˆ°æ­£ç¡®çš„è®¾å¤‡
    loss_fn = loss_fn.to(accelerator.device)
    print(f"âœ… æŸå¤±å‡½æ•°å·²ç§»åŠ¨åˆ°è®¾å¤‡: {accelerator.device}")
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    
    for epoch in range(args.num_epochs):
        vae.train()
        epoch_loss = 0.0
        
        progress_bar = tqdm(
            dataloader,
            desc=f"Epoch {epoch+1}/{args.num_epochs}",
            disable=not accelerator.is_local_main_process,
            dynamic_ncols=True,  # åŠ¨æ€è°ƒæ•´è¿›åº¦æ¡å®½åº¦
            leave=True,          # ä¿ç•™è¿›åº¦æ¡
            position=0           # è¿›åº¦æ¡ä½ç½®
        )
        
        for step, batch in enumerate(progress_bar):
            with accelerator.accumulate(vae):
                # è·å–å›¾åƒæ•°æ®
                images = batch['image']  # [B, C, H, W]
                
                # VAEå‰å‘ä¼ æ’­ (å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒ)
                # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œéœ€è¦é€šè¿‡.moduleè®¿é—®åŸå§‹æ¨¡å‹
                vae_model = vae.module if hasattr(vae, 'module') else vae

                posterior = vae_model.encode(images).latent_dist
                latents = posterior.sample()
                reconstruction = vae_model.decode(latents).sample
                
                # è®¡ç®—æŸå¤±
                loss_dict = loss_fn(reconstruction, images, posterior)
                loss = loss_dict['total_loss']
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(vae.parameters(), args.max_grad_norm)
                
                optimizer.step()
                optimizer.zero_grad()

                # å®šæœŸæ¸…ç†GPUç¼“å­˜
                if global_step % 50 == 0 and torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # æ›´æ–°è¿›åº¦æ¡
            if accelerator.sync_gradients:
                global_step += 1
                epoch_loss += loss.item()
                
                # è®°å½•æ—¥å¿—
                if global_step % args.log_interval == 0:
                    avg_loss = epoch_loss / (step + 1)
                    
                    # å®‰å…¨è·å–æŸå¤±å€¼ï¼ˆå¤„ç†tensorå’Œfloatï¼‰
                    def safe_item(value):
                        return value.item() if hasattr(value, 'item') else value

                    logs = {
                        "epoch": epoch,
                        "step": global_step,
                        "lr": lr_scheduler.get_last_lr()[0],
                        "loss/total": loss_dict['total_loss'].item(),
                        "loss/recon": loss_dict['recon_loss'].item(),
                        "loss/kl": loss_dict['kl_loss'].item(),
                        "loss/perceptual": safe_item(loss_dict['perceptual_loss']),
                        "loss/freq": safe_item(loss_dict['freq_loss']),
                    }
                    
                    # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                    progress_bar.set_postfix({
                        'loss': f"{avg_loss:.4f}",
                        'recon': f"{loss_dict['recon_loss'].item():.4f}",
                        'kl': f"{loss_dict['kl_loss'].item():.6f}",
                        'freq': f"{safe_item(loss_dict['freq_loss']):.4f}",
                        'lr': f"{lr_scheduler.get_last_lr()[0]:.2e}"
                    })
                    
                    if args.use_wandb and accelerator.is_main_process:
                        wandb.log(logs, step=global_step)
                
                # ä¿å­˜æ ·æœ¬å›¾åƒ
                if global_step % args.sample_interval == 0 and accelerator.is_main_process:
                    save_sample_images(
                        images[:4], reconstruction[:4], 
                        args.output_dir, global_step
                    )
        
        # æ›´æ–°å­¦ä¹ ç‡
        lr_scheduler.step()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_interval == 0 and accelerator.is_main_process:
            save_checkpoint(vae, optimizer, epoch, global_step, args.output_dir)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if accelerator.is_main_process:
        save_final_model(vae, args.output_dir)
        print(f"Training completed! Model saved to {args.output_dir}")

def save_sample_images(original, reconstruction, output_dir, step):
    """ä¿å­˜æ ·æœ¬å›¾åƒ (åªä¿ç•™æœ€è¿‘10ä¸ª)"""
    import torchvision.utils as vutils

    # æ‹¼æ¥åŸå›¾å’Œé‡å»ºå›¾
    comparison = torch.cat([original, reconstruction], dim=0)

    # ä¿å­˜å›¾åƒ
    sample_dir = Path(output_dir) / "samples"
    sample_dir.mkdir(exist_ok=True)

    # æ–°æ ·æœ¬æ–‡ä»¶å
    new_sample_path = sample_dir / f"step_{step:06d}.png"

    # ä¿å­˜æ–°æ ·æœ¬
    vutils.save_image(
        comparison,
        new_sample_path,
        nrow=4,
        normalize=True,
        value_range=(0, 1)
    )

    # åªä¿ç•™æœ€è¿‘10ä¸ªæ ·æœ¬æ–‡ä»¶ (èŠ‚çœç©ºé—´)
    try:
        sample_files = sorted(sample_dir.glob("step_*.png"))
        if len(sample_files) > 10:
            # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶
            for old_file in sample_files[:-10]:
                old_file.unlink()
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ ·æœ¬: {old_file.name}")
    except Exception as e:
        print(f"âš ï¸  æ¸…ç†æ—§æ ·æœ¬æ—¶å‡ºé”™: {e}")

def save_checkpoint(model, optimizer, epoch, step, output_dir):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ (åªä¿ç•™æœ€æ–°çš„1ä¸ª)"""
    checkpoint_dir = Path(output_dir) / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)

    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œéœ€è¦é€šè¿‡.moduleè®¿é—®åŸå§‹æ¨¡å‹
    model_to_save = model.module if hasattr(model, 'module') else model

    checkpoint = {
        'model_state_dict': model_to_save.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'step': step,
    }

    # æ–°æ£€æŸ¥ç‚¹æ–‡ä»¶å
    new_checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pt"

    # åˆ é™¤æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ (ä¿ç•™æœ€æ–°1ä¸ª)
    try:
        for old_checkpoint in checkpoint_dir.glob("checkpoint_epoch_*.pt"):
            if old_checkpoint != new_checkpoint_path:
                old_checkpoint.unlink()
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_checkpoint.name}")
    except Exception as e:
        print(f"âš ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}")

    # ä¿å­˜æ–°æ£€æŸ¥ç‚¹
    torch.save(checkpoint, new_checkpoint_path)
    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {new_checkpoint_path.name}")

def save_final_model(model, output_dir):
    """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œéœ€è¦é€šè¿‡.moduleè®¿é—®åŸå§‹æ¨¡å‹
    model_to_save = model.module if hasattr(model, 'module') else model
    model_to_save.save_pretrained(Path(output_dir) / "final_model")

def main():
    parser = argparse.ArgumentParser(description="Train VAE for Micro-Doppler Images")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®é›†ç›®å½•")
    parser.add_argument("--resolution", type=int, default=256, help="å›¾åƒåˆ†è¾¨ç‡")
    parser.add_argument("--use_augmentation", action="store_true", help="ä½¿ç”¨æ•°æ®å¢å¹¿")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    
    # æŸå¤±æƒé‡
    parser.add_argument("--kl_weight", type=float, default=1e-6, help="KLæ•£åº¦æƒé‡")
    parser.add_argument("--perceptual_weight", type=float, default=0.1, help="æ„ŸçŸ¥æŸå¤±æƒé‡")
    parser.add_argument("--freq_weight", type=float, default=0.05, help="é¢‘åŸŸæŸå¤±æƒé‡")

    # VAEæ¶æ„å‚æ•°
    parser.add_argument("--down_block_types", type=str, default="DownEncoderBlock2D,DownEncoderBlock2D,DownEncoderBlock2D",
                       help="ä¸‹é‡‡æ ·å—ç±»å‹ (é€—å·åˆ†éš”)")
    parser.add_argument("--up_block_types", type=str, default="UpDecoderBlock2D,UpDecoderBlock2D,UpDecoderBlock2D",
                       help="ä¸Šé‡‡æ ·å—ç±»å‹ (é€—å·åˆ†éš”)")
    parser.add_argument("--block_out_channels", type=str, default="64,128,256",
                       help="è¾“å‡ºé€šé“æ•° (é€—å·åˆ†éš”)")
    parser.add_argument("--layers_per_block", type=int, default=1, help="æ¯å±‚å—æ•°")
    parser.add_argument("--latent_channels", type=int, default=4, help="æ½œåœ¨ç©ºé—´é€šé“æ•°")
    parser.add_argument("--sample_size", type=int, default=None, help="VAE sample_sizeå‚æ•° (å½±å“ä¸‹é‡‡æ ·è¡Œä¸º)")

    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--mixed_precision", type=str, default="fp16", help="æ··åˆç²¾åº¦")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    
    # æ—¥å¿—å’Œä¿å­˜
    parser.add_argument("--output_dir", type=str, default="./outputs/vae", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--experiment_name", type=str, default="baseline", help="å®éªŒåç§°")
    parser.add_argument("--log_interval", type=int, default=10, help="æ—¥å¿—è®°å½•é—´éš”")
    parser.add_argument("--sample_interval", type=int, default=500, help="æ ·æœ¬ä¿å­˜é—´éš”")
    parser.add_argument("--save_interval", type=int, default=10, help="æ¨¡å‹ä¿å­˜é—´éš”")
    parser.add_argument("--use_wandb", action="store_true", help="ä½¿ç”¨wandbè®°å½•")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # å¼€å§‹è®­ç»ƒ
    train_vae(args)

if __name__ == "__main__":
    main()
