#!/usr/bin/env python3
"""
VAEè®­ç»ƒé…ç½®æµ‹è¯•è„šæœ¬
éªŒè¯VAEæ¶æ„é…ç½®æ˜¯å¦æ­£ç¡® (128Ã—128 â†’ 32Ã—32)
"""

import torch
import warnings
warnings.filterwarnings("ignore")

def test_vae_config():
    """æµ‹è¯•VAEé…ç½®"""
    print("ğŸ” æµ‹è¯•VAEè®­ç»ƒé…ç½® (128Ã—128 â†’ 32Ã—32)")
    print("=" * 60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print(f"\nğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½å™¨:")
    try:
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).parent))
        
        from ..utils.data_loader import MicroDopplerDataset
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›† (ä½¿ç”¨128Ã—128åˆ†è¾¨ç‡)
        dataset = MicroDopplerDataset(
            data_dir="data",  # å‡è®¾æ•°æ®ç›®å½•
            resolution=128,   # å…³é”®: 128Ã—128åˆ†è¾¨ç‡
            augment=False,
            split="test"
        )
        
        print(f"   âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        print(f"   ğŸ“ åˆ†è¾¨ç‡: 128Ã—128")
        print(f"   ğŸ”„ æ•°æ®å¢å¹¿: å…³é—­")
        
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•VAEæ¶æ„
    print(f"\nğŸ—ï¸  æµ‹è¯•VAEæ¶æ„ (128Ã—128 â†’ 32Ã—32):")
    try:
        from diffusers import AutoencoderKL
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„å®é™…é…ç½®
        vae = AutoencoderKL(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],  # 3å±‚
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],        # 3å±‚
            block_out_channels=[128, 256, 512],                                                   # 3å±‚é€šé“æ•°
            latent_channels=4,
            sample_size=128,                                                 # è®¾ç½®ä¸º128åŒ¹é…è¾“å…¥å°ºå¯¸
            layers_per_block=1,                                              # æ ‡å‡†é…ç½®
            act_fn="silu",
            norm_num_groups=32,
            scaling_factor=0.18215,
        ).to(device)
        
        total_params = sum(p.numel() for p in vae.parameters())
        print(f"   âœ… VAEåˆ›å»ºæˆåŠŸ - å‚æ•°é‡: {total_params:,}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_input = torch.randn(1, 3, 128, 128).to(device)
        
        with torch.no_grad():
            # ç¼–ç 
            latent_dist = vae.encode(test_input).latent_dist
            latent = latent_dist.sample()
            print(f"   âœ… æ½œåœ¨ç©ºé—´å½¢çŠ¶: {latent.shape}")
            
            # è§£ç 
            reconstructed = vae.decode(latent).sample
            print(f"   âœ… é‡å»ºå½¢çŠ¶: {reconstructed.shape}")
            
            # éªŒè¯å½¢çŠ¶
            expected_latent_shape = (1, 4, 32, 32)
            expected_output_shape = (1, 3, 128, 128)
            
            if latent.shape == expected_latent_shape:
                print(f"   âœ… æ½œåœ¨ç©ºé—´å½¢çŠ¶æ­£ç¡®: {latent.shape}")
            else:
                print(f"   âŒ æ½œåœ¨ç©ºé—´å½¢çŠ¶é”™è¯¯: {latent.shape}, æœŸæœ›: {expected_latent_shape}")
                return False
                
            if reconstructed.shape == expected_output_shape:
                print(f"   âœ… é‡å»ºå½¢çŠ¶æ­£ç¡®: {reconstructed.shape}")
            else:
                print(f"   âŒ é‡å»ºå½¢çŠ¶é”™è¯¯: {reconstructed.shape}, æœŸæœ›: {expected_output_shape}")
                return False
        
        # è®¡ç®—å‹ç¼©æ¯”
        compression_ratio = (128 * 128 * 3) / (32 * 32 * 4)
        print(f"   ğŸ“Š å‹ç¼©æ¯”: {compression_ratio:.1f}:1")
        print(f"   ğŸ”½ ä¸‹é‡‡æ ·å› å­: {128 // 32}å€")
        
    except Exception as e:
        print(f"   âŒ VAEæ¶æ„æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•è®­ç»ƒå‚æ•°å…¼å®¹æ€§
    print(f"\nâš™ï¸  æµ‹è¯•è®­ç»ƒå‚æ•°:")
    
    training_config = {
        "resolution": 128,
        "batch_size": 8,
        "learning_rate": 1e-4,
        "num_epochs": 80,
        "kl_weight": 1e-6,
        "perceptual_weight": 0.5,
        "down_blocks": 3,
        "latent_channels": 4,
        "expected_latent_size": 32
    }
    
    for key, value in training_config.items():
        print(f"   âœ… {key}: {value}")
    
    print(f"\nğŸ¯ é…ç½®éªŒè¯æ€»ç»“:")
    print(f"   âœ… VAEæ¶æ„: 3å±‚ä¸‹é‡‡æ · (128â†’64â†’32)")
    print(f"   âœ… æ½œåœ¨ç©ºé—´: 32Ã—32Ã—4")
    print(f"   âœ… å‹ç¼©æ¯”: 12:1")
    print(f"   âœ… ä¸æ‰©æ•£æ¨¡å‹å…¼å®¹")
    
    return True

def show_training_command():
    """æ˜¾ç¤ºæ­£ç¡®çš„VAEè®­ç»ƒå‘½ä»¤"""
    print(f"\nğŸš€ æ­£ç¡®çš„VAEè®­ç»ƒå‘½ä»¤:")
    print(f"python training/train_vae.py \\")
    print(f"    --data_dir \"/kaggle/input/dataset\" \\")
    print(f"    --resolution 128 \\")
    print(f"    --batch_size 8 \\")
    print(f"    --num_epochs 80 \\")
    print(f"    --down_block_types \"DownEncoderBlock2D,DownEncoderBlock2D,DownEncoderBlock2D\" \\")
    print(f"    --up_block_types \"UpDecoderBlock2D,UpDecoderBlock2D,UpDecoderBlock2D\" \\")
    print(f"    --block_out_channels \"128,256,512\" \\")
    print(f"    --sample_size 128 \\")
    print(f"    --output_dir \"/kaggle/working/outputs/vae\"")

def main():
    """ä¸»å‡½æ•°"""
    success = test_vae_config()
    
    if success:
        show_training_command()
        print(f"\nâœ… VAEé…ç½®æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
    else:
        print(f"\nâŒ VAEé…ç½®æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")

if __name__ == "__main__":
    main()
