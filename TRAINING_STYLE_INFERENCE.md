# åŸºäºè®­ç»ƒæ—¶é€»è¾‘çš„æ¨ç†è„šæœ¬

## ğŸ¯ é—®é¢˜è§£å†³æ€è·¯

ä½ çš„è§‚å¯Ÿéå¸¸æ­£ç¡®ï¼è®­ç»ƒæ—¶èƒ½æ­£å¸¸ç”Ÿæˆæ ·æœ¬å›¾åƒï¼Œè¯´æ˜è®­ç»ƒæ—¶çš„æ¨ç†é€»è¾‘æ˜¯å®Œå…¨æ­£ç¡®çš„ã€‚é—®é¢˜å‡ºåœ¨æˆ‘ä»¬çš„æ¨ç†è„šæœ¬ä¸è®­ç»ƒæ—¶çš„é€»è¾‘ä¸ä¸€è‡´ã€‚

## ğŸ” å…³é”®å·®å¼‚åˆ†æ

### è®­ç»ƒæ—¶çš„æ¨ç†é€»è¾‘ï¼ˆæ­£å¸¸å·¥ä½œï¼‰ï¼š
```python
# æ¥è‡ª train_diffusion.py çš„ generate_samples å‡½æ•°
def generate_samples(unet, condition_encoder, vae, noise_scheduler, user_ids, output_dir, step, device, data_module=None):
    # 1. ç›´æ¥ä½¿ç”¨è®­ç»ƒä¸­çš„æ¨¡å‹å¯¹è±¡
    # 2. åˆ›å»ºDDIMè°ƒåº¦å™¨: DDIMScheduler.from_config(noise_scheduler.config)
    # 3. ç®€å•çš„ç”¨æˆ·IDæ˜ å°„: user_idx = user_id - 1
    # 4. ç›´æ¥çš„æ¡ä»¶ç¼–ç : encoder_hidden_states = condition_encoder(user_idx_tensor)
    # 5. æ ‡å‡†çš„å»å™ªå¾ªç¯
```

### åŸæ¨ç†è„šæœ¬çš„é—®é¢˜ï¼š
1. **å¤æ‚çš„æ¨¡å‹åŠ è½½é€»è¾‘**ï¼šå°è¯•ä»æ–‡ä»¶æ¨æ–­é…ç½®
2. **ç»´åº¦ä¸åŒ¹é…å¤„ç†**ï¼šæ·»åŠ äº†æŠ•å½±å±‚ç­‰å¤æ‚é€»è¾‘
3. **è¿‡åº¦å·¥ç¨‹åŒ–**ï¼šå¼•å…¥äº†å¤ªå¤š"æ™ºèƒ½"å¤„ç†

## ğŸš€ æ–°çš„æ¨ç†è„šæœ¬ç‰¹ç‚¹

### å®Œå…¨å¤åˆ¶è®­ç»ƒæ—¶é€»è¾‘ï¼š
1. **ç›¸åŒçš„æ¨¡å‹åˆ›å»ºæ–¹å¼**
2. **ç›¸åŒçš„è°ƒåº¦å™¨åˆ›å»ºæ–¹å¼**
3. **ç›¸åŒçš„æ¡ä»¶ç¼–ç é€»è¾‘**
4. **ç›¸åŒçš„å»å™ªè¿‡ç¨‹**
5. **ç›¸åŒçš„VAEè§£ç è¿‡ç¨‹**

### å…³é”®ä»£ç å¯¹æ¯”ï¼š

**è®­ç»ƒæ—¶ï¼ˆæ­£å¸¸å·¥ä½œï¼‰ï¼š**
```python
# åˆ›å»ºæ¡ä»¶ç¼–ç å™¨
condition_encoder = UserConditionEncoder(
    num_users=num_users,
    embed_dim=unet.config.cross_attention_dim
)

# ç”¨æˆ·æ¡ä»¶ç¼–ç 
user_idx = user_id - 1 if user_id > 0 else user_id
user_idx_tensor = torch.tensor([user_idx], device=device)
encoder_hidden_states = condition_encoder(user_idx_tensor)
encoder_hidden_states = encoder_hidden_states.unsqueeze(1)

# å»å™ªè¿‡ç¨‹
for t in ddim_scheduler.timesteps:
    timestep = t.unsqueeze(0).to(device)
    noise_pred = unet(latents, timestep, encoder_hidden_states=encoder_hidden_states, return_dict=False)[0]
    latents = ddim_scheduler.step(noise_pred, t, latents, return_dict=False)[0]
```

**æ–°æ¨ç†è„šæœ¬ï¼ˆå¤åˆ¶è®­ç»ƒé€»è¾‘ï¼‰ï¼š**
```python
# å®Œå…¨ç›¸åŒçš„é€»è¾‘
condition_encoder = UserConditionEncoder(
    num_users=num_users,
    embed_dim=unet.config.cross_attention_dim
)

user_idx = user_id - 1 if user_id > 0 else user_id
user_idx_tensor = torch.tensor([user_idx], device=device)
encoder_hidden_states = condition_encoder(user_idx_tensor)
encoder_hidden_states = encoder_hidden_states.unsqueeze(1)

for t in ddim_scheduler.timesteps:
    timestep = t.unsqueeze(0).to(device)
    noise_pred = unet(latents, timestep, encoder_hidden_states=encoder_hidden_states, return_dict=False)[0]
    latents = ddim_scheduler.step(noise_pred, t, latents, return_dict=False)[0]
```

## ğŸ“‹ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬å‘½ä»¤ï¼š
```bash
python inference/generate_training_style.py \
    --vae_path "/kaggle/input/final-model" \
    --unet_path "/kaggle/input/diffusion-final-model" \
    --condition_encoder_path "/kaggle/input/diffusion-final-model/condition_encoder.pt" \
    --num_users 31 \
    --user_ids 1 5 10 15 \
    --num_images_per_user 5 \
    --num_inference_steps 20 \
    --output_dir "/kaggle/working/generated_images" \
    --device auto \
    --seed 42
```

### å‚æ•°è¯´æ˜ï¼š
- `--vae_path`: VAEæ¨¡å‹è·¯å¾„
- `--unet_path`: UNetæ¨¡å‹è·¯å¾„
- `--condition_encoder_path`: æ¡ä»¶ç¼–ç å™¨è·¯å¾„
- `--num_users`: ç”¨æˆ·æ€»æ•°ï¼ˆ31ï¼‰
- `--user_ids`: è¦ç”Ÿæˆçš„ç”¨æˆ·IDåˆ—è¡¨
- `--num_images_per_user`: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„å›¾åƒæ•°é‡
- `--num_inference_steps`: æ¨ç†æ­¥æ•°ï¼ˆå»ºè®®20-50ï¼‰
- `--output_dir`: è¾“å‡ºç›®å½•
- `--device`: è®¾å¤‡ï¼ˆauto/cuda/cpuï¼‰
- `--seed`: éšæœºç§å­

## âœ… é¢„æœŸæ•ˆæœ

### åº”è¯¥èƒ½æ­£å¸¸å·¥ä½œï¼Œå› ä¸ºï¼š
1. **é€»è¾‘å®Œå…¨ç›¸åŒ**ï¼šä¸è®­ç»ƒæ—¶çš„generate_sampleså‡½æ•°é€»è¾‘ä¸€è‡´
2. **æ— å¤æ‚å¤„ç†**ï¼šæ²¡æœ‰ç»´åº¦æ£€æµ‹ã€æŠ•å½±å±‚ç­‰å¤æ‚é€»è¾‘
3. **ç›´æ¥ç®€å•**ï¼šç›´æ¥ä½¿ç”¨UNetçš„cross_attention_dimåˆ›å»ºæ¡ä»¶ç¼–ç å™¨
4. **æƒé‡å…¼å®¹**ï¼šå¦‚æœæƒé‡ä¸å…¼å®¹ï¼Œä¼šä½¿ç”¨éšæœºæƒé‡ä½†è‡³å°‘èƒ½è¿è¡Œ

### è¾“å‡ºç»“æ„ï¼š
```
/kaggle/working/generated_images/
â”œâ”€â”€ user_01/
â”‚   â”œâ”€â”€ generated_000.png
â”‚   â”œâ”€â”€ generated_001.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ user_05/
â”œâ”€â”€ user_10/
â””â”€â”€ user_15/
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¦‚æœä»ç„¶å‡ºé”™ï¼š
1. **æ£€æŸ¥æ¨¡å‹æ–‡ä»¶**ï¼šç¡®ä¿æ‰€æœ‰è·¯å¾„æ­£ç¡®
2. **æ£€æŸ¥æƒé‡å…¼å®¹æ€§**ï¼šæ¡ä»¶ç¼–ç å™¨æƒé‡å¯èƒ½ä¸å…¼å®¹ï¼Œä½†ä¼šä½¿ç”¨éšæœºæƒé‡
3. **å‡å°‘å†…å­˜ä½¿ç”¨**ï¼šå‡å°‘num_images_per_useræˆ–num_inference_steps

### ä¸åŸè„šæœ¬çš„åŒºåˆ«ï¼š
- **æ›´ç®€å•**ï¼šæ²¡æœ‰å¤æ‚çš„ç»´åº¦æ£€æµ‹å’ŒæŠ•å½±å±‚
- **æ›´ç›´æ¥**ï¼šç›´æ¥å¤åˆ¶è®­ç»ƒæ—¶çš„é€»è¾‘
- **æ›´å¯é **ï¼šå¦‚æœè®­ç»ƒæ—¶èƒ½å·¥ä½œï¼Œè¿™ä¸ªä¹Ÿåº”è¯¥èƒ½å·¥ä½œ

## ğŸ’¡ ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ¡ˆåº”è¯¥æœ‰æ•ˆ

1. **è®­ç»ƒæ—¶éªŒè¯**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„generate_sampleså‡½æ•°å·²ç»éªŒè¯äº†è¿™ä¸ªé€»è¾‘
2. **é…ç½®ä¸€è‡´**ï¼šä½¿ç”¨ç›¸åŒçš„æ¨¡å‹åˆ›å»ºå’Œé…ç½®é€»è¾‘
3. **ç®€å•å¯é **ï¼šé¿å…äº†å¤æ‚çš„"æ™ºèƒ½"å¤„ç†ï¼Œå‡å°‘å‡ºé”™å¯èƒ½
4. **æƒé‡å®¹é”™**ï¼šå³ä½¿æƒé‡ä¸å…¼å®¹ï¼Œä¹Ÿä¼šé™çº§åˆ°éšæœºæƒé‡è€Œä¸æ˜¯å´©æºƒ

è¿™ä¸ªæ–°è„šæœ¬åº”è¯¥èƒ½å¤Ÿè§£å†³ç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒå®Œå…¨å¤åˆ¶äº†è®­ç»ƒæ—¶å·²ç»éªŒè¯å¯è¡Œçš„é€»è¾‘ã€‚
