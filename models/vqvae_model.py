#!/usr/bin/env python3
"""
é˜²åç¼©VQ-VAEæ¨¡å‹å®ç°
åŸºäºdiffusers VQModelï¼Œå¢åŠ EMAæ›´æ–°ã€ç æœ¬ç›‘æ§ç­‰é˜²åç¼©æœºåˆ¶
é’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple, Optional
import matplotlib.pyplot as plt

# å¯¼å…¥diffusers - ç»Ÿä¸€ç¯å¢ƒä¿è¯å¯ç”¨
from diffusers.models.autoencoders.vq_model import VQModel
from diffusers.models.autoencoders.vq_model import VectorQuantizer

class EMAVectorQuantizer(nn.Module):
    """
    å¸¦EMAæ›´æ–°çš„å‘é‡é‡åŒ–å™¨ï¼Œé˜²æ­¢ç æœ¬åç¼©
    """
    
    def __init__(
        self,
        n_embed: int = 1024,
        embed_dim: int = 256,
        beta: float = 0.25,
        decay: float = 0.99,
        eps: float = 1e-5,
        restart_threshold: float = 1.0,
    ):
        super().__init__()
        
        self.n_embed = n_embed
        self.embed_dim = embed_dim
        self.beta = beta
        self.decay = decay
        self.eps = eps
        self.restart_threshold = restart_threshold
        
        # ç æœ¬åµŒå…¥
        self.embedding = nn.Embedding(n_embed, embed_dim)
        self.embedding.weight.data.uniform_(-1/n_embed, 1/n_embed)
        
        # EMAå‚æ•° (ä¸å‚ä¸æ¢¯åº¦æ›´æ–°)
        self.register_buffer('ema_count', torch.zeros(n_embed))
        self.register_buffer('ema_weight', self.embedding.weight.data.clone())
        
        # ç›‘æ§å‚æ•°
        self.register_buffer('usage_count', torch.zeros(n_embed))
        self.register_buffer('total_updates', torch.tensor(0))
    
    def forward(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        Args:
            inputs: [B, C, H, W] è¾“å…¥ç‰¹å¾
        Returns:
            quantized: é‡åŒ–åçš„ç‰¹å¾
            loss: VQæŸå¤±
            encoding_indices: ç¼–ç ç´¢å¼•
        """
        # å±•å¹³è¾“å…¥ [B, C, H, W] -> [B*H*W, C]
        input_shape = inputs.shape
        flat_input = inputs.view(-1, self.embed_dim)
        
        # è®¡ç®—è·ç¦»
        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) 
                    + torch.sum(self.embedding.weight**2, dim=1)
                    - 2 * torch.matmul(flat_input, self.embedding.weight.t()))
        
        # æ‰¾åˆ°æœ€è¿‘çš„ç æœ¬å‘é‡
        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self.n_embed, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)
        
        # é‡åŒ–
        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)
        
        # æ›´æ–°EMA (ä»…åœ¨è®­ç»ƒæ—¶)
        if self.training:
            self._update_ema(encodings, flat_input)
            self._update_usage_stats(encoding_indices)
        
        # è®¡ç®—æŸå¤±
        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        q_latent_loss = F.mse_loss(quantized, inputs.detach())
        loss = q_latent_loss + self.beta * e_latent_loss
        
        # Straight-through estimator
        quantized = inputs + (quantized - inputs).detach()
        
        return quantized, loss, encoding_indices.view(input_shape[0], -1)
    
    def _update_ema(self, encodings: torch.Tensor, flat_input: torch.Tensor):
        """æ›´æ–°EMAå‚æ•°"""
        with torch.no_grad():
            # æ›´æ–°è®¡æ•°
            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, 0)
            
            # æ›´æ–°æƒé‡
            n = torch.sum(encodings, 0, keepdim=True)
            self.ema_weight = (self.decay * self.ema_weight + 
                              (1 - self.decay) * torch.matmul(encodings.t(), flat_input))
            
            # å½’ä¸€åŒ–å¹¶æ›´æ–°åµŒå…¥
            normalized_weight = self.ema_weight / (self.ema_count.unsqueeze(1) + self.eps)
            self.embedding.weight.data.copy_(normalized_weight)
            
            # é‡ç½®æœªä½¿ç”¨çš„ç æœ¬å‘é‡
            self._restart_unused_codes()
    
    def _update_usage_stats(self, encoding_indices: torch.Tensor):
        """æ›´æ–°ä½¿ç”¨ç»Ÿè®¡"""
        with torch.no_grad():
            unique_indices = torch.unique(encoding_indices)
            self.usage_count[unique_indices] += 1
            self.total_updates += 1
    
    def _restart_unused_codes(self):
        """é‡ç½®æœªä½¿ç”¨çš„ç æœ¬å‘é‡"""
        if self.total_updates > 0 and self.total_updates % 1000 == 0:  # æ¯1000æ¬¡æ›´æ–°æ£€æŸ¥ä¸€æ¬¡
            with torch.no_grad():
                # æ‰¾åˆ°ä½¿ç”¨é¢‘ç‡ä½çš„ç æœ¬å‘é‡
                usage_freq = self.ema_count / (self.total_updates + self.eps)
                unused_mask = usage_freq < self.restart_threshold / self.n_embed
                
                if unused_mask.sum() > 0:
                    print(f"é‡ç½® {unused_mask.sum()} ä¸ªæœªä½¿ç”¨çš„ç æœ¬å‘é‡")
                    
                    # ç”¨éšæœºå‘é‡é‡ç½®
                    n_restart = unused_mask.sum()
                    self.embedding.weight.data[unused_mask] = torch.randn(
                        n_restart, self.embed_dim, device=self.embedding.weight.device
                    ) * 0.1
                    
                    # é‡ç½®EMAå‚æ•°
                    self.ema_count[unused_mask] = 0
                    self.ema_weight[unused_mask] = self.embedding.weight.data[unused_mask].clone()
    
    def get_codebook_usage(self) -> Dict[str, float]:
        """è·å–ç æœ¬ä½¿ç”¨ç»Ÿè®¡"""
        with torch.no_grad():
            active_codes = (self.usage_count > 0).sum().item()
            usage_entropy = self._compute_entropy(self.usage_count)
            
            return {
                'active_codes': active_codes,
                'total_codes': self.n_embed,
                'usage_rate': active_codes / self.n_embed,
                'usage_entropy': usage_entropy,
                'max_usage': self.usage_count.max().item(),
                'min_usage': self.usage_count[self.usage_count > 0].min().item() if active_codes > 0 else 0,
            }
    
    def _compute_entropy(self, counts: torch.Tensor) -> float:
        """è®¡ç®—ä½¿ç”¨åˆ†å¸ƒçš„ç†µ"""
        counts = counts.float()
        total = counts.sum()
        if total == 0:
            return 0.0
        
        probs = counts / total
        probs = probs[probs > 0]  # åªè€ƒè™‘éé›¶æ¦‚ç‡
        entropy = -(probs * torch.log(probs)).sum().item()
        return entropy
    
    def plot_usage_distribution(self, save_path: Optional[str] = None):
        """å¯è§†åŒ–ç æœ¬ä½¿ç”¨åˆ†å¸ƒ"""
        usage = self.usage_count.cpu().numpy()
        
        plt.figure(figsize=(12, 4))
        
        # ä½¿ç”¨åˆ†å¸ƒç›´æ–¹å›¾
        plt.subplot(1, 2, 1)
        plt.hist(usage[usage > 0], bins=50, alpha=0.7)
        plt.xlabel('Usage Count')
        plt.ylabel('Number of Codes')
        plt.title('Codebook Usage Distribution')
        
        # ä½¿ç”¨é¢‘ç‡æ’åº
        plt.subplot(1, 2, 2)
        sorted_usage = np.sort(usage)[::-1]
        plt.plot(sorted_usage)
        plt.xlabel('Code Index (sorted)')
        plt.ylabel('Usage Count')
        plt.title('Codebook Usage (Sorted)')
        plt.yscale('log')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.show()


class MicroDopplerVQVAE(VQModel):
    """
    é’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ä¼˜åŒ–çš„VQ-VAEæ¨¡å‹
    åŸºäºdiffusers VQModelï¼Œæ›¿æ¢é‡åŒ–å™¨ä¸ºEMAç‰ˆæœ¬
    """
    
    def __init__(
        self,
        in_channels: int = 3,
        out_channels: int = 3,
        down_block_types: Tuple[str] = ("DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"),
        up_block_types: Tuple[str] = ("UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"),
        block_out_channels: Tuple[int] = (128, 256, 512),
        layers_per_block: int = 2,
        act_fn: str = "silu",
        latent_channels: int = 256,
        sample_size: int = 128,
        num_vq_embeddings: int = 1024,
        norm_num_groups: int = 32,
        vq_embed_dim: Optional[int] = None,
        scaling_factor: float = 0.18215,
        # VQ-VAEç‰¹å®šå‚æ•°
        commitment_cost: float = 0.25,
        ema_decay: float = 0.99,
        restart_threshold: float = 1.0,
    ):
        # ä½¿ç”¨çˆ¶ç±»åˆå§‹åŒ–åŸºç¡€ç»“æ„
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            down_block_types=down_block_types,
            up_block_types=up_block_types,
            block_out_channels=block_out_channels,
            layers_per_block=layers_per_block,
            act_fn=act_fn,
            latent_channels=latent_channels,
            sample_size=sample_size,
            num_vq_embeddings=num_vq_embeddings,
            norm_num_groups=norm_num_groups,
            vq_embed_dim=vq_embed_dim,
            scaling_factor=scaling_factor,
        )
        
        # æ›¿æ¢é‡åŒ–å™¨ä¸ºEMAç‰ˆæœ¬
        embed_dim = vq_embed_dim if vq_embed_dim is not None else latent_channels
        self.quantize = EMAVectorQuantizer(
            n_embed=num_vq_embeddings,
            embed_dim=embed_dim,
            beta=commitment_cost,
            decay=ema_decay,
            restart_threshold=restart_threshold,
        )
        
        print(f"ğŸ¯ å¾®å¤šæ™®å‹’VQ-VAEåˆå§‹åŒ–:")
        print(f"   ç æœ¬å¤§å°: {num_vq_embeddings}")
        print(f"   åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"   EMAè¡°å‡: {ema_decay}")
        print(f"   Commitmentæƒé‡: {commitment_cost}")
    
    def encode(self, x: torch.Tensor, return_dict: bool = True):
        """ç¼–ç ä¸ºç¦»æ•£tokens"""
        h = self.encoder(x)
        h = self.quant_conv(h)
        quantized, vq_loss, encoding_indices = self.quantize(h)
        
        if not return_dict:
            return (quantized, vq_loss, encoding_indices)
        
        return {
            'latents': quantized,
            'vq_loss': vq_loss,
            'encoding_indices': encoding_indices,
        }
    
    def decode(self, h: torch.Tensor, force_not_quantize: bool = False):
        """ä»æ½œåœ¨è¡¨ç¤ºè§£ç """
        if not force_not_quantize:
            quantized, _, _ = self.quantize(h)
            h = quantized
        
        h = self.post_quant_conv(h)
        dec = self.decoder(h)
        return dec
    
    def forward(self, sample: torch.Tensor, return_dict: bool = True):
        """å®Œæ•´çš„å‰å‘ä¼ æ’­"""
        encode_result = self.encode(sample, return_dict=True)
        dec = self.decode(encode_result['latents'])
        
        if not return_dict:
            return (dec, encode_result['vq_loss'])
        
        return {
            'sample': dec,
            'vq_loss': encode_result['vq_loss'],
            'encoding_indices': encode_result['encoding_indices'],
            'latents': encode_result['latents'],
        }
    
    def get_codebook_stats(self) -> Dict[str, float]:
        """è·å–ç æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        return self.quantize.get_codebook_usage()
    
    def plot_codebook_usage(self, save_path: Optional[str] = None):
        """å¯è§†åŒ–ç æœ¬ä½¿ç”¨æƒ…å†µ"""
        self.quantize.plot_usage_distribution(save_path)
    
    def reset_codebook_stats(self):
        """é‡ç½®ç æœ¬ç»Ÿè®¡"""
        self.quantize.usage_count.zero_()
        self.quantize.total_updates.zero_()
