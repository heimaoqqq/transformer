#!/usr/bin/env python3
"""
GPUé…ç½®æ£€æµ‹è„šæœ¬
ç”¨äºè¯Šæ–­Kaggleç¯å¢ƒä¸­çš„GPUé…ç½®é—®é¢˜
"""

import torch
import os
import subprocess
import sys

def check_nvidia_smi():
    """æ£€æŸ¥nvidia-smiè¾“å‡º"""
    print("ğŸ” æ£€æŸ¥nvidia-smiè¾“å‡º:")
    print("=" * 50)
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        print(result.stdout)
        if result.stderr:
            print("é”™è¯¯:", result.stderr)
    except FileNotFoundError:
        print("âŒ nvidia-smi æœªæ‰¾åˆ°")
    except Exception as e:
        print(f"âŒ è¿è¡Œnvidia-smiå¤±è´¥: {e}")

def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒå˜é‡"""
    print("\nğŸ” æ£€æŸ¥CUDAç¯å¢ƒå˜é‡:")
    print("=" * 50)
    
    cuda_vars = [
        'CUDA_VISIBLE_DEVICES',
        'CUDA_DEVICE_ORDER',
        'CUDA_LAUNCH_BLOCKING',
        'CUDA_CACHE_PATH',
        'CUDA_HOME',
        'CUDA_PATH'
    ]
    
    for var in cuda_vars:
        value = os.environ.get(var, 'Not set')
        print(f"{var}: {value}")

def check_pytorch_cuda():
    """æ£€æŸ¥PyTorch CUDAé…ç½®"""
    print("\nğŸ” æ£€æŸ¥PyTorch CUDAé…ç½®:")
    print("=" * 50)
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.current_device()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            print(f"\nGPU {i}:")
            print(f"  åç§°: {props.name}")
            print(f"  å†…å­˜: {memory_gb:.1f} GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"  å¤šå¤„ç†å™¨æ•°: {props.multi_processor_count}")
            
            # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                torch.cuda.set_device(i)
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                reserved = torch.cuda.memory_reserved(i) / 1024**3
                print(f"  å·²åˆ†é…å†…å­˜: {allocated:.2f} GB")
                print(f"  å·²ä¿ç•™å†…å­˜: {reserved:.2f} GB")
                print(f"  å¯ç”¨å†…å­˜: {memory_gb - reserved:.2f} GB")
    else:
        print("âŒ CUDAä¸å¯ç”¨")

def check_accelerate_config():
    """æ£€æŸ¥Accelerateé…ç½®"""
    print("\nğŸ” æ£€æŸ¥Accelerateé…ç½®:")
    print("=" * 50)
    
    try:
        from accelerate import Accelerator
        from accelerate.utils import gather_object
        
        accelerator = Accelerator()
        print(f"è®¾å¤‡: {accelerator.device}")
        print(f"è¿›ç¨‹æ•°: {accelerator.num_processes}")
        print(f"åˆ†å¸ƒå¼ç±»å‹: {accelerator.distributed_type}")
        print(f"æ˜¯å¦ä¸»è¿›ç¨‹: {accelerator.is_main_process}")
        print(f"æœ¬åœ°è¿›ç¨‹ç´¢å¼•: {accelerator.local_process_index}")
        print(f"è¿›ç¨‹ç´¢å¼•: {accelerator.process_index}")
        
    except ImportError:
        print("âŒ Accelerateæœªå®‰è£…")
    except Exception as e:
        print(f"âŒ Accelerateæ£€æŸ¥å¤±è´¥: {e}")

def test_gpu_memory():
    """æµ‹è¯•GPUå†…å­˜åˆ†é…"""
    print("\nğŸ” æµ‹è¯•GPUå†…å­˜åˆ†é…:")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return
    
    for i in range(torch.cuda.device_count()):
        print(f"\næµ‹è¯•GPU {i}:")
        try:
            torch.cuda.set_device(i)
            
            # å°è¯•åˆ†é…ä¸åŒå¤§å°çš„å†…å­˜
            test_sizes = [100, 500, 1000, 2000]  # MB
            
            for size_mb in test_sizes:
                try:
                    # åˆ†é…å†…å­˜
                    size_bytes = size_mb * 1024 * 1024
                    tensor = torch.zeros(size_bytes // 4, device=f'cuda:{i}')  # float32 = 4 bytes
                    print(f"  âœ… æˆåŠŸåˆ†é… {size_mb} MB")
                    del tensor
                    torch.cuda.empty_cache()
                except RuntimeError as e:
                    print(f"  âŒ åˆ†é… {size_mb} MB å¤±è´¥: {e}")
                    break
                    
        except Exception as e:
            print(f"  âŒ GPU {i} æµ‹è¯•å¤±è´¥: {e}")

def suggest_solutions():
    """å»ºè®®è§£å†³æ–¹æ¡ˆ"""
    print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
    print("=" * 50)
    
    gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
    
    if gpu_count == 0:
        print("1. æ£€æŸ¥Kaggle Notebookè®¾ç½®ä¸­æ˜¯å¦å¯ç”¨äº†GPU")
        print("2. é‡å¯Notebookå¹¶é‡æ–°é€‰æ‹©GPUè¿è¡Œæ—¶")
        print("3. æ£€æŸ¥Kaggleè´¦æˆ·çš„GPUé…é¢")
    elif gpu_count == 1:
        print("1. æ£€æŸ¥Kaggleæ˜¯å¦æä¾›äº†åŒGPUç¯å¢ƒ")
        print("2. å¯èƒ½éœ€è¦ç”³è¯·æ›´é«˜çº§çš„GPUå®ä¾‹")
        print("3. å½“å‰å•GPUé…ç½®å¯èƒ½æ˜¯æ­£å¸¸çš„")
    else:
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUï¼Œé…ç½®çœ‹èµ·æ¥æ­£å¸¸")
        print("å¦‚æœä»æœ‰å†…å­˜é—®é¢˜ï¼Œå¯èƒ½æ˜¯:")
        print("1. æ‰¹æ¬¡å¤§å°è¿‡å¤§")
        print("2. æ¨¡å‹å‚æ•°è¿‡å¤š")
        print("3. éœ€è¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPUé…ç½®è¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    check_nvidia_smi()
    check_cuda_environment()
    check_pytorch_cuda()
    check_accelerate_config()
    test_gpu_memory()
    suggest_solutions()
    
    print("\nâœ… è¯Šæ–­å®Œæˆ!")

if __name__ == "__main__":
    main()
