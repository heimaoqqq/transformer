#!/usr/bin/env python3
"""
ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨diffusersæ ‡å‡†VQModelè®­ç»ƒVQ-VAE
å®Œå…¨åŸºäºdiffusers.VQModelçš„æ ‡å‡†å®ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import argparse
from tqdm import tqdm
import sys
import matplotlib.pyplot as plt
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from diffusers import VQModel
    DIFFUSERS_AVAILABLE = True
    print("âœ… diffusersåº“å¯ç”¨")
except ImportError:
    print("âŒ diffusersåº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…æœ€æ–°ç‰ˆæœ¬: pip install diffusers")
    DIFFUSERS_AVAILABLE = False
    sys.exit(1)

from utils.data_loader import create_micro_doppler_dataset, create_datasets_with_split

class VQVAETrainer:
    """VQ-VAEè®­ç»ƒå™¨ - ç¬¬ä¸€æ­¥"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ ç¬¬ä¸€æ­¥ï¼šVQ-VAEè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åˆ›å»ºdiffusersæ ‡å‡†VQModel
        print("ğŸ—ï¸ åˆ›å»ºdiffusers VQModel")
        self.vqvae_model = VQModel(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256, 512, 512],
            layers_per_block=2,
            act_fn="silu",
            latent_channels=args.latent_channels,
            norm_num_groups=32,
            vq_embed_dim=args.vq_embed_dim,
            num_vq_embeddings=args.vocab_size,
            scaling_factor=0.18215,
        )
        
        self.vqvae_model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.vqvae_model.parameters())
        print(f"   ğŸ“Š VQ-VAEå‚æ•°: {total_params:,}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {args.vocab_size}")
        print(f"   ğŸ”¢ åµŒå…¥ç»´åº¦: {args.vq_embed_dim}")
        print(f"   ğŸ“ æ½œåœ¨é€šé“: {args.latent_channels}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.vqvae_model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            betas=(0.9, 0.95)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
        print(f"âœ… VQ-VAEè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def train(self):
        """è®­ç»ƒVQ-VAE"""
        print(f"ğŸš€ å¼€å§‹VQ-VAEè®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆå¸¦è‡ªåŠ¨åˆ’åˆ†ï¼‰
        if self.args.use_validation:
            train_dataset, val_dataset = create_datasets_with_split(
                data_dir=self.args.data_dir,
                train_ratio=0.8,
                val_ratio=0.2,
                return_user_id=True,  # åˆ†å±‚åˆ’åˆ†éœ€è¦user_idï¼Œè®­ç»ƒæ—¶å†å¤„ç†
                random_seed=42
            )

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_dataloader = DataLoader(
                train_dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                num_workers=self.args.num_workers,
                pin_memory=True
            )

            val_dataloader = DataLoader(
                val_dataset,
                batch_size=self.args.batch_size,
                shuffle=False,
                num_workers=self.args.num_workers,
                pin_memory=True
            )

            dataloader = train_dataloader  # ä¸»è¦è®­ç»ƒç”¨
            dataset = train_dataset  # ç”¨äºç»Ÿè®¡ä¿¡æ¯

            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
            print(f"   è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_dataset)}")
            print(f"   éªŒè¯æ ·æœ¬æ•°é‡: {len(val_dataset)}")
            print(f"   æ€»æ ·æœ¬æ•°é‡: {len(train_dataset) + len(val_dataset)}")
            print(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
            print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_dataloader)}")
            print(f"   éªŒè¯æ‰¹æ¬¡æ•°é‡: {len(val_dataloader)}")
        else:
            # ä¸ä½¿ç”¨éªŒè¯é›†ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
            dataset = create_micro_doppler_dataset(
                data_dir=self.args.data_dir,
                return_user_id=False  # ä¸ä½¿ç”¨éªŒè¯é›†æ—¶ç¡®å®ä¸éœ€è¦user_id
            )

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = DataLoader(
                dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                num_workers=self.args.num_workers,
                pin_memory=True
            )
            val_dataloader = None

            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
            print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
            print(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
            print(f"   æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
        
        best_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            print(f"\nğŸ¯ Epoch {epoch+1}/{self.args.num_epochs}")
            
            self.vqvae_model.train()
            
            total_loss = 0
            total_recon_loss = 0
            total_vq_loss = 0
            num_batches = 0
            
            pbar = tqdm(dataloader, desc=f"VQ-VAE Training")
            
            for batch_idx, batch in enumerate(pbar):
                # å¤„ç†batchæ ¼å¼ - æ”¯æŒå¸¦user_idçš„æ•°æ®
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    # æ ¼å¼: (images, user_ids) - åªå–imagesç”¨äºVQ-VAEè®­ç»ƒ
                    images, _ = batch
                    images = images.to(self.device)
                elif isinstance(batch, (list, tuple)):
                    images = batch[0].to(self.device) if len(batch) > 0 else batch.to(self.device)
                else:
                    images = batch.to(self.device)
                
                # VQ-VAEå‰å‘ä¼ æ’­
                # ç¼–ç 
                encoder_output = self.vqvae_model.encode(images)
                latents = encoder_output.latents
                
                # è§£ç 
                decoder_output = self.vqvae_model.decode(latents)
                reconstructed = decoder_output.sample
                
                # è®¡ç®—é‡æ„æŸå¤±
                recon_loss = nn.functional.mse_loss(reconstructed, images)
                
                # VQæŸå¤±ï¼ˆcommitment lossï¼‰
                vq_loss = 0
                if hasattr(encoder_output, 'commit_loss') and encoder_output.commit_loss is not None:
                    vq_loss = encoder_output.commit_loss.mean()
                
                # æ€»æŸå¤±
                total_batch_loss = recon_loss + self.args.commitment_cost * vq_loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_batch_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.vqvae_model.parameters(), 
                    max_norm=1.0
                )
                
                self.optimizer.step()
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += total_batch_loss.item()
                total_recon_loss += recon_loss.item()
                if isinstance(vq_loss, torch.Tensor):
                    total_vq_loss += vq_loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'recon': f'{recon_loss.item():.4f}',
                    'vq': f'{vq_loss.item() if isinstance(vq_loss, torch.Tensor) else vq_loss:.4f}',
                    'total': f'{total_batch_loss.item():.4f}'
                })
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches
            avg_recon_loss = total_recon_loss / num_batches
            avg_vq_loss = total_vq_loss / num_batches
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.scheduler.get_last_lr()[0]
            
            print(f"   ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"      æ€»æŸå¤±: {avg_loss:.4f}")
            print(f"      é‡æ„æŸå¤±: {avg_recon_loss:.4f}")
            print(f"      VQæŸå¤±: {avg_vq_loss:.4f}")
            print(f"      å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self._save_model(epoch, avg_loss, is_best=True)
                print(f"   âœ… ä¿å­˜æœ€ä½³VQ-VAEæ¨¡å‹ (æŸå¤±: {avg_loss:.4f})")
            
            # éªŒè¯é›†è¯„ä¼°
            val_loss = None
            if self.args.use_validation and val_dataloader is not None:
                val_loss = self._validate(val_dataloader)
                print(f"      éªŒè¯æŸå¤±: {val_loss:.4f}")

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹å’Œç”Ÿæˆæ ·æœ¬
            if (epoch + 1) % self.args.save_every == 0:
                self._save_model(epoch, avg_loss, is_best=False)
                self._generate_samples(epoch, dataloader)
                print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹å’Œæ ·æœ¬")
        
        print(f"\nğŸ‰ VQ-VAEè®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.4f}")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}")
        print(f"ğŸ”„ ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®­ç»ƒTransformer:")
        print(f"   python train_step2_transformer.py --vqvae_path {self.output_dir}/vqvae_best --data_dir {self.args.data_dir}")
    
    def _save_model(self, epoch, loss, is_best=False):
        """ä¿å­˜VQ-VAEæ¨¡å‹"""
        if is_best:
            save_path = self.output_dir / "vqvae_best"
        else:
            save_path = self.output_dir / f"vqvae_epoch_{epoch+1}"
        
        # ä½¿ç”¨diffusersæ ‡å‡†ä¿å­˜æ–¹æ³•
        self.vqvae_model.save_pretrained(save_path)
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        torch.save({
            'epoch': epoch,
            'loss': loss,
            'args': self.args,
        }, save_path / "training_info.pth")
    
    def _generate_samples(self, epoch, dataloader):
        """ç”Ÿæˆé‡æ„æ ·æœ¬"""
        self.vqvae_model.eval()
        
        # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
        with torch.no_grad():
            for batch in dataloader:
                if isinstance(batch, dict):
                    images = batch['image'][:4].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    # æ ¼å¼: (images, user_ids) - åªå–images
                    images, _ = batch
                    images = images[:4].to(self.device)
                elif isinstance(batch, (list, tuple)):
                    images = batch[0][:4].to(self.device)
                else:
                    images = batch[:4].to(self.device)
                
                # ç¼–ç å’Œè§£ç 
                encoder_output = self.vqvae_model.encode(images)
                decoder_output = self.vqvae_model.decode(encoder_output.latents)
                reconstructed = decoder_output.sample
                
                # ä¿å­˜å¯¹æ¯”å›¾åƒ
                self._save_comparison_images(images, reconstructed, epoch)
                break
        
        self.vqvae_model.train()
    
    def _save_comparison_images(self, original, reconstructed, epoch):
        """ä¿å­˜åŸå›¾å’Œé‡æ„å›¾çš„å¯¹æ¯”"""
        # åˆ›å»ºæ ·æœ¬ç›®å½•
        samples_dir = self.output_dir / "samples"
        samples_dir.mkdir(exist_ok=True)
        
        # è½¬æ¢ä¸ºnumpy
        original = original.cpu().numpy()
        reconstructed = reconstructed.cpu().numpy()
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        original = (original + 1) / 2
        reconstructed = (reconstructed + 1) / 2
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        
        for i in range(4):
            # åŸå›¾
            if original.shape[1] == 3:
                axes[0, i].imshow(original[i].transpose(1, 2, 0))
            else:
                axes[0, i].imshow(original[i, 0], cmap='viridis')
            axes[0, i].set_title(f'Original {i+1}')
            axes[0, i].axis('off')
            
            # é‡æ„å›¾
            if reconstructed.shape[1] == 3:
                axes[1, i].imshow(reconstructed[i].transpose(1, 2, 0))
            else:
                axes[1, i].imshow(reconstructed[i, 0], cmap='viridis')
            axes[1, i].set_title(f'Reconstructed {i+1}')
            axes[1, i].axis('off')
        
        plt.tight_layout()
        plt.savefig(samples_dir / f"epoch_{epoch+1:03d}.png", dpi=150, bbox_inches='tight')
        plt.close()

    def _validate(self, val_dataloader):
        """éªŒè¯æ¨¡å‹"""
        self.vqvae_model.eval()

        total_loss = 0
        total_recon_loss = 0
        total_vq_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch in val_dataloader:
                # å¤„ç†batchæ ¼å¼ - æ”¯æŒå¸¦user_idçš„æ•°æ®
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    # æ ¼å¼: (images, user_ids) - åªå–imagesç”¨äºVQ-VAEéªŒè¯
                    images, _ = batch
                    images = images.to(self.device)
                elif isinstance(batch, (list, tuple)):
                    images = batch[0].to(self.device) if len(batch) > 0 else batch.to(self.device)
                else:
                    images = batch.to(self.device)

                # VQ-VAEå‰å‘ä¼ æ’­
                encoder_output = self.vqvae_model.encode(images)
                latents = encoder_output.latents

                decoder_output = self.vqvae_model.decode(latents)
                reconstructed = decoder_output.sample

                # è®¡ç®—æŸå¤±
                recon_loss = nn.functional.mse_loss(reconstructed, images)

                vq_loss = 0
                if hasattr(encoder_output, 'commit_loss') and encoder_output.commit_loss is not None:
                    vq_loss = encoder_output.commit_loss.mean()

                total_batch_loss = recon_loss + self.args.commitment_cost * vq_loss

                # æ›´æ–°ç»Ÿè®¡
                total_loss += total_batch_loss.item()
                total_recon_loss += recon_loss.item()
                if isinstance(vq_loss, torch.Tensor):
                    total_vq_loss += vq_loss.item()
                num_batches += 1

        self.vqvae_model.train()
        return total_loss / num_batches if num_batches > 0 else 0

def main():
    parser = argparse.ArgumentParser(description="ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒVQ-VAE")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="./step1_vqvae_output", help="è¾“å‡ºç›®å½•")
    
    # VQ-VAEæ¨¡å‹å‚æ•°
    parser.add_argument("--vocab_size", type=int, default=1024, help="VQç æœ¬å¤§å°")
    parser.add_argument("--vq_embed_dim", type=int, default=256, help="VQåµŒå…¥ç»´åº¦")
    parser.add_argument("--latent_channels", type=int, default=4, help="æ½œåœ¨ç©ºé—´é€šé“æ•°")
    parser.add_argument("--commitment_cost", type=float, default=0.25, help="VQ commitmentæŸå¤±æƒé‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--save_every", type=int, default=10, help="ä¿å­˜æ£€æŸ¥ç‚¹é—´éš”")
    parser.add_argument("--use_validation", action="store_true", help="æ˜¯å¦ä½¿ç”¨éªŒè¯é›†")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--val_ratio", type=float, default=0.2, help="éªŒè¯é›†æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    print("ğŸš€ ç¬¬ä¸€æ­¥ï¼šVQ-VAEè®­ç»ƒ")
    print("=" * 60)
    print("ä½¿ç”¨diffusers.VQModelæ ‡å‡†å®ç°")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = VQVAETrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
