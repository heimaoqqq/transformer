#!/usr/bin/env python3
"""
ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨diffusersæ ‡å‡†VQModelè®­ç»ƒVQ-VAE
å®Œå…¨åŸºäºdiffusers.VQModelçš„æ ‡å‡†å®ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import argparse
from tqdm import tqdm
import sys
import matplotlib.pyplot as plt
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from diffusers import VQModel
    DIFFUSERS_AVAILABLE = True
    print("âœ… diffusersåº“å¯ç”¨")
except ImportError:
    print("âŒ diffusersåº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…æœ€æ–°ç‰ˆæœ¬: pip install diffusers")
    DIFFUSERS_AVAILABLE = False
    sys.exit(1)

from utils.data_loader import create_micro_doppler_dataset, create_datasets_with_split

class VQVAETrainer:
    """VQ-VAEè®­ç»ƒå™¨ - ç¬¬ä¸€æ­¥"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ ç¬¬ä¸€æ­¥ï¼šVQ-VAEè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æ ¹æ®ç›®æ ‡æ½œåœ¨ç©ºé—´å°ºå¯¸é…ç½®ä¸‹é‡‡æ ·å±‚æ•°
        downsample_layers = getattr(args, 'downsample_layers', 4)  # é»˜è®¤4å±‚

        # é…ç½®ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·å—
        down_blocks = ["DownEncoderBlock2D"] * downsample_layers
        up_blocks = ["UpDecoderBlock2D"] * downsample_layers

        # é…ç½®ç‰¹å¾é€šé“ï¼ˆæ ¹æ®å±‚æ•°è°ƒæ•´ï¼‰
        if downsample_layers == 2:
            channels = [128, 256]
        elif downsample_layers == 3:
            channels = [128, 256, 512]
        elif downsample_layers == 4:
            channels = [128, 256, 512, 512]
        else:
            channels = [128, 256, 512, 512]  # é»˜è®¤é…ç½®

        # åˆ›å»ºdiffusersæ ‡å‡†VQModel
        print("ğŸ—ï¸ åˆ›å»ºdiffusers VQModel")
        print(f"   ğŸ“ ä¸‹é‡‡æ ·å±‚æ•°: {downsample_layers}")
        print(f"   ğŸ“Š ç‰¹å¾é€šé“: {channels}")

        self.vqvae_model = VQModel(
            in_channels=3,
            out_channels=3,
            down_block_types=down_blocks,
            up_block_types=up_blocks,
            block_out_channels=channels,
            layers_per_block=2,
            act_fn="silu",
            latent_channels=args.latent_channels,
            norm_num_groups=32,
            vq_embed_dim=args.vq_embed_dim,
            num_vq_embeddings=args.vocab_size,
            scaling_factor=0.18215,
        )
        
        self.vqvae_model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.vqvae_model.parameters())
        print(f"   ğŸ“Š VQ-VAEå‚æ•°: {total_params:,}")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {args.vocab_size}")
        print(f"   ğŸ”¢ åµŒå…¥ç»´åº¦: {args.vq_embed_dim}")
        print(f"   ğŸ“ æ½œåœ¨é€šé“: {args.latent_channels}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.vqvae_model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            betas=(0.9, 0.95)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
        print(f"âœ… VQ-VAEè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

        # è¾“å‡ºå…³é”®è®­ç»ƒå‚æ•°
        self._log_training_parameters()

    def _log_training_parameters(self):
        """è¾“å‡ºå½±å“è®­ç»ƒè´¨é‡çš„å…³é”®å‚æ•°"""
        print("\n" + "="*60)
        print("ğŸ”§ å…³é”®è®­ç»ƒå‚æ•° - å½±å“è®­ç»ƒè´¨é‡çš„æ ¸å¿ƒé…ç½®")
        print("="*60)

        # æ¨¡å‹æ¶æ„å‚æ•°
        print("ğŸ“ æ¨¡å‹æ¶æ„å‚æ•°:")
        print(f"   ğŸ—ï¸ VQ-VAEæ¶æ„: diffusers.VQModel (æ ‡å‡†å®ç°)")
        print(f"   ğŸ“š ç æœ¬å¤§å°: {self.args.vocab_size}")
        print(f"   ğŸ”¢ VQåµŒå…¥ç»´åº¦: {self.args.vq_embed_dim}")
        print(f"   ğŸ“ æ½œåœ¨é€šé“æ•°: {self.args.latent_channels}")
        print(f"   ğŸ¯ ç¼©æ”¾å› å­: 0.18215 (diffusersæ ‡å‡†)")

        # è®­ç»ƒè¶…å‚æ•°
        print("\nâš™ï¸ è®­ç»ƒè¶…å‚æ•°:")
        print(f"   ğŸ“ˆ å­¦ä¹ ç‡: {self.args.learning_rate}")
        print(f"   ğŸ”„ è®­ç»ƒè½®æ•°: {self.args.num_epochs}")
        print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
        print(f"   âš–ï¸ æƒé‡è¡°å‡: {self.args.weight_decay}")
        print(f"   ğŸ’ª VQæ‰¿è¯ºæŸå¤±æƒé‡: {self.args.commitment_cost}")
        print(f"   ğŸ“Š ä¼˜åŒ–å™¨: AdamW (betas=(0.9, 0.95))")
        print(f"   ğŸ“‰ å­¦ä¹ ç‡è°ƒåº¦: CosineAnnealingLR")

        # æ•°æ®å¤„ç†å‚æ•°
        image_size = getattr(self.args, 'image_size', 128)
        high_quality = getattr(self.args, 'high_quality_resize', True)
        scale_ratio = 256 / image_size

        print("\nğŸ–¼ï¸ æ•°æ®å¤„ç†å‚æ•°:")
        print(f"   ğŸ“ åŸå§‹å›¾åƒå°ºå¯¸: 256Ã—256 (æ‚¨çš„å¾®å¤šæ™®å‹’æ•°æ®é›†)")
        print(f"   ğŸ¯ ç›®æ ‡å›¾åƒå°ºå¯¸: {image_size}Ã—{image_size}")

        # è¯¦ç»†çš„ç¼©æ”¾æŠ€æœ¯è¯´æ˜
        if high_quality:
            print(f"   ğŸ”§ ç¼©æ”¾æŠ€æœ¯: Lanczosæ’å€¼ + æŠ—é”¯é½¿ (é»˜è®¤é«˜è´¨é‡)")
            print(f"   âœ¨ æŠ€æœ¯ä¼˜åŠ¿: æœ€ä½³ç»†èŠ‚ä¿æŒï¼Œå‡å°‘ç¼©æ”¾ä¼ªå½±")
            print(f"   ğŸ¯ é€‚ç”¨åœºæ™¯: å¾®å¤šæ™®å‹’ç»†èŠ‚é‡è¦ï¼Œæ¨èç”Ÿäº§ä½¿ç”¨")
        else:
            print(f"   ğŸ”§ ç¼©æ”¾æŠ€æœ¯: åŒçº¿æ€§æ’å€¼ (å¿«é€Ÿæ¨¡å¼)")
            print(f"   âš¡ æŠ€æœ¯ä¼˜åŠ¿: å¤„ç†é€Ÿåº¦å¿«ï¼Œæ ‡å‡†è´¨é‡")
            print(f"   ğŸ¯ é€‚ç”¨åœºæ™¯: å¿«é€Ÿå®éªŒå’Œæµ‹è¯•")

        print(f"   ğŸ“Š ç¼©æ”¾æ¯”ä¾‹: {scale_ratio:.1f}Ã—ä¸‹é‡‡æ ·")
        if scale_ratio > 1:
            print(f"   âš ï¸  ä¿¡æ¯æŸå¤±: {(1 - 1/scale_ratio**2)*100:.1f}%åƒç´ ä¿¡æ¯")
        else:
            print(f"   âœ… ä¿¡æ¯ä¿æŒ: 100%åŸå§‹åˆ†è¾¨ç‡")

        print(f"   ğŸ¨ é¢œè‰²é€šé“: 3 (RGB)")
        print(f"   ğŸ“Š å½’ä¸€åŒ–èŒƒå›´: [-1, 1] (mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])")
        print(f"   ğŸ”„ æ•°æ®æµç¨‹: 256Ã—256 â†’ {'Lanczos' if high_quality else 'Bilinear'}ç¼©æ”¾({image_size}Ã—{image_size}) â†’ VQ-VAEç¼–ç ")

        # é˜²åç¼©æŠ€æœ¯
        print("\nğŸ›¡ï¸ ç æœ¬åç¼©é˜²æŠ¤æŠ€æœ¯:")
        print(f"   ğŸ”§ æŠ€æœ¯æ ˆ: diffuserså†…ç½®EMA + æ‰¿è¯ºæŸå¤±")
        print(f"   ğŸ“Š EMAè¡°å‡: è‡ªé€‚åº” (diffusersç®¡ç†)")
        print(f"   âš–ï¸ æ‰¿è¯ºæŸå¤±: {self.args.commitment_cost} * ||sg[z_e] - z_q||Â²")
        print(f"   ğŸ”„ ç æœ¬æ›´æ–°: æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA)")
        print(f"   ğŸ¯ é‡åŒ–ç­–ç•¥: æœ€è¿‘é‚» + æ¢¯åº¦ç›´é€šä¼°è®¡")

        # è´¨é‡ä¿è¯æŠ€æœ¯
        image_size = getattr(self.args, 'image_size', 128)
        downsample_layers = getattr(self.args, 'downsample_layers', 4)
        compression_factor = 2 ** downsample_layers
        latent_size = image_size // compression_factor

        print("\nğŸ¨ é«˜è´¨é‡é‡å»ºæŠ€æœ¯:")
        print(f"   ğŸ—ï¸ ç¼–ç å™¨: {downsample_layers}æ¬¡ä¸‹é‡‡æ · (å‹ç¼©å› å­: {compression_factor}:1)")

        # åŠ¨æ€ç”Ÿæˆä¸‹é‡‡æ ·æµç¨‹æ˜¾ç¤º
        sizes = [image_size]
        for i in range(downsample_layers):
            sizes.append(sizes[-1] // 2)
        size_flow = "â†’".join(map(str, sizes))
        print(f"   ğŸ“ å°ºå¯¸å˜åŒ–: {size_flow}")

        print(f"   ğŸ”„ è§£ç å™¨: {downsample_layers}æ¬¡ä¸Šé‡‡æ · (å¯¹åº”æ¢å¤)")
        print(f"   ğŸ“Š æ½œåœ¨ç©ºé—´: {latent_size}Ã—{latent_size}Ã—{self.args.latent_channels} (å‹ç¼©æ¯”{compression_factor}:1)")

        # æ ¹æ®å±‚æ•°æ˜¾ç¤ºç‰¹å¾é€šé“
        if downsample_layers == 2:
            channels_str = "[128, 256]"
        elif downsample_layers == 3:
            channels_str = "[128, 256, 512]"
        elif downsample_layers == 4:
            channels_str = "[128, 256, 512, 512]"
        else:
            channels_str = "[128, 256, 512, 512]"

        print(f"   ğŸ“ˆ ç‰¹å¾é€šé“: {channels_str} (é€å±‚å¢åŠ )")
        print(f"   ğŸ¯ æ¿€æ´»å‡½æ•°: SiLU (Swish) - å¹³æ»‘æ¢¯åº¦")
        print(f"   ğŸ“Š å½’ä¸€åŒ–: GroupNorm (32ç»„) - ç¨³å®šè®­ç»ƒ")
        print(f"   ğŸ”§ æ®‹å·®è¿æ¥: æ·±å±‚ç‰¹å¾ä¿æŒ")
        print(f"   âš¡ æ³¨æ„åŠ›æœºåˆ¶: æ—  (ä¸“æ³¨é‡å»ºè´¨é‡)")

        # è®­ç»ƒç­–ç•¥
        print("\nğŸš€ è®­ç»ƒç­–ç•¥:")
        print(f"   ğŸ’¾ æ¨¡å‹ä¿å­˜: æœ€ä½³æŸå¤± + æ¯{self.args.save_every}è½®æ£€æŸ¥ç‚¹")
        print(f"   ğŸ“Š éªŒè¯è¯„ä¼°: {'å¯ç”¨' if self.args.use_validation else 'ç¦ç”¨'}")
        print(f"   ğŸ–¼ï¸ æ ·æœ¬ç”Ÿæˆ: æ¯{self.args.save_every}è½®ç”Ÿæˆé‡å»ºå¯¹æ¯”å›¾")
        print(f"   âœ‚ï¸ æ¢¯åº¦è£å‰ª: max_norm=1.0 (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)")
        print(f"   ğŸ¯ æŸå¤±å‡½æ•°: MSEé‡å»ºæŸå¤± + VQæ‰¿è¯ºæŸå¤±")

        print("="*60)
        print("ğŸ’¡ æŠ€æœ¯è¯´æ˜:")
        print("   ğŸ–¼ï¸ å›¾åƒç¼©æ”¾: Lanczosæ’å€¼+æŠ—é”¯é½¿ (é»˜è®¤é«˜è´¨é‡)")
        print("   ğŸ”¬ æ½œåœ¨ç¼©æ”¾: diffusersæ ‡å‡†scaling_factor=0.18215")
        print("   ğŸ›¡ï¸ é˜²åç¼©: EMAæ›´æ–° + æ‰¿è¯ºæŸå¤± + æ¢¯åº¦ç›´é€šä¼°è®¡")
        print("   ğŸ¨ é«˜è´¨é‡: SiLUæ¿€æ´» + GroupNorm + æ®‹å·®è¿æ¥")
        print("   ğŸ“Š æˆç†ŸæŠ€æœ¯: åŸºäºVQGAN/VQVAE-2çš„æˆç†Ÿæ¶æ„")
        print("="*60 + "\n")

    def train(self):
        """è®­ç»ƒVQ-VAE"""
        print(f"ğŸš€ å¼€å§‹VQ-VAEè®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆå¸¦è‡ªåŠ¨åˆ’åˆ†ï¼‰
        if self.args.use_validation:
            train_dataset, val_dataset = create_datasets_with_split(
                data_dir=self.args.data_dir,
                train_ratio=0.8,
                val_ratio=0.2,
                return_user_id=True,  # åˆ†å±‚åˆ’åˆ†éœ€è¦user_idï¼Œè®­ç»ƒæ—¶å†å¤„ç†
                random_seed=42,
                image_size=self.args.image_size,
                high_quality_resize=self.args.high_quality_resize
            )

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_dataloader = DataLoader(
                train_dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                num_workers=self.args.num_workers,
                pin_memory=True
            )

            val_dataloader = DataLoader(
                val_dataset,
                batch_size=self.args.batch_size,
                shuffle=False,
                num_workers=self.args.num_workers,
                pin_memory=True
            )

            dataloader = train_dataloader  # ä¸»è¦è®­ç»ƒç”¨
            dataset = train_dataset  # ç”¨äºç»Ÿè®¡ä¿¡æ¯

            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
            print(f"   è®­ç»ƒæ ·æœ¬æ•°é‡: {len(train_dataset)}")
            print(f"   éªŒè¯æ ·æœ¬æ•°é‡: {len(val_dataset)}")
            print(f"   æ€»æ ·æœ¬æ•°é‡: {len(train_dataset) + len(val_dataset)}")
            print(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
            print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_dataloader)}")
            print(f"   éªŒè¯æ‰¹æ¬¡æ•°é‡: {len(val_dataloader)}")
        else:
            # ä¸ä½¿ç”¨éªŒè¯é›†ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
            dataset = create_micro_doppler_dataset(
                data_dir=self.args.data_dir,
                return_user_id=False  # ä¸ä½¿ç”¨éªŒè¯é›†æ—¶ç¡®å®ä¸éœ€è¦user_id
            )

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = DataLoader(
                dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                num_workers=self.args.num_workers,
                pin_memory=True
            )
            val_dataloader = None

            print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
            print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
            print(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
            print(f"   æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
        
        best_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            print(f"\nğŸ¯ Epoch {epoch+1}/{self.args.num_epochs}")
            
            self.vqvae_model.train()
            
            total_loss = 0
            total_recon_loss = 0
            total_vq_loss = 0
            num_batches = 0
            
            pbar = tqdm(dataloader, desc=f"VQ-VAE Training")
            
            for batch_idx, batch in enumerate(pbar):
                # å¤„ç†batchæ ¼å¼ - æ”¯æŒå¸¦user_idçš„æ•°æ®
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    # æ ¼å¼: (images, user_ids) - åªå–imagesç”¨äºVQ-VAEè®­ç»ƒ
                    images, _ = batch
                    images = images.to(self.device)
                elif isinstance(batch, (list, tuple)):
                    images = batch[0].to(self.device) if len(batch) > 0 else batch.to(self.device)
                else:
                    images = batch.to(self.device)
                
                # VQ-VAEå‰å‘ä¼ æ’­
                # ç¼–ç 
                encoder_output = self.vqvae_model.encode(images)
                latents = encoder_output.latents
                
                # è§£ç 
                decoder_output = self.vqvae_model.decode(latents)
                reconstructed = decoder_output.sample
                
                # è®¡ç®—é‡æ„æŸå¤±
                recon_loss = nn.functional.mse_loss(reconstructed, images)
                
                # VQæŸå¤±ï¼ˆcommitment lossï¼‰
                vq_loss = 0

                # VQæŸå¤±åœ¨decoder_outputä¸­ï¼ˆæ ¹æ®diffusersæºç ï¼‰
                if hasattr(decoder_output, 'commit_loss') and decoder_output.commit_loss is not None:
                    vq_loss = decoder_output.commit_loss.mean()
                elif hasattr(decoder_output, 'quantization_loss') and decoder_output.quantization_loss is not None:
                    vq_loss = decoder_output.quantization_loss.mean()
                # å¤‡é€‰ï¼šæ£€æŸ¥encoder_output
                elif hasattr(encoder_output, 'commit_loss') and encoder_output.commit_loss is not None:
                    vq_loss = encoder_output.commit_loss.mean()
                elif hasattr(encoder_output, 'quantization_loss') and encoder_output.quantization_loss is not None:
                    vq_loss = encoder_output.quantization_loss.mean()
                else:
                    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè®¾ä¸º0
                    vq_loss = torch.tensor(0.0, device=images.device)
                
                # æ€»æŸå¤±
                total_batch_loss = recon_loss + self.args.commitment_cost * vq_loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_batch_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.vqvae_model.parameters(), 
                    max_norm=1.0
                )
                
                self.optimizer.step()
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += total_batch_loss.item()
                total_recon_loss += recon_loss.item()
                if isinstance(vq_loss, torch.Tensor):
                    total_vq_loss += vq_loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'recon': f'{recon_loss.item():.4f}',
                    'vq': f'{vq_loss.item() if isinstance(vq_loss, torch.Tensor) else vq_loss:.4f}',
                    'total': f'{total_batch_loss.item():.4f}'
                })
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches
            avg_recon_loss = total_recon_loss / num_batches
            avg_vq_loss = total_vq_loss / num_batches
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.scheduler.get_last_lr()[0]
            
            # è®¡ç®—ç æœ¬åˆ©ç”¨ç‡
            codebook_usage = self._calculate_codebook_usage(dataloader)

            print(f"   ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"      æ€»æŸå¤±: {avg_loss:.4f}")
            print(f"      é‡æ„æŸå¤±: {avg_recon_loss:.4f}")
            print(f"      VQæŸå¤±: {avg_vq_loss:.6f}")  # å¢åŠ ç²¾åº¦æ˜¾ç¤º
            print(f"      å­¦ä¹ ç‡: {current_lr:.6f}")
            print(f"      ğŸ“š ç æœ¬åˆ©ç”¨ç‡: {codebook_usage:.2f}% ({codebook_usage*self.args.vocab_size/100:.0f}/{self.args.vocab_size})")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self._save_model(epoch, avg_loss, is_best=True)
                print(f"   âœ… ä¿å­˜æœ€ä½³VQ-VAEæ¨¡å‹ (æŸå¤±: {avg_loss:.4f})")
            
            # éªŒè¯é›†è¯„ä¼°
            val_loss = None
            if self.args.use_validation and val_dataloader is not None:
                val_loss = self._validate(val_dataloader)
                print(f"      éªŒè¯æŸå¤±: {val_loss:.4f}")

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹å’Œç”Ÿæˆæ ·æœ¬
            if (epoch + 1) % self.args.save_every == 0:
                self._save_model(epoch, avg_loss, is_best=False)
                self._generate_samples(epoch, dataloader)
                print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹å’Œæ ·æœ¬")
        
        print(f"\nğŸ‰ VQ-VAEè®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.4f}")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}")
        print(f"ğŸ”„ ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®­ç»ƒTransformer:")
        print(f"   python train_step2_transformer.py --vqvae_path {self.output_dir}/vqvae_best --data_dir {self.args.data_dir}")
    
    def _save_model(self, epoch, loss, is_best=False):
        """ä¿å­˜VQ-VAEæ¨¡å‹"""
        if is_best:
            save_path = self.output_dir / "vqvae_best"
        else:
            save_path = self.output_dir / f"vqvae_epoch_{epoch+1}"
        
        # ä½¿ç”¨diffusersæ ‡å‡†ä¿å­˜æ–¹æ³•
        self.vqvae_model.save_pretrained(save_path)
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        torch.save({
            'epoch': epoch,
            'loss': loss,
            'args': self.args,
        }, save_path / "training_info.pth")
    
    def _generate_samples(self, epoch, dataloader):
        """ç”Ÿæˆé‡æ„æ ·æœ¬"""
        self.vqvae_model.eval()
        
        # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
        with torch.no_grad():
            for batch in dataloader:
                if isinstance(batch, dict):
                    images = batch['image'][:4].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    # æ ¼å¼: (images, user_ids) - åªå–images
                    images, _ = batch
                    images = images[:4].to(self.device)
                elif isinstance(batch, (list, tuple)):
                    images = batch[0][:4].to(self.device)
                else:
                    images = batch[:4].to(self.device)
                
                # ç¼–ç å’Œè§£ç 
                encoder_output = self.vqvae_model.encode(images)
                decoder_output = self.vqvae_model.decode(encoder_output.latents)
                reconstructed = decoder_output.sample
                
                # ä¿å­˜å¯¹æ¯”å›¾åƒ
                self._save_comparison_images(images, reconstructed, epoch)
                break
        
        self.vqvae_model.train()
    
    def _save_comparison_images(self, original, reconstructed, epoch):
        """ä¿å­˜åŸå›¾å’Œé‡æ„å›¾çš„å¯¹æ¯”"""
        # åˆ›å»ºæ ·æœ¬ç›®å½•
        samples_dir = self.output_dir / "samples"
        samples_dir.mkdir(exist_ok=True)
        
        # è½¬æ¢ä¸ºnumpy
        original = original.cpu().numpy()
        reconstructed = reconstructed.cpu().numpy()
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        original = (original + 1) / 2
        reconstructed = (reconstructed + 1) / 2
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        
        for i in range(4):
            # åŸå›¾
            if original.shape[1] == 3:
                axes[0, i].imshow(original[i].transpose(1, 2, 0))
            else:
                axes[0, i].imshow(original[i, 0], cmap='viridis')
            axes[0, i].set_title(f'Original {i+1}')
            axes[0, i].axis('off')
            
            # é‡æ„å›¾
            if reconstructed.shape[1] == 3:
                axes[1, i].imshow(reconstructed[i].transpose(1, 2, 0))
            else:
                axes[1, i].imshow(reconstructed[i, 0], cmap='viridis')
            axes[1, i].set_title(f'Reconstructed {i+1}')
            axes[1, i].axis('off')
        
        plt.tight_layout()
        plt.savefig(samples_dir / f"epoch_{epoch+1:03d}.png", dpi=150, bbox_inches='tight')
        plt.close()

    def _validate(self, val_dataloader):
        """éªŒè¯æ¨¡å‹"""
        self.vqvae_model.eval()

        total_loss = 0
        total_recon_loss = 0
        total_vq_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch in val_dataloader:
                # å¤„ç†batchæ ¼å¼ - æ”¯æŒå¸¦user_idçš„æ•°æ®
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    # æ ¼å¼: (images, user_ids) - åªå–imagesç”¨äºVQ-VAEéªŒè¯
                    images, _ = batch
                    images = images.to(self.device)
                elif isinstance(batch, (list, tuple)):
                    images = batch[0].to(self.device) if len(batch) > 0 else batch.to(self.device)
                else:
                    images = batch.to(self.device)

                # VQ-VAEå‰å‘ä¼ æ’­
                encoder_output = self.vqvae_model.encode(images)
                latents = encoder_output.latents

                decoder_output = self.vqvae_model.decode(latents)
                reconstructed = decoder_output.sample

                # è®¡ç®—æŸå¤±
                recon_loss = nn.functional.mse_loss(reconstructed, images)

                vq_loss = 0
                if hasattr(encoder_output, 'commit_loss') and encoder_output.commit_loss is not None:
                    vq_loss = encoder_output.commit_loss.mean()
                elif hasattr(encoder_output, 'quantization_loss') and encoder_output.quantization_loss is not None:
                    vq_loss = encoder_output.quantization_loss.mean()
                elif hasattr(encoder_output, 'loss') and encoder_output.loss is not None:
                    vq_loss = encoder_output.loss.mean()
                else:
                    vq_loss = torch.tensor(0.0, device=images.device)

                total_batch_loss = recon_loss + self.args.commitment_cost * vq_loss

                # æ›´æ–°ç»Ÿè®¡
                total_loss += total_batch_loss.item()
                total_recon_loss += recon_loss.item()
                if isinstance(vq_loss, torch.Tensor):
                    total_vq_loss += vq_loss.item()
                num_batches += 1

        self.vqvae_model.train()
        return total_loss / num_batches if num_batches > 0 else 0

    def _calculate_codebook_usage(self, dataloader):
        """è®¡ç®—ç æœ¬åˆ©ç”¨ç‡"""
        self.vqvae_model.eval()

        used_codes = set()
        total_codes = self.args.vocab_size

        with torch.no_grad():
            # åªä½¿ç”¨ä¸€éƒ¨åˆ†æ•°æ®æ¥è®¡ç®—åˆ©ç”¨ç‡ï¼Œé¿å…å¤ªæ…¢
            sample_count = 0
            max_samples = min(10, len(dataloader))  # å‡å°‘åˆ°10ä¸ªbatchè¿›è¡Œè°ƒè¯•

            for batch in dataloader:
                if sample_count >= max_samples:
                    break

                # å¤„ç†batchæ ¼å¼
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, _ = batch
                    images = images.to(self.device)
                elif isinstance(batch, (list, tuple)):
                    images = batch[0].to(self.device) if len(batch) > 0 else batch.to(self.device)
                else:
                    images = batch.to(self.device)

                try:
                    # æ–¹æ³•1ï¼šå°è¯•ç›´æ¥è·å–é‡åŒ–ç´¢å¼•
                    encoder_output = self.vqvae_model.encode(images, return_dict=True)

                    # è°ƒè¯•ï¼šåœ¨ç¬¬ä¸€ä¸ªbatchæ—¶è¾“å‡ºencoder_outputçš„æ‰€æœ‰å±æ€§
                    if sample_count == 0:
                        print(f"\nğŸ” ç æœ¬åˆ©ç”¨ç‡è°ƒè¯•:")
                        print(f"   encoder_outputç±»å‹: {type(encoder_output)}")
                        print(f"   encoder_outputå±æ€§: {list(encoder_output.__dict__.keys()) if hasattr(encoder_output, '__dict__') else 'N/A'}")
                        if hasattr(encoder_output, 'latents'):
                            print(f"   latentså½¢çŠ¶: {encoder_output.latents.shape}")

                    # å°è¯•è·å–é‡åŒ–ç´¢å¼•çš„å¤šç§æ–¹æ³•
                    indices = None

                    # æ–¹æ³•1ï¼šæ£€æŸ¥encoder_outputçš„å„ç§å¯èƒ½å±æ€§
                    for attr_name in ['encoding_indices', 'quantization_indices', 'indices', 'min_encoding_indices']:
                        if hasattr(encoder_output, attr_name):
                            indices = getattr(encoder_output, attr_name)
                            if sample_count == 0:
                                print(f"   âœ… æ‰¾åˆ°ç´¢å¼•å±æ€§: {attr_name}, å½¢çŠ¶: {indices.shape if indices is not None else 'None'}")
                            break

                    # æ–¹æ³•2ï¼šå°è¯•è®¿é—®VQå±‚
                    if indices is None:
                        try:
                            # å°è¯•ç›´æ¥è®¿é—®quantizeå±‚
                            if hasattr(self.vqvae_model, 'quantize'):
                                quantize_result = self.vqvae_model.quantize(encoder_output.latents)
                                if hasattr(quantize_result, 'min_encoding_indices'):
                                    indices = quantize_result.min_encoding_indices
                                    if sample_count == 0:
                                        print(f"   âœ… é€šè¿‡quantizeå±‚è·å–ç´¢å¼•, å½¢çŠ¶: {indices.shape}")
                        except Exception as e:
                            if sample_count == 0:
                                print(f"   âš ï¸ quantizeå±‚è®¿é—®å¤±è´¥: {e}")

                    # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ–¹æ³•
                    if indices is None:
                        if sample_count == 0:
                            print(f"   âš ï¸ æœªæ‰¾åˆ°é‡åŒ–ç´¢å¼•ï¼Œå¯èƒ½éœ€è¦å…¶ä»–æ–¹æ³•")
                            # æ£€æŸ¥VQModelçš„æ‰€æœ‰æ–¹æ³•
                            methods = [method for method in dir(self.vqvae_model) if not method.startswith('_')]
                            print(f"   VQModelå¯ç”¨æ–¹æ³•: {methods[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
                        sample_count += 1
                        continue

                    # æ”¶é›†ä½¿ç”¨çš„ç æœ¬ç´¢å¼•
                    if indices is not None:
                        unique_indices = torch.unique(indices.flatten()).cpu().numpy()
                        used_codes.update(unique_indices)
                        if sample_count == 0:
                            print(f"   ğŸ“Š ç¬¬ä¸€ä¸ªbatchä½¿ç”¨çš„ç æœ¬æ•°: {len(unique_indices)}")

                except Exception as e:
                    if sample_count == 0:
                        print(f"   âŒ ç æœ¬åˆ©ç”¨ç‡è®¡ç®—å‡ºé”™: {e}")
                    sample_count += 1
                    continue

                sample_count += 1

        self.vqvae_model.train()

        # è®¡ç®—åˆ©ç”¨ç‡
        if len(used_codes) > 0:
            usage_rate = len(used_codes) / total_codes * 100
            print(f"   ğŸ“š æ€»å…±ä½¿ç”¨çš„ç æœ¬æ•°: {len(used_codes)}/{total_codes}")
        else:
            usage_rate = 0
            print(f"   âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•ç æœ¬ä½¿ç”¨")

        return usage_rate

def main():
    parser = argparse.ArgumentParser(description="ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒVQ-VAE")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="./step1_vqvae_output", help="è¾“å‡ºç›®å½•")
    
    # VQ-VAEæ¨¡å‹å‚æ•°
    parser.add_argument("--vocab_size", type=int, default=1024, help="VQç æœ¬å¤§å°")
    parser.add_argument("--vq_embed_dim", type=int, default=256, help="VQåµŒå…¥ç»´åº¦")
    parser.add_argument("--latent_channels", type=int, default=4, help="æ½œåœ¨ç©ºé—´é€šé“æ•°")
    parser.add_argument("--commitment_cost", type=float, default=0.25, help="VQ commitmentæŸå¤±æƒé‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--save_every", type=int, default=10, help="ä¿å­˜æ£€æŸ¥ç‚¹é—´éš”")
    parser.add_argument("--use_validation", action="store_true", help="æ˜¯å¦ä½¿ç”¨éªŒè¯é›†")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--val_ratio", type=float, default=0.2, help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--image_size", type=int, default=128, help="ç›®æ ‡å›¾åƒå°ºå¯¸ (128=å¿«é€Ÿè®­ç»ƒ, 256=æœ€é«˜è´¨é‡)")
    parser.add_argument("--high_quality_resize", action="store_true", default=True, help="ä½¿ç”¨Lanczosæ’å€¼+æŠ—é”¯é½¿ (é»˜è®¤æ¨è)")
    parser.add_argument("--fast_resize", action="store_false", dest="high_quality_resize", help="ä½¿ç”¨åŒçº¿æ€§æ’å€¼ (ä»…ç”¨äºå¿«é€Ÿæµ‹è¯•)")
    parser.add_argument("--downsample_layers", type=int, default=4, choices=[2, 3, 4],
                       help="ä¸‹é‡‡æ ·å±‚æ•° (2=32Ã—32æ½œåœ¨ç©ºé—´, 3=16Ã—16æ½œåœ¨ç©ºé—´, 4=8Ã—8æ½œåœ¨ç©ºé—´)")
    parser.add_argument("--latent_space", type=str, choices=['32x32', '16x16', '8x8'],
                       help="ç›´æ¥æŒ‡å®šæ½œåœ¨ç©ºé—´å°ºå¯¸ (ä¼šè¦†ç›–downsample_layers)")

    
    args = parser.parse_args()

    # å¤„ç†æ½œåœ¨ç©ºé—´å°ºå¯¸å‚æ•°
    if hasattr(args, 'latent_space') and args.latent_space:
        if args.latent_space == '32x32':
            args.downsample_layers = 2
        elif args.latent_space == '16x16':
            args.downsample_layers = 3
        elif args.latent_space == '8x8':
            args.downsample_layers = 4

    print("ğŸš€ ç¬¬ä¸€æ­¥ï¼šVQ-VAEè®­ç»ƒ")
    print("=" * 60)
    print("ä½¿ç”¨diffusers.VQModelæ ‡å‡†å®ç°")
    print("=" * 60)

    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = VQVAETrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
