#!/usr/bin/env python3
"""
è·¨ç¯å¢ƒå…¼å®¹æ€§æµ‹è¯•è„šæœ¬
éªŒè¯VQ-VAEæ¨¡å‹åœ¨ä¸åŒç¯å¢ƒé—´çš„å…¼å®¹æ€§
"""

import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

def test_vqvae_loading():
    """æµ‹è¯•VQ-VAEæ¨¡å‹åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•VQ-VAEæ¨¡å‹åŠ è½½å…¼å®¹æ€§...")
    
    # æ¨¡æ‹Ÿä¸åŒç¯å¢ƒä¸‹çš„æ¨¡å‹åŠ è½½
    try:
        from models.vqvae_model import MicroDopplerVQVAE
        print("âœ… MicroDopplerVQVAEå¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        model = MicroDopplerVQVAE(
            num_vq_embeddings=1024,
            commitment_cost=0.25,
            ema_decay=0.99,
        )
        print("âœ… VQ-VAEæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¿å­˜
        test_checkpoint = {
            'epoch': 10,
            'model_state_dict': model.state_dict(),
            'args': type('Args', (), {
                'codebook_size': 1024,
                'commitment_cost': 0.25,
                'ema_decay': 0.99,
            })(),
        }
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_path = Path("temp_vqvae_test.pth")
        torch.save(test_checkpoint, temp_path)
        print("âœ… VQ-VAEæ¨¡å‹ä¿å­˜æˆåŠŸ")
        
        # æµ‹è¯•åŠ è½½
        loaded_checkpoint = torch.load(temp_path, map_location="cpu")
        
        # é‡å»ºæ¨¡å‹
        loaded_model = MicroDopplerVQVAE(
            num_vq_embeddings=loaded_checkpoint['args'].codebook_size,
            commitment_cost=loaded_checkpoint['args'].commitment_cost,
            ema_decay=loaded_checkpoint['args'].ema_decay,
        )
        loaded_model.load_state_dict(loaded_checkpoint['model_state_dict'])
        print("âœ… VQ-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•å…³é”®æ¥å£
        test_input = torch.randn(1, 3, 128, 128)
        with torch.no_grad():
            # ç¼–ç 
            encoded = loaded_model.encode(test_input)
            tokens = encoded['encoding_indices']
            print(f"âœ… ç¼–ç æˆåŠŸ: tokens shape = {tokens.shape}")
            
            # æµ‹è¯•ç æœ¬åµŒå…¥è®¿é—® (Transformeré˜¶æ®µéœ€è¦çš„)
            embedding_weight = loaded_model.quantize.embedding.weight
            print(f"âœ… ç æœ¬åµŒå…¥è®¿é—®æˆåŠŸ: shape = {embedding_weight.shape}")
            
            # æµ‹è¯•è§£ç  (Transformeré˜¶æ®µéœ€è¦çš„)
            latent_size = int(tokens.shape[1] ** 0.5)
            tokens_2d = tokens.view(1, latent_size, latent_size)
            quantized_latents = loaded_model.quantize.embedding(tokens_2d)
            quantized_latents = quantized_latents.permute(0, 3, 1, 2)
            
            decoded = loaded_model.decode(quantized_latents, force_not_quantize=True)
            print(f"âœ… è§£ç æˆåŠŸ: output shape = {decoded.shape}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_path.unlink()
        
        return True
        
    except Exception as e:
        print(f"âŒ VQ-VAEå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_transformer_interface():
    """æµ‹è¯•Transformeréœ€è¦çš„VQ-VAEæ¥å£"""
    print("\nğŸ¤– æµ‹è¯•Transformeræ¥å£å…¼å®¹æ€§...")
    
    try:
        from models.vqvae_model import MicroDopplerVQVAE
        
        # åˆ›å»ºVQ-VAEæ¨¡å‹
        vqvae_model = MicroDopplerVQVAE(
            num_vq_embeddings=1024,
            commitment_cost=0.25,
            ema_decay=0.99,
        )
        vqvae_model.eval()
        
        # æ¨¡æ‹ŸTransformerç”Ÿæˆçš„tokenåºåˆ—
        batch_size = 1
        seq_len = 256  # 16x16çš„tokenåºåˆ—
        vocab_size = 1024
        
        # ç”Ÿæˆéšæœºtokenåºåˆ— (æ¨¡æ‹ŸTransformerè¾“å‡º)
        generated_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))
        print(f"âœ… æ¨¡æ‹ŸTransformerè¾“å‡º: {generated_tokens.shape}")
        
        # è½¬æ¢ä¸º2D (Transformeré˜¶æ®µçš„å…³é”®æ­¥éª¤)
        latent_size = int(seq_len ** 0.5)  # 16
        tokens_2d = generated_tokens.view(batch_size, latent_size, latent_size)
        print(f"âœ… é‡å¡‘ä¸º2D: {tokens_2d.shape}")
        
        with torch.no_grad():
            # ç¡®ä¿tokenç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†… (Transformeré˜¶æ®µçš„å…³é”®æ­¥éª¤)
            tokens_2d = torch.clamp(tokens_2d, 0, vqvae_model.quantize.n_embed - 1)
            
            # è·å–é‡åŒ–å‘é‡ (Transformeré˜¶æ®µçš„å…³é”®æ­¥éª¤)
            quantized_latents = vqvae_model.quantize.embedding(tokens_2d)
            quantized_latents = quantized_latents.permute(0, 3, 1, 2)  # [B, C, H, W]
            print(f"âœ… è·å–é‡åŒ–å‘é‡: {quantized_latents.shape}")
            
            # è§£ç ä¸ºå›¾åƒ (Transformeré˜¶æ®µçš„å…³é”®æ­¥éª¤)
            generated_image = vqvae_model.decode(quantized_latents, force_not_quantize=True)
            print(f"âœ… è§£ç ä¸ºå›¾åƒ: {generated_image.shape}")
            
            # å½’ä¸€åŒ– (Transformeré˜¶æ®µçš„å…³é”®æ­¥éª¤)
            image = (generated_image.squeeze(0) * 0.5 + 0.5).clamp(0, 1)
            print(f"âœ… å›¾åƒå½’ä¸€åŒ–: {image.shape}")
        
        print("âœ… æ‰€æœ‰Transformeræ¥å£æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ Transformeræ¥å£æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_version_independence():
    """æµ‹è¯•ç‰ˆæœ¬ç‹¬ç«‹æ€§"""
    print("\nğŸ”„ æµ‹è¯•ç‰ˆæœ¬ç‹¬ç«‹æ€§...")
    
    try:
        # æµ‹è¯•æ¨¡å‹å®šä¹‰ä¸ä¾èµ–diffuserså…·ä½“ç‰ˆæœ¬
        from models.vqvae_model import MicroDopplerVQVAE, EMAVectorQuantizer
        print("âœ… è‡ªå®šä¹‰æ¨¡å‹ç±»å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•å…³é”®ç»„ä»¶
        quantizer = EMAVectorQuantizer(n_embed=1024, embed_dim=256)
        print("âœ… è‡ªå®šä¹‰é‡åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¸ä¾èµ–diffusersçš„å…·ä½“API
        model = MicroDopplerVQVAE()
        state_dict = model.state_dict()
        print(f"âœ… æ¨¡å‹æƒé‡è·å–æˆåŠŸ: {len(state_dict)} ä¸ªå‚æ•°")
        
        # éªŒè¯æƒé‡æ˜¯çº¯PyTorchæ ¼å¼
        for key, value in list(state_dict.items())[:3]:
            assert isinstance(value, torch.Tensor), f"æƒé‡ {key} ä¸æ˜¯PyTorchå¼ é‡"
        print("âœ… æƒé‡æ ¼å¼éªŒè¯é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç‰ˆæœ¬ç‹¬ç«‹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”¬ è·¨ç¯å¢ƒå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("VQ-VAEæ¨¡å‹åŠ è½½", test_vqvae_loading),
        ("Transformeræ¥å£", test_transformer_interface),
        ("ç‰ˆæœ¬ç‹¬ç«‹æ€§", test_version_independence),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        result = test_func()
        results.append((test_name, result))
    
    # æ€»ç»“
    print(f"\n{'='*20} æµ‹è¯•æ€»ç»“ {'='*20}")
    all_passed = True
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰å…¼å®¹æ€§æµ‹è¯•é€šè¿‡!")
        print("âœ… VQ-VAEæ¨¡å‹å¯ä»¥åœ¨ä¸åŒç¯å¢ƒé—´å®‰å…¨ä½¿ç”¨")
        print("âœ… åˆ†é˜¶æ®µè®­ç»ƒå®Œå…¨å¯è¡Œ")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("âš ï¸ éœ€è¦æ£€æŸ¥å…¼å®¹æ€§é—®é¢˜")

if __name__ == "__main__":
    main()
