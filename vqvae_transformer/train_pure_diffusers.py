#!/usr/bin/env python3
"""
å®Œå…¨åŸºäºdiffusersæ ‡å‡†ç»„ä»¶çš„VQ-VAE + Transformerå®ç°
ä½¿ç”¨diffusers.VQModel + diffusers.Transformer2DModel
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import argparse
from tqdm import tqdm
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from diffusers import VQModel, Transformer2DModel
    from diffusers.configuration_utils import ConfigMixin, register_to_config
    from diffusers.models.modeling_utils import ModelMixin
    DIFFUSERS_AVAILABLE = True
    print("âœ… diffusersåº“å¯ç”¨")
except ImportError:
    print("âŒ diffusersåº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…æœ€æ–°ç‰ˆæœ¬: pip install diffusers")
    DIFFUSERS_AVAILABLE = False
    sys.exit(1)

from utils.data_loader import create_micro_doppler_dataset

class PureDiffusersTrainer:
    """å®Œå…¨åŸºäºdiffusersæ ‡å‡†ç»„ä»¶çš„è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ åˆå§‹åŒ–Pure Diffusersè®­ç»ƒå™¨")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åˆ›å»ºæˆ–åŠ è½½VQ-VAEæ¨¡å‹
        if args.vqvae_path and Path(args.vqvae_path).exists():
            print(f"ğŸ“¦ åŠ è½½ç°æœ‰VQ-VAE: {args.vqvae_path}")
            self.vqvae_model = VQModel.from_pretrained(args.vqvae_path)
        else:
            print("ğŸ—ï¸ åˆ›å»ºæ–°çš„diffusers VQModel")
            self.vqvae_model = VQModel(
                in_channels=3,
                out_channels=3,
                down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"],
                up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"],
                block_out_channels=[128, 256, 512],
                layers_per_block=2,
                act_fn="silu",
                latent_channels=4,
                norm_num_groups=32,
                vq_embed_dim=256,
                num_vq_embeddings=args.vocab_size,
            )
        
        self.vqvae_model.to(self.device)
        
        # ğŸ”’ å†»ç»“VQ-VAEï¼ˆå¦‚æœæ˜¯é¢„è®­ç»ƒçš„ï¼‰
        if args.vqvae_path and Path(args.vqvae_path).exists():
            print("ğŸ”’ å†»ç»“é¢„è®­ç»ƒVQ-VAE")
            self.vqvae_model.eval()
            for param in self.vqvae_model.parameters():
                param.requires_grad = False
        
        # åˆ›å»ºTransformeræ¨¡å‹
        print("ğŸ—ï¸ åˆ›å»ºdiffusers Transformer2DModel")
        self.transformer_model = Transformer2DModel(
            num_attention_heads=args.num_attention_heads,
            attention_head_dim=args.attention_head_dim,
            in_channels=args.latent_channels,
            num_layers=args.num_layers,
            dropout=args.dropout,
            norm_num_groups=32,
            cross_attention_dim=None,  # ä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
            activation_fn="gelu",
            num_embeds_ada_norm=None,
            attention_bias=True,
            only_cross_attention=False,
            double_self_attention=False,
            upcast_attention=False,
            norm_elementwise_affine=True,
            norm_eps=1e-5,
            attention_type="default",
        )
        
        self.transformer_model.to(self.device)
        
        # ç”¨æˆ·åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(args.num_users + 1, args.latent_channels)
        self.user_embedding.to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if args.vqvae_path and Path(args.vqvae_path).exists():
            # åªä¼˜åŒ–Transformerå’Œç”¨æˆ·åµŒå…¥
            params = list(self.transformer_model.parameters()) + list(self.user_embedding.parameters())
        else:
            # ä¼˜åŒ–æ‰€æœ‰å‚æ•°
            params = list(self.vqvae_model.parameters()) + list(self.transformer_model.parameters()) + list(self.user_embedding.parameters())
        
        self.optimizer = optim.AdamW(
            params,
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            betas=(0.9, 0.95)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
        print(f"âœ… Pure Diffusersè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = create_micro_doppler_dataset(
            data_dir=self.args.data_dir,
            return_user_id=True
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=self.args.num_workers,
            pin_memory=True
        )
        
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
        
        best_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            print(f"\nğŸ¯ Epoch {epoch+1}/{self.args.num_epochs}")
            
            # ç¡®ä¿VQ-VAEçŠ¶æ€æ­£ç¡®
            if self.args.vqvae_path and Path(self.args.vqvae_path).exists():
                self.vqvae_model.eval()  # é¢„è®­ç»ƒçš„ä¿æŒeval
            else:
                self.vqvae_model.train()  # æ–°å»ºçš„è¿›è¡Œè®­ç»ƒ
            
            self.transformer_model.train()
            self.user_embedding.train()
            
            total_loss = 0
            num_batches = 0
            
            pbar = tqdm(dataloader, desc=f"Training")
            
            for batch_idx, batch in enumerate(pbar):
                # å¤„ç†batchæ ¼å¼
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                    user_ids = batch['user_id'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                    images = images.to(self.device)
                    user_ids = user_ids.to(self.device)
                else:
                    continue
                
                # VQ-VAEç¼–ç 
                if self.args.vqvae_path and Path(self.args.vqvae_path).exists():
                    # é¢„è®­ç»ƒVQ-VAEï¼Œä½¿ç”¨no_grad
                    with torch.no_grad():
                        vq_output = self.vqvae_model.encode(images)
                        latents = vq_output.latents
                else:
                    # æ–°å»ºVQ-VAEï¼Œæ­£å¸¸è®­ç»ƒ
                    vq_output = self.vqvae_model.encode(images)
                    latents = vq_output.latents
                
                # æ·»åŠ ç”¨æˆ·æ¡ä»¶
                batch_size, channels, height, width = latents.shape
                user_embeds = self.user_embedding(user_ids)  # [B, C]
                user_embeds = user_embeds.unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]
                user_embeds = user_embeds.expand(-1, -1, height, width)  # [B, C, H, W]
                
                # ç»„åˆlatentså’Œç”¨æˆ·åµŒå…¥
                conditioned_latents = latents + user_embeds
                
                # Transformerå‰å‘ä¼ æ’­
                transformer_output = self.transformer_model(
                    conditioned_latents,
                    return_dict=True
                )
                
                predicted_latents = transformer_output.sample
                
                # è®¡ç®—é‡æ„æŸå¤±
                recon_loss = nn.functional.mse_loss(predicted_latents, latents)
                
                # VQæŸå¤±ï¼ˆå¦‚æœVQ-VAEåœ¨è®­ç»ƒï¼‰
                vq_loss = 0
                if not (self.args.vqvae_path and Path(self.args.vqvae_path).exists()):
                    if hasattr(vq_output, 'commit_loss'):
                        vq_loss = vq_output.commit_loss.mean()
                
                # æ€»æŸå¤±
                total_batch_loss = recon_loss + 0.25 * vq_loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_batch_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.optimizer.param_groups[0]['params'], 
                    max_norm=1.0
                )
                
                self.optimizer.step()
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += total_batch_loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'recon_loss': f'{recon_loss.item():.4f}',
                    'vq_loss': f'{vq_loss:.4f}' if isinstance(vq_loss, torch.Tensor) else f'{vq_loss:.4f}',
                    'total_loss': f'{total_batch_loss.item():.4f}'
                })
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.scheduler.get_last_lr()[0]
            
            print(f"   ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"      å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"      å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self._save_models(epoch, avg_loss, is_best=True)
                print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {avg_loss:.4f})")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_every == 0:
                self._save_models(epoch, avg_loss, is_best=False)
                print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹")
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.4f}")
    
    def _save_models(self, epoch, loss, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        if is_best:
            # ä¿å­˜VQ-VAE
            vqvae_path = self.output_dir / "vqvae_best"
            self.vqvae_model.save_pretrained(vqvae_path)
            
            # ä¿å­˜Transformer
            transformer_path = self.output_dir / "transformer_best"
            self.transformer_model.save_pretrained(transformer_path)
            
            # ä¿å­˜ç”¨æˆ·åµŒå…¥
            torch.save({
                'user_embedding': self.user_embedding.state_dict(),
                'epoch': epoch,
                'loss': loss,
                'args': self.args,
            }, self.output_dir / "user_embedding_best.pth")
        else:
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_dir = self.output_dir / f"checkpoint_epoch_{epoch+1}"
            checkpoint_dir.mkdir(exist_ok=True)
            
            self.vqvae_model.save_pretrained(checkpoint_dir / "vqvae")
            self.transformer_model.save_pretrained(checkpoint_dir / "transformer")
            
            torch.save({
                'user_embedding': self.user_embedding.state_dict(),
                'epoch': epoch,
                'loss': loss,
                'args': self.args,
            }, checkpoint_dir / "user_embedding.pth")

def main():
    parser = argparse.ArgumentParser(description="Pure Diffusers VQ-VAE + Transformerè®­ç»ƒ")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--vqvae_path", type=str, default=None, help="é¢„è®­ç»ƒVQ-VAEè·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--output_dir", type=str, default="./pure_diffusers_output", help="è¾“å‡ºç›®å½•")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--vocab_size", type=int, default=1024, help="VQç æœ¬å¤§å°")
    parser.add_argument("--latent_channels", type=int, default=4, help="æ½œåœ¨ç©ºé—´é€šé“æ•°")
    parser.add_argument("--num_users", type=int, default=31, help="ç”¨æˆ·æ•°é‡")
    parser.add_argument("--num_layers", type=int, default=8, help="Transformerå±‚æ•°")
    parser.add_argument("--num_attention_heads", type=int, default=8, help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--attention_head_dim", type=int, default=64, help="æ³¨æ„åŠ›å¤´ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropoutç‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--save_every", type=int, default=10, help="ä¿å­˜æ£€æŸ¥ç‚¹é—´éš”")
    
    args = parser.parse_args()
    
    print("ğŸš€ Pure Diffusers VQ-VAE + Transformerè®­ç»ƒ")
    print("=" * 60)
    print("ä½¿ç”¨diffusers.VQModel + diffusers.Transformer2DModel")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = PureDiffusersTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
