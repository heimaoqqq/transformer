#!/usr/bin/env python3
"""
ç¬¬äºŒæ­¥ï¼šä½¿ç”¨diffusersæ ‡å‡†Transformer2DModelè®­ç»ƒTransformer
åŸºäºé¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import argparse
from tqdm import tqdm
import sys
import matplotlib.pyplot as plt
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from diffusers import VQModel, Transformer2DModel
    DIFFUSERS_AVAILABLE = True
    print("âœ… diffusersåº“å¯ç”¨")
except ImportError:
    print("âŒ diffusersåº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…æœ€æ–°ç‰ˆæœ¬: pip install diffusers")
    DIFFUSERS_AVAILABLE = False
    sys.exit(1)

from utils.data_loader import create_micro_doppler_dataset, create_datasets_with_split

class TransformerTrainer:
    """Transformerè®­ç»ƒå™¨ - ç¬¬äºŒæ­¥"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ ç¬¬äºŒæ­¥ï¼šTransformerè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   VQ-VAEè·¯å¾„: {args.vqvae_path}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åŠ è½½é¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹
        print("ğŸ“¦ åŠ è½½é¢„è®­ç»ƒVQ-VAEæ¨¡å‹")
        self.vqvae_model = VQModel.from_pretrained(args.vqvae_path)
        self.vqvae_model.to(self.device)
        
        # ğŸ”’ å†»ç»“VQ-VAEæ¨¡å‹
        print("ğŸ”’ å†»ç»“VQ-VAEæ¨¡å‹")
        self.vqvae_model.eval()
        for param in self.vqvae_model.parameters():
            param.requires_grad = False
        print("   âœ… VQ-VAEå·²å†»ç»“ï¼Œä¸ä¼šæ›´æ–°å‚æ•°")
        
        # è·å–VQ-VAEçš„æ½œåœ¨ç©ºé—´ä¿¡æ¯
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, 128, 128).to(self.device)
            dummy_output = self.vqvae_model.encode(dummy_input)
            latent_shape = dummy_output.latents.shape
            self.latent_channels = latent_shape[1]
            self.latent_height = latent_shape[2]
            self.latent_width = latent_shape[3]
        
        print(f"   ğŸ“ æ½œåœ¨ç©ºé—´å½¢çŠ¶: {self.latent_channels}x{self.latent_height}x{self.latent_width}")

        # è®¡ç®—åˆé€‚çš„norm_num_groups
        # norm_num_groupså¿…é¡»èƒ½æ•´é™¤in_channels
        possible_groups = [1, 2, 4, 8, 16, 32]
        norm_num_groups = 1
        for groups in possible_groups:
            if self.latent_channels % groups == 0:
                norm_num_groups = groups

        print(f"   ğŸ”§ ä½¿ç”¨norm_num_groups: {norm_num_groups} (é€‚é…{self.latent_channels}é€šé“)")

        # åˆ›å»ºTransformeræ¨¡å‹
        print("ğŸ—ï¸ åˆ›å»ºdiffusers Transformer2DModel")
        self.transformer_model = Transformer2DModel(
            num_attention_heads=args.num_attention_heads,
            attention_head_dim=args.attention_head_dim,
            in_channels=self.latent_channels,
            num_layers=args.num_layers,
            dropout=args.dropout,
            norm_num_groups=norm_num_groups,  # åŠ¨æ€è®¡ç®—
            cross_attention_dim=args.cross_attention_dim,
            activation_fn="gelu",
            num_embeds_ada_norm=None,
            attention_bias=True,
            only_cross_attention=False,
            double_self_attention=False,
            upcast_attention=False,
            norm_elementwise_affine=True,
            norm_eps=1e-5,
            attention_type="default",
        )
        
        self.transformer_model.to(self.device)
        
        # ç”¨æˆ·åµŒå…¥å±‚
        print("ğŸ‘¥ åˆ›å»ºç”¨æˆ·åµŒå…¥å±‚")
        self.user_embedding = nn.Embedding(args.num_users + 1, self.latent_channels)
        self.user_embedding.to(self.device)
        
        # è¾“å‡ºæŠ•å½±å±‚ï¼ˆä»Transformerè¾“å‡ºåˆ°æ½œåœ¨ç©ºé—´ï¼‰
        self.output_projection = nn.Conv2d(
            self.latent_channels, 
            self.latent_channels, 
            kernel_size=1
        )
        self.output_projection.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        transformer_params = sum(p.numel() for p in self.transformer_model.parameters())
        user_params = sum(p.numel() for p in self.user_embedding.parameters())
        proj_params = sum(p.numel() for p in self.output_projection.parameters())
        total_trainable = transformer_params + user_params + proj_params
        
        print(f"   ğŸ“Š æ¨¡å‹å‚æ•°:")
        print(f"      Transformer: {transformer_params:,}")
        print(f"      ç”¨æˆ·åµŒå…¥: {user_params:,}")
        print(f"      è¾“å‡ºæŠ•å½±: {proj_params:,}")
        print(f"      æ€»å¯è®­ç»ƒå‚æ•°: {total_trainable:,}")
        
        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–Transformerç›¸å…³å‚æ•°ï¼‰
        trainable_params = (
            list(self.transformer_model.parameters()) + 
            list(self.user_embedding.parameters()) + 
            list(self.output_projection.parameters())
        )
        
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            betas=(0.9, 0.95)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
        print(f"âœ… Transformerè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def train(self):
        """è®­ç»ƒTransformer"""
        print(f"ğŸš€ å¼€å§‹Transformerè®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆå¸¦è‡ªåŠ¨åˆ’åˆ†ï¼‰
        if self.args.use_validation:
            train_dataset, val_dataset = create_datasets_with_split(
                data_dir=self.args.data_dir,
                train_ratio=0.8,
                val_ratio=0.2,
                return_user_id=True,  # Transformerè®­ç»ƒéœ€è¦ç”¨æˆ·ID
                random_seed=42
            )

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_dataloader = DataLoader(
                train_dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                num_workers=self.args.num_workers,
                pin_memory=True
            )

            val_dataloader = DataLoader(
                val_dataset,
                batch_size=self.args.batch_size,
                shuffle=False,
                num_workers=self.args.num_workers,
                pin_memory=True
            )

            dataloader = train_dataloader  # ä¸»è¦è®­ç»ƒç”¨
        else:
            # ä¸ä½¿ç”¨éªŒè¯é›†ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
            dataset = create_micro_doppler_dataset(
                data_dir=self.args.data_dir,
                return_user_id=True  # Transformerè®­ç»ƒéœ€è¦ç”¨æˆ·ID
            )

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = DataLoader(
                dataset,
                batch_size=self.args.batch_size,
                shuffle=True,
                num_workers=self.args.num_workers,
                pin_memory=True
            )
            val_dataloader = None
        
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
        print(f"   æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
        
        best_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            print(f"\nğŸ¯ Epoch {epoch+1}/{self.args.num_epochs}")
            
            # ç¡®ä¿VQ-VAEä¿æŒå†»ç»“çŠ¶æ€
            self.vqvae_model.eval()
            
            # Transformerè®­ç»ƒæ¨¡å¼
            self.transformer_model.train()
            self.user_embedding.train()
            self.output_projection.train()
            
            total_loss = 0
            num_batches = 0
            
            pbar = tqdm(dataloader, desc=f"Transformer Training")
            
            for batch_idx, batch in enumerate(pbar):
                # å¤„ç†batchæ ¼å¼
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                    user_ids = batch['user_id'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                    images = images.to(self.device)
                    user_ids = user_ids.to(self.device)
                else:
                    continue
                
                # ä½¿ç”¨å†»ç»“çš„VQ-VAEç¼–ç å›¾åƒ
                with torch.no_grad():
                    encoder_output = self.vqvae_model.encode(images)
                    target_latents = encoder_output.latents  # [B, C, H, W]
                
                # æ·»åŠ ç”¨æˆ·æ¡ä»¶
                batch_size = target_latents.shape[0]
                user_embeds = self.user_embedding(user_ids)  # [B, C]
                user_embeds = user_embeds.unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]
                user_embeds = user_embeds.expand(-1, -1, self.latent_height, self.latent_width)  # [B, C, H, W]
                
                # ç»„åˆç”¨æˆ·æ¡ä»¶ä½œä¸ºè¾“å…¥
                conditioned_input = user_embeds
                
                # Transformerå‰å‘ä¼ æ’­
                transformer_output = self.transformer_model(
                    conditioned_input,
                    encoder_hidden_states=None,  # ä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
                    return_dict=True
                )
                
                # è¾“å‡ºæŠ•å½±
                predicted_latents = self.output_projection(transformer_output.sample)
                
                # è®¡ç®—æŸå¤±
                loss = nn.functional.mse_loss(predicted_latents, target_latents)
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.optimizer.param_groups[0]['params'], 
                    max_norm=1.0
                )
                
                self.optimizer.step()
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{total_loss/num_batches:.4f}'
                })
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.scheduler.get_last_lr()[0]
            
            print(f"   ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"      å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"      å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self._save_models(epoch, avg_loss, is_best=True)
                print(f"   âœ… ä¿å­˜æœ€ä½³Transformeræ¨¡å‹ (æŸå¤±: {avg_loss:.4f})")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹å’Œç”Ÿæˆæ ·æœ¬
            if (epoch + 1) % self.args.save_every == 0:
                self._save_models(epoch, avg_loss, is_best=False)
                self._generate_samples(epoch, dataloader)
                print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹å’Œæ ·æœ¬")
        
        print(f"\nğŸ‰ Transformerè®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.4f}")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}")
    
    def _save_models(self, epoch, loss, is_best=False):
        """ä¿å­˜Transformeræ¨¡å‹"""
        if is_best:
            save_dir = self.output_dir / "transformer_best"
        else:
            save_dir = self.output_dir / f"transformer_epoch_{epoch+1}"
        
        save_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜Transformer
        self.transformer_model.save_pretrained(save_dir / "transformer")
        
        # ä¿å­˜å…¶ä»–ç»„ä»¶
        torch.save({
            'user_embedding': self.user_embedding.state_dict(),
            'output_projection': self.output_projection.state_dict(),
            'epoch': epoch,
            'loss': loss,
            'args': self.args,
        }, save_dir / "additional_components.pth")
    
    def _generate_samples(self, epoch, dataloader):
        """ç”Ÿæˆæ ·æœ¬"""
        self.transformer_model.eval()
        self.user_embedding.eval()
        self.output_projection.eval()
        
        with torch.no_grad():
            # é€‰æ‹©ä¸åŒçš„ç”¨æˆ·ID
            user_ids = torch.tensor([1, 8, 16, 31], device=self.device)[:4]
            
            # ç”Ÿæˆæ½œåœ¨è¡¨ç¤º
            user_embeds = self.user_embedding(user_ids)
            user_embeds = user_embeds.unsqueeze(-1).unsqueeze(-1)
            user_embeds = user_embeds.expand(-1, -1, self.latent_height, self.latent_width)
            
            # Transformerç”Ÿæˆ
            transformer_output = self.transformer_model(
                user_embeds,
                return_dict=True
            )
            
            generated_latents = self.output_projection(transformer_output.sample)
            
            # VQ-VAEè§£ç 
            decoder_output = self.vqvae_model.decode(generated_latents)
            generated_images = decoder_output.sample
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾åƒ
            self._save_generated_images(generated_images, user_ids, epoch)
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        self.transformer_model.train()
        self.user_embedding.train()
        self.output_projection.train()
    
    def _save_generated_images(self, images, user_ids, epoch):
        """ä¿å­˜ç”Ÿæˆçš„å›¾åƒ"""
        # åˆ›å»ºæ ·æœ¬ç›®å½•
        samples_dir = self.output_dir / "samples"
        samples_dir.mkdir(exist_ok=True)
        
        # è½¬æ¢ä¸ºnumpy
        images = images.cpu().numpy()
        user_ids = user_ids.cpu().numpy()
        
        # å½’ä¸€åŒ–åˆ°[0,1]
        images = (images + 1) / 2
        
        # åˆ›å»ºå›¾åƒç½‘æ ¼
        fig, axes = plt.subplots(1, 4, figsize=(16, 4))
        
        for i in range(4):
            if images.shape[1] == 3:
                axes[i].imshow(images[i].transpose(1, 2, 0))
            else:
                axes[i].imshow(images[i, 0], cmap='viridis')
            axes[i].set_title(f'User {user_ids[i]}')
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.savefig(samples_dir / f"generated_epoch_{epoch+1:03d}.png", dpi=150, bbox_inches='tight')
        plt.close()

def main():
    parser = argparse.ArgumentParser(description="ç¬¬äºŒæ­¥ï¼šè®­ç»ƒTransformer")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--vqvae_path", type=str, required=True, help="é¢„è®­ç»ƒVQ-VAEè·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./step2_transformer_output", help="è¾“å‡ºç›®å½•")
    
    # Transformeræ¨¡å‹å‚æ•°
    parser.add_argument("--num_users", type=int, default=31, help="ç”¨æˆ·æ•°é‡")
    parser.add_argument("--num_layers", type=int, default=8, help="Transformerå±‚æ•°")
    parser.add_argument("--num_attention_heads", type=int, default=8, help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--attention_head_dim", type=int, default=64, help="æ³¨æ„åŠ›å¤´ç»´åº¦")
    parser.add_argument("--cross_attention_dim", type=int, default=None, help="äº¤å‰æ³¨æ„åŠ›ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropoutç‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--save_every", type=int, default=10, help="ä¿å­˜æ£€æŸ¥ç‚¹é—´éš”")
    parser.add_argument("--use_validation", action="store_true", help="æ˜¯å¦ä½¿ç”¨éªŒè¯é›†")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--val_ratio", type=float, default=0.2, help="éªŒè¯é›†æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    print("ğŸš€ ç¬¬äºŒæ­¥ï¼šTransformerè®­ç»ƒ")
    print("=" * 60)
    print("ä½¿ç”¨diffusers.Transformer2DModelæ ‡å‡†å®ç°")
    print("åŸºäºå†»ç»“çš„é¢„è®­ç»ƒVQ-VAE")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = TransformerTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
