#!/usr/bin/env python3
"""
å¿«é€Ÿç¯å¢ƒæ£€æŸ¥è„šæœ¬
åœ¨è®­ç»ƒå‰éªŒè¯æ‰€æœ‰ä¾èµ–å’ŒAPIæ˜¯å¦æ­£ç¡®
"""

import sys
import torch
import importlib
from pathlib import Path

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    version = sys.version_info
    print(f"ğŸ Pythonç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
    
    if version.major == 3 and version.minor >= 8:
        print("âœ… Pythonç‰ˆæœ¬ç¬¦åˆè¦æ±‚")
        return True
    else:
        print("âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦3.8+")
        return False

def check_pytorch():
    """æ£€æŸ¥PyTorch"""
    try:
        import torch
        import torchvision
        import torchaudio
        
        print(f"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"   TorchVision: {torchvision.__version__}")
        print(f"   TorchAudio: {torchaudio.__version__}")
        
        # æ£€æŸ¥CUDA
        if torch.cuda.is_available():
            print(f"   CUDAå¯ç”¨: {torch.version.cuda}")
            print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"   GPU {i}: {props.name} ({props.total_memory/1024**3:.1f}GB)")
        else:
            print("   CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        
        # ç‰ˆæœ¬æ£€æŸ¥
        expected_version = "2.1.0"
        if expected_version in torch.__version__:
            print("âœ… PyTorchç‰ˆæœ¬æ­£ç¡®")
            return True
        else:
            print(f"âš ï¸ PyTorchç‰ˆæœ¬ä¸åŒ¹é…ï¼ŒæœŸæœ›{expected_version}ï¼Œå®é™…{torch.__version__}")
            return False
            
    except ImportError as e:
        print(f"âŒ PyTorchå¯¼å…¥å¤±è´¥: {e}")
        return False

def check_huggingface():
    """æ£€æŸ¥HuggingFaceç”Ÿæ€ç³»ç»Ÿ"""
    packages = {
        'diffusers': '0.25.1',
        'transformers': '4.36.2', 
        'accelerate': '0.25.0',
        'huggingface_hub': '0.19.4',
    }
    
    all_good = True
    
    for package, expected_version in packages.items():
        try:
            module = importlib.import_module(package)
            actual_version = getattr(module, '__version__', 'unknown')
            
            print(f"ğŸ¤— {package}: {actual_version}")
            
            if expected_version in actual_version:
                print(f"   âœ… ç‰ˆæœ¬æ­£ç¡®")
            else:
                print(f"   âš ï¸ ç‰ˆæœ¬ä¸åŒ¹é…ï¼ŒæœŸæœ›{expected_version}")
                all_good = False
                
        except ImportError as e:
            print(f"âŒ {package}å¯¼å…¥å¤±è´¥: {e}")
            all_good = False
    
    return all_good

def check_vq_vae_api():
    """æ£€æŸ¥VQ-VAE API - å°è¯•ä¸åŒç‰ˆæœ¬çš„å¯¼å…¥è·¯å¾„"""
    print("\nğŸ§ª æµ‹è¯•VQ-VAE API...")

    VQModel = None

    # å°è¯•ä¸åŒç‰ˆæœ¬çš„APIè·¯å¾„
    try:
        # æµ‹è¯•æ–°çš„APIè·¯å¾„
        from diffusers.models.autoencoders.vq_model import VQModel
        print("âœ… VQModelå¯¼å…¥æˆåŠŸ (æ–°ç‰ˆAPI)")
    except ImportError:
        try:
            # æµ‹è¯•æ—§ç‰ˆAPIè·¯å¾„
            from diffusers.models.vq_model import VQModel
            print("âœ… VQModelå¯¼å…¥æˆåŠŸ (æ—§ç‰ˆAPI)")
        except ImportError:
            try:
                # æµ‹è¯•ç›´æ¥å¯¼å…¥
                from diffusers import VQModel
                print("âœ… VQModelå¯¼å…¥æˆåŠŸ (ç›´æ¥å¯¼å…¥)")
            except ImportError:
                print("âŒ VQModel: æ‰€æœ‰å¯¼å…¥è·¯å¾„éƒ½å¤±è´¥")
                print("   å»ºè®®è¿è¡Œ: python setup_environment.py")
                return False

    if VQModel is not None:
        try:
            # æµ‹è¯•åˆ›å»ºæ¨¡å‹ - ä½¿ç”¨æ›´ç®€å•çš„é…ç½®
            model = VQModel(
                in_channels=3,
                out_channels=3,
                down_block_types=["DownEncoderBlock2D"],
                up_block_types=["UpDecoderBlock2D"],
                block_out_channels=[128],
                layers_per_block=1,
                latent_channels=128,
                sample_size=32,
                num_vq_embeddings=256,
                norm_num_groups=32,
                vq_embed_dim=128,
            )
            print("âœ… VQModelåˆ›å»ºæˆåŠŸ")

            # æµ‹è¯•å‰å‘ä¼ æ’­
            test_input = torch.randn(1, 3, 32, 32)
            with torch.no_grad():
                result = model.encode(test_input)
                print(f"âœ… VQModelç¼–ç æˆåŠŸ: {result.latents.shape}")

                decoded = model.decode(result.latents)
                print(f"âœ… VQModelè§£ç æˆåŠŸ: {decoded.sample.shape}")

            return True

        except Exception as e:
            print(f"âŒ VQModelåˆ›å»º/æµ‹è¯•å¤±è´¥: {e}")
            print("âš ï¸ VQModelå¯¼å…¥æˆåŠŸä½†åˆ›å»ºå¤±è´¥ï¼Œå¯èƒ½æ˜¯å‚æ•°é—®é¢˜")
            print("   å»ºè®®è¿è¡Œ: python setup_environment.py")
            return True  # å¯¼å…¥æˆåŠŸå°±ç®—åŸºæœ¬é€šè¿‡

    return False

def check_transformer_api():
    """æ£€æŸ¥Transformer API"""
    print("\nğŸ¤– æµ‹è¯•Transformer API...")
    
    try:
        from transformers import GPT2Config, GPT2LMHeadModel
        print("âœ… GPT2å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åˆ›å»ºæ¨¡å‹
        config = GPT2Config(
            vocab_size=1024,
            n_positions=256,
            n_embd=512,
            n_layer=4,
            n_head=8,
            use_cache=False,
        )
        
        model = GPT2LMHeadModel(config)
        print("âœ… GPT2æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_input = torch.randint(0, 1024, (1, 32))
        with torch.no_grad():
            output = model(test_input)
            print(f"âœ… GPT2å‰å‘ä¼ æ’­æˆåŠŸ: {output.logits.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Transformer APIæµ‹è¯•å¤±è´¥: {e}")
        return False

def check_other_dependencies():
    """æ£€æŸ¥å…¶ä»–ä¾èµ–"""
    print("\nğŸ“¦ æ£€æŸ¥å…¶ä»–ä¾èµ–...")
    
    dependencies = [
        'numpy', 'PIL', 'cv2', 'matplotlib', 
        'sklearn', 'scipy', 'einops', 'tqdm'
    ]
    
    all_good = True
    
    for dep in dependencies:
        try:
            if dep == 'PIL':
                import PIL
                print(f"âœ… Pillow: {PIL.__version__}")
            elif dep == 'cv2':
                import cv2
                print(f"âœ… OpenCV: {cv2.__version__}")
            elif dep == 'sklearn':
                import sklearn
                print(f"âœ… Scikit-learn: {sklearn.__version__}")
            else:
                module = importlib.import_module(dep)
                version = getattr(module, '__version__', 'unknown')
                print(f"âœ… {dep}: {version}")
                
        except ImportError as e:
            print(f"âŒ {dep}å¯¼å…¥å¤±è´¥: {e}")
            all_good = False
    
    return all_good

def estimate_memory_requirements():
    """ä¼°ç®—å†…å­˜éœ€æ±‚"""
    print("\nğŸ’¾ å†…å­˜éœ€æ±‚ä¼°ç®—...")
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            
            print(f"GPU {i} ({props.name}): {memory_gb:.1f}GB")
            
            if memory_gb >= 16:
                print("   âœ… å†…å­˜å……è¶³ï¼Œå¯ä»¥ä½¿ç”¨å¤§æ‰¹æ¬¡è®­ç»ƒ")
                print("   æ¨èé…ç½®: VQ-VAE batch_size=16, Transformer batch_size=8")
            elif memory_gb >= 8:
                print("   âœ… å†…å­˜è¶³å¤Ÿï¼Œä½¿ç”¨ä¸­ç­‰æ‰¹æ¬¡è®­ç»ƒ")
                print("   æ¨èé…ç½®: VQ-VAE batch_size=12, Transformer batch_size=6")
            else:
                print("   âš ï¸ å†…å­˜è¾ƒå°‘ï¼Œéœ€è¦å°æ‰¹æ¬¡è®­ç»ƒ")
                print("   æ¨èé…ç½®: VQ-VAE batch_size=8, Transformer batch_size=4")
    else:
        print("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” VQ-VAE + Transformer ç¯å¢ƒæ£€æŸ¥")
    print("=" * 50)
    
    checks = [
        ("Pythonç‰ˆæœ¬", check_python_version),
        ("PyTorch", check_pytorch), 
        ("HuggingFaceç”Ÿæ€", check_huggingface),
        ("VQ-VAE API", check_vq_vae_api),
        ("Transformer API", check_transformer_api),
        ("å…¶ä»–ä¾èµ–", check_other_dependencies),
    ]
    
    results = []
    
    for name, check_func in checks:
        print(f"\n{'='*20} {name} {'='*20}")
        result = check_func()
        results.append((name, result))
    
    # å†…å­˜éœ€æ±‚ä¼°ç®—
    print(f"\n{'='*20} å†…å­˜éœ€æ±‚ {'='*20}")
    estimate_memory_requirements()
    
    # æ€»ç»“
    print(f"\n{'='*20} æ£€æŸ¥æ€»ç»“ {'='*20}")
    
    all_passed = True
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç¯å¢ƒé…ç½®æ­£ç¡®")
        print("ğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒ:")
        print("   python train_main.py --data_dir /path/to/data")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œå»ºè®®:")
        print("1. è¿è¡Œç¯å¢ƒå®‰è£…è„šæœ¬: python setup_environment.py")
        print("2. æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶æ‰‹åŠ¨ä¿®å¤")
        print("3. é‡æ–°è¿è¡Œæ­¤æ£€æŸ¥è„šæœ¬")

if __name__ == "__main__":
    main()
