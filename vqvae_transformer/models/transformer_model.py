#!/usr/bin/env python3
"""
æ¡ä»¶Transformeræ¨¡å‹å®ç°
ç”¨äºä»ç”¨æˆ·IDç”ŸæˆVQ-VAE tokenåºåˆ—
é’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„ç”¨æˆ·ç‰¹å¾ç”Ÿæˆä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, Dict
from transformers import GPT2Config, GPT2LMHeadModel
from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]

class UserConditionEncoder(nn.Module):
    """ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨"""
    
    def __init__(
        self,
        num_users: int = 31,
        embed_dim: int = 512,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        self.num_users = num_users
        self.embed_dim = embed_dim
        
        # ç”¨æˆ·IDåµŒå…¥
        self.user_embedding = nn.Embedding(num_users, embed_dim)
        
        # å¯å­¦ä¹ çš„ç”¨æˆ·ç‰¹å¾å¢å¼º
        self.user_mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.Dropout(dropout),
        )
        
        # åˆå§‹åŒ–
        nn.init.normal_(self.user_embedding.weight, std=0.02)
    
    def forward(self, user_ids: torch.Tensor) -> torch.Tensor:
        """
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
        Returns:
            user_embeds: [batch_size, embed_dim] ç”¨æˆ·åµŒå…¥
        """
        user_embeds = self.user_embedding(user_ids)
        user_embeds = self.user_mlp(user_embeds)
        return user_embeds

class MicroDopplerTransformer(nn.Module):
    """
    å¾®å¤šæ™®å‹’æ¡ä»¶Transformer
    åŸºäºGPTæ¶æ„ï¼Œæ”¯æŒç”¨æˆ·IDæ¡ä»¶ç”Ÿæˆ
    """
    
    def __init__(
        self,
        vocab_size: int = 1024,          # VQç æœ¬å¤§å°
        max_seq_len: int = 256,          # æœ€å¤§åºåˆ—é•¿åº¦ (16x16 = 256)
        num_users: int = 31,             # ç”¨æˆ·æ•°é‡
        n_embd: int = 512,               # åµŒå…¥ç»´åº¦
        n_layer: int = 8,                # Transformerå±‚æ•°
        n_head: int = 8,                 # æ³¨æ„åŠ›å¤´æ•°
        dropout: float = 0.1,            # Dropoutç‡
        use_cross_attention: bool = True, # æ˜¯å¦ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len
        self.num_users = num_users
        self.n_embd = n_embd
        self.use_cross_attention = use_cross_attention
        
        # ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨
        self.user_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=n_embd,
            dropout=dropout,
        )
        
        # é…ç½®GPTæ¨¡å‹
        config = GPT2Config(
            vocab_size=vocab_size + 1,  # +1 for special tokens
            n_positions=max_seq_len + 1,  # +1 for user token
            n_embd=n_embd,
            n_layer=n_layer,
            n_head=n_head,
            resid_pdrop=dropout,
            embd_pdrop=dropout,
            attn_pdrop=dropout,
            use_cache=False,
            add_cross_attention=use_cross_attention,
        )
        
        # åˆ›å»ºGPTæ¨¡å‹
        self.transformer = GPT2LMHeadModel(config)
        
        # ç‰¹æ®Štoken
        self.user_token_id = vocab_size  # ç”¨æˆ·token ID
        self.pad_token_id = vocab_size   # padding token
        
        # å¦‚æœä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œéœ€è¦æŠ•å½±å±‚
        if use_cross_attention:
            self.user_proj = nn.Linear(n_embd, n_embd)
        
        print(f"ğŸ¤– å¾®å¤šæ™®å‹’Transformeråˆå§‹åŒ–:")
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
        print(f"   åºåˆ—é•¿åº¦: {max_seq_len}")
        print(f"   ç”¨æˆ·æ•°é‡: {num_users}")
        print(f"   åµŒå…¥ç»´åº¦: {n_embd}")
        print(f"   Transformerå±‚æ•°: {n_layer}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_head}")
        print(f"   äº¤å‰æ³¨æ„åŠ›: {use_cross_attention}")
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   æ€»å‚æ•°é‡: {total_params/1e6:.1f}M")
    
    def prepare_inputs(
        self, 
        user_ids: torch.Tensor, 
        token_sequences: Optional[torch.Tensor] = None,
        max_length: Optional[int] = None,
    ) -> Dict[str, torch.Tensor]:
        """
        å‡†å¤‡æ¨¡å‹è¾“å…¥
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
            token_sequences: [batch_size, seq_len] tokenåºåˆ— (è®­ç»ƒæ—¶æä¾›)
            max_length: ç”Ÿæˆæ—¶çš„æœ€å¤§é•¿åº¦
        """
        batch_size = user_ids.size(0)
        device = user_ids.device
        
        if token_sequences is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šæ„é€ è¾“å…¥åºåˆ—
            seq_len = token_sequences.size(1)
            
            # è¾“å…¥åºåˆ—ï¼š[user_token] + [token1, token2, ..., token_n-1]
            user_tokens = torch.full((batch_size, 1), self.user_token_id, device=device)
            input_ids = torch.cat([user_tokens, token_sequences[:, :-1]], dim=1)
            
            # ç›®æ ‡åºåˆ—ï¼š[user_token] + [token1, token2, ..., token_n]
            labels = torch.cat([user_tokens, token_sequences], dim=1)
            
            # æ³¨æ„åŠ›æ©ç 
            attention_mask = torch.ones_like(input_ids)
            
        else:
            # ç”Ÿæˆæ¨¡å¼ï¼šåªæœ‰ç”¨æˆ·token
            max_length = max_length or self.max_seq_len
            input_ids = torch.full((batch_size, 1), self.user_token_id, device=device)
            labels = None
            attention_mask = torch.ones_like(input_ids)
        
        # ç”¨æˆ·æ¡ä»¶ç¼–ç 
        user_embeds = self.user_encoder(user_ids)  # [batch_size, n_embd]
        
        if self.use_cross_attention:
            # äº¤å‰æ³¨æ„åŠ›æ¨¡å¼ï¼šç”¨æˆ·åµŒå…¥ä½œä¸ºencoder_hidden_states
            encoder_hidden_states = self.user_proj(user_embeds).unsqueeze(1)  # [batch_size, 1, n_embd]
            encoder_attention_mask = torch.ones(batch_size, 1, device=device)
        else:
            # è‡ªæ³¨æ„åŠ›æ¨¡å¼ï¼šç”¨æˆ·åµŒå…¥æ›¿æ¢ç”¨æˆ·tokençš„åµŒå…¥
            encoder_hidden_states = None
            encoder_attention_mask = None
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'encoder_hidden_states': encoder_hidden_states,
            'encoder_attention_mask': encoder_attention_mask,
            'user_embeds': user_embeds,
        }
    
    def forward(
        self,
        user_ids: torch.Tensor,
        token_sequences: Optional[torch.Tensor] = None,
        return_dict: bool = True,
    ):
        """
        å‰å‘ä¼ æ’­
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
            token_sequences: [batch_size, seq_len] tokenåºåˆ—
        """
        inputs = self.prepare_inputs(user_ids, token_sequences)
        
        # å¦‚æœä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œéœ€è¦æ‰‹åŠ¨æ›¿æ¢ç”¨æˆ·tokençš„åµŒå…¥
        if not self.use_cross_attention:
            # è·å–tokenåµŒå…¥
            inputs_embeds = self.transformer.transformer.wte(inputs['input_ids'])
            
            # æ›¿æ¢ç”¨æˆ·tokenä½ç½®çš„åµŒå…¥
            user_positions = (inputs['input_ids'] == self.user_token_id)
            inputs_embeds[user_positions] = inputs['user_embeds'].unsqueeze(1).expand(-1, user_positions.sum(1).max(), -1)[user_positions]
            
            # ä½¿ç”¨åµŒå…¥è€Œä¸æ˜¯token ID
            outputs = self.transformer(
                inputs_embeds=inputs_embeds,
                attention_mask=inputs['attention_mask'],
                labels=inputs['labels'],
                return_dict=return_dict,
            )
        else:
            # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
            outputs = self.transformer(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                encoder_hidden_states=inputs['encoder_hidden_states'],
                encoder_attention_mask=inputs['encoder_attention_mask'],
                labels=inputs['labels'],
                return_dict=return_dict,
            )
        
        return outputs

    @torch.no_grad()
    def generate(
        self,
        user_ids: torch.Tensor,
        max_length: int = 256,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        do_sample: bool = True,
        num_return_sequences: int = 1,
    ) -> torch.Tensor:
        """
        ç”Ÿæˆtokenåºåˆ—
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            do_sample: æ˜¯å¦é‡‡æ ·
            num_return_sequences: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„åºåˆ—æ•°
        Returns:
            generated_sequences: [batch_size * num_return_sequences, seq_len]
        """
        batch_size = user_ids.size(0)
        device = user_ids.device

        # æ‰©å±•ç”¨æˆ·IDä»¥æ”¯æŒå¤šåºåˆ—ç”Ÿæˆ
        if num_return_sequences > 1:
            user_ids = user_ids.unsqueeze(1).expand(-1, num_return_sequences).flatten()
            batch_size = user_ids.size(0)

        # å‡†å¤‡åˆå§‹è¾“å…¥
        inputs = self.prepare_inputs(user_ids, max_length=max_length)
        input_ids = inputs['input_ids']  # [batch_size, 1] åªæœ‰ç”¨æˆ·token

        # ç”Ÿæˆå¾ªç¯
        for _ in range(max_length):
            # å‰å‘ä¼ æ’­
            if not self.use_cross_attention:
                # æ‰‹åŠ¨å¤„ç†ç”¨æˆ·åµŒå…¥
                inputs_embeds = self.transformer.transformer.wte(input_ids)
                user_positions = (input_ids == self.user_token_id)
                if user_positions.any():
                    user_embeds = self.user_encoder(user_ids % self.num_users)  # å¤„ç†æ‰©å±•çš„user_ids
                    inputs_embeds[user_positions] = user_embeds.unsqueeze(1).expand(-1, user_positions.sum(1).max(), -1)[user_positions]

                outputs = self.transformer(
                    inputs_embeds=inputs_embeds,
                    attention_mask=inputs['attention_mask'],
                    return_dict=True,
                )
            else:
                outputs = self.transformer(
                    input_ids=input_ids,
                    attention_mask=inputs['attention_mask'],
                    encoder_hidden_states=inputs['encoder_hidden_states'],
                    encoder_attention_mask=inputs['encoder_attention_mask'],
                    return_dict=True,
                )

            # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
            next_token_logits = outputs.logits[:, -1, :] / temperature

            # è¿‡æ»¤è¯æ±‡è¡¨ï¼Œåªä¿ç•™æœ‰æ•ˆçš„VQ token
            next_token_logits[:, self.vocab_size:] = -float('inf')  # å±è”½ç‰¹æ®Štoken

            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            if do_sample:
                # Top-ké‡‡æ ·
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = -float('inf')

                # Top-p (nucleus)é‡‡æ ·
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    next_token_logits[indices_to_remove] = -float('inf')

                # é‡‡æ ·
                probs = F.softmax(next_token_logits, dim=-1)
                next_tokens = torch.multinomial(probs, num_samples=1)
            else:
                # è´ªå¿ƒè§£ç 
                next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)

            # æ·»åŠ åˆ°åºåˆ—
            input_ids = torch.cat([input_ids, next_tokens], dim=-1)

            # æ›´æ–°æ³¨æ„åŠ›æ©ç 
            inputs['attention_mask'] = torch.cat([
                inputs['attention_mask'],
                torch.ones(batch_size, 1, device=device)
            ], dim=-1)

        # ç§»é™¤ç”¨æˆ·tokenï¼Œåªè¿”å›ç”Ÿæˆçš„VQ tokenåºåˆ—
        generated_sequences = input_ids[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªç”¨æˆ·token

        return generated_sequences
