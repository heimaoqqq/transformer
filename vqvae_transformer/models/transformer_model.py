#!/usr/bin/env python3
"""
æ¡ä»¶Transformeræ¨¡å‹å®ç°
ç”¨äºä»ç”¨æˆ·IDç”ŸæˆVQ-VAE tokenåºåˆ—
é’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„ç”¨æˆ·ç‰¹å¾ç”Ÿæˆä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, Dict

# æ¡ä»¶å¯¼å…¥transformers - åœ¨VQ-VAEç¯å¢ƒä¸­å¯èƒ½ä¸å¯ç”¨
try:
    from transformers import GPT2Config, GPT2LMHeadModel
    from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("âš ï¸ transformersä¸å¯ç”¨ï¼ŒTransformeræ¨¡å‹å°†ä¸å¯ç”¨")
    TRANSFORMERS_AVAILABLE = False
    # åˆ›å»ºå…¼å®¹çš„åŸºç±»
    class GPT2Config:
        def __init__(self, *args, **kwargs):
            pass

    class GPT2LMHeadModel(nn.Module):
        def __init__(self, *args, **kwargs):
            super().__init__()

    class CausalLMOutputWithCrossAttentions:
        def __init__(self, *args, **kwargs):
            pass

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]

class UserConditionEncoder(nn.Module):
    """ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨"""
    
    def __init__(
        self,
        num_users: int = 31,
        embed_dim: int = 512,
        dropout: float = 0.1,
    ):
        super().__init__()
        
        self.num_users = num_users
        self.embed_dim = embed_dim
        
        # ç”¨æˆ·IDåµŒå…¥ - æ”¯æŒç”¨æˆ·IDä»1å¼€å§‹
        self.user_embedding = nn.Embedding(num_users + 1, embed_dim)
        
        # å¢å¼ºçš„ç”¨æˆ·ç‰¹å¾å­¦ä¹ ç½‘ç»œ - ä¸“ä¸ºå¾®å°å·®å¼‚è®¾è®¡
        self.user_mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),  # æ›´å¤§çš„éšè—å±‚
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 4, embed_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.LayerNorm(embed_dim),  # æ·»åŠ LayerNormç¨³å®šè®­ç»ƒ
        )

        # ç”¨æˆ·ç‰¹å¾å¤šå¤´æ³¨æ„åŠ› - å¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›
        self.user_self_attention = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # åˆå§‹åŒ–
        nn.init.normal_(self.user_embedding.weight, std=0.02)
    
    def forward(self, user_ids: torch.Tensor) -> torch.Tensor:
        """
        å¢å¼ºçš„ç”¨æˆ·ç‰¹å¾ç¼–ç 
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
        Returns:
            user_embeds: [batch_size, embed_dim] å¢å¼ºçš„ç”¨æˆ·åµŒå…¥
        """
        # åŸºç¡€ç”¨æˆ·åµŒå…¥
        user_embeds = self.user_embedding(user_ids)  # [B, embed_dim]

        # é€šè¿‡MLPå¢å¼ºç‰¹å¾
        enhanced_embeds = self.user_mlp(user_embeds)  # [B, embed_dim]

        # è‡ªæ³¨æ„åŠ›è¿›ä¸€æ­¥å¢å¼ºç”¨æˆ·ç‰¹å¾è¡¨è¾¾
        # ä¸ºäº†ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬éœ€è¦åºåˆ—ç»´åº¦
        user_seq = enhanced_embeds.unsqueeze(1)  # [B, 1, embed_dim]
        attended_embeds, _ = self.user_self_attention(
            user_seq, user_seq, user_seq
        )  # [B, 1, embed_dim]

        # æ®‹å·®è¿æ¥
        final_embeds = enhanced_embeds + attended_embeds.squeeze(1)

        return final_embeds

class MicroDopplerTransformer(nn.Module):
    """
    å¾®å¤šæ™®å‹’æ¡ä»¶Transformer
    åŸºäºGPTæ¶æ„ï¼Œæ”¯æŒç”¨æˆ·IDæ¡ä»¶ç”Ÿæˆ
    """
    
    def __init__(
        self,
        vocab_size: int = 1024,          # VQç æœ¬å¤§å°
        max_seq_len: int = 256,          # æœ€å¤§åºåˆ—é•¿åº¦ (16x16 = 256)
        num_users: int = 31,             # ç”¨æˆ·æ•°é‡
        n_embd: int = 512,               # åµŒå…¥ç»´åº¦
        n_layer: int = 8,                # Transformerå±‚æ•°
        n_head: int = 8,                 # æ³¨æ„åŠ›å¤´æ•°
        dropout: float = 0.1,            # Dropoutç‡
        use_cross_attention: bool = True, # æ˜¯å¦ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
    ):
        super().__init__()
        
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len
        self.num_users = num_users
        self.n_embd = n_embd
        self.use_cross_attention = use_cross_attention

        # ç¡®ä¿åœ¨ä½¿ç”¨å‰è®¾ç½®æ‰©å±•å› å­
        self.user_expansion_factor = 4 if use_cross_attention else 1
        
        # ç”¨æˆ·æ¡ä»¶ç¼–ç å™¨
        self.user_encoder = UserConditionEncoder(
            num_users=num_users,
            embed_dim=n_embd,
            dropout=dropout,
        )
        
        # é…ç½®è‡ªå®šä¹‰GPTæ¨¡å‹ - ä¸“ä¸ºVQ-VAEè§†è§‰tokenä¼˜åŒ–
        config = GPT2Config(
            vocab_size=vocab_size + 1,  # VQ-VAEç æœ¬å¤§å°(1024) + 1ä¸ªç‰¹æ®Štoken
            n_positions=max_seq_len + 1,  # åºåˆ—é•¿åº¦(1024) + 1ä¸ªç”¨æˆ·token
            n_embd=n_embd,  # åµŒå…¥ç»´åº¦(512)
            n_layer=n_layer,  # Transformerå±‚æ•°(8)
            n_head=n_head,  # æ³¨æ„åŠ›å¤´æ•°(8)
            n_inner=n_embd * 4,  # FFNå†…éƒ¨ç»´åº¦(2048)
            activation_function="gelu_new",  # ä½¿ç”¨æ–°ç‰ˆGELU
            resid_pdrop=dropout,  # æ®‹å·®è¿æ¥dropout
            embd_pdrop=dropout,   # åµŒå…¥å±‚dropout
            attn_pdrop=dropout,   # æ³¨æ„åŠ›dropout
            layer_norm_epsilon=1e-5,  # LayerNorm epsilon
            initializer_range=0.02,   # æƒé‡åˆå§‹åŒ–èŒƒå›´
            use_cache=False,  # è®­ç»ƒæ—¶ä¸ä½¿ç”¨ç¼“å­˜
            add_cross_attention=use_cross_attention,  # æ˜¯å¦æ·»åŠ äº¤å‰æ³¨æ„åŠ›
            # ç¡®ä¿ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡
            _name_or_path="",
        )
        
        # åˆ›å»ºè‡ªå®šä¹‰GPTæ¨¡å‹ï¼ˆä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰
        self.transformer = GPT2LMHeadModel(config)

        # é‡æ–°åˆå§‹åŒ–æƒé‡ä»¥ç¡®ä¿é€‚åˆè§†è§‰token
        self._init_weights()
        
        # ç‰¹æ®Štoken
        self.user_token_id = vocab_size  # ç”¨æˆ·token ID
        self.pad_token_id = vocab_size   # padding token
        
        # å¢å¼ºçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶
        if use_cross_attention:
            # å¤šå±‚ç”¨æˆ·ç‰¹å¾æŠ•å½±ï¼Œå¢å¼ºç”¨æˆ·ä¿¡æ¯è¡¨è¾¾
            self.user_proj = nn.Sequential(
                nn.Linear(n_embd, n_embd * 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(n_embd * 2, n_embd),
                nn.LayerNorm(n_embd),
            )

            # ç”¨æˆ·ç‰¹å¾æ‰©å±• - ä»1ä¸ªtokenæ‰©å±•åˆ°å¤šä¸ªtokenå¢å¼ºè¡¨è¾¾èƒ½åŠ›
            self.user_expansion_factor = 4  # æ‰©å±•ä¸º4ä¸ªtoken
            self.user_expand = nn.Linear(n_embd, n_embd * self.user_expansion_factor)
        
        print(f"ğŸ¤– å¾®å¤šæ™®å‹’Transformeråˆå§‹åŒ–:")
        print(f"   æ¨¡å‹ç±»å‹: è‡ªå®šä¹‰GPT2 (ä¸“ä¸ºè§†è§‰tokenä¼˜åŒ–)")
        print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size} + 1ä¸ªç‰¹æ®Štoken")
        print(f"   åºåˆ—é•¿åº¦: {max_seq_len} (32Ã—32 token map) + 1ä¸ªç”¨æˆ·token")
        print(f"   ç”¨æˆ·æ•°é‡: {num_users}")
        print(f"   åµŒå…¥ç»´åº¦: {n_embd}")
        print(f"   Transformerå±‚æ•°: {n_layer}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {n_head}")
        print(f"   äº¤å‰æ³¨æ„åŠ›: {use_cross_attention}")
        print(f"   é¢„è®­ç»ƒæƒé‡: ä¸ä½¿ç”¨ (ä»å¤´è®­ç»ƒ)")
        
        # è¯¦ç»†çš„å‚æ•°é‡ç»Ÿè®¡
        user_encoder_params = sum(p.numel() for p in self.user_encoder.parameters())
        transformer_params = sum(p.numel() for p in self.transformer.parameters())
        if use_cross_attention:
            user_proj_params = sum(p.numel() for p in self.user_proj.parameters())
            user_expand_params = sum(p.numel() for p in self.user_expand.parameters())
        else:
            user_proj_params = 0
            user_expand_params = 0

        total_params = sum(p.numel() for p in self.parameters())

        print(f"   ğŸ“Š å‚æ•°é‡è¯¦ç»†ç»Ÿè®¡:")
        print(f"      ç”¨æˆ·ç¼–ç å™¨: {user_encoder_params/1e6:.2f}M")
        print(f"      Transformerä¸»ä½“: {transformer_params/1e6:.2f}M")
        print(f"      ç”¨æˆ·æŠ•å½±å±‚: {user_proj_params/1e6:.2f}M")
        print(f"      ç”¨æˆ·æ‰©å±•å±‚: {user_expand_params/1e6:.2f}M")
        print(f"      æ€»å‚æ•°é‡: {total_params/1e6:.1f}M")

    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡ - ä¸“ä¸ºè§†è§‰tokenä¼˜åŒ–"""
        def _init_module(module):
            if isinstance(module, nn.Linear):
                # çº¿æ€§å±‚ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                # åµŒå…¥å±‚ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            elif isinstance(module, nn.LayerNorm):
                # LayerNormä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–
                torch.nn.init.zeros_(module.bias)
                torch.nn.init.ones_(module.weight)

        # åº”ç”¨åˆå§‹åŒ–åˆ°æ‰€æœ‰æ¨¡å—
        self.apply(_init_module)
        print("âœ… æ¨¡å‹æƒé‡å·²é‡æ–°åˆå§‹åŒ–ï¼ˆä¸“ä¸ºè§†è§‰tokenä¼˜åŒ–ï¼‰")

        # éªŒè¯å¢å¼ºåŠŸèƒ½æ˜¯å¦æ­£ç¡®å¯ç”¨
        self._verify_enhancements()

    def _verify_enhancements(self):
        """éªŒè¯å¢å¼ºåŠŸèƒ½æ˜¯å¦æ­£ç¡®å¯ç”¨"""
        print(f"ğŸ” éªŒè¯æ¨¡å‹å¢å¼ºåŠŸèƒ½:")

        # æ£€æŸ¥ç”¨æˆ·ç¼–ç å™¨
        user_mlp_layers = len(self.user_encoder.user_mlp)
        print(f"   ç”¨æˆ·MLPå±‚æ•°: {user_mlp_layers} (åº”è¯¥>6)")

        # æ£€æŸ¥è‡ªæ³¨æ„åŠ›
        has_self_attention = hasattr(self.user_encoder, 'user_self_attention')
        print(f"   ç”¨æˆ·è‡ªæ³¨æ„åŠ›: {'âœ…å¯ç”¨' if has_self_attention else 'âŒæœªå¯ç”¨'}")

        # æ£€æŸ¥äº¤å‰æ³¨æ„åŠ›å¢å¼º
        if self.use_cross_attention:
            has_user_proj = hasattr(self, 'user_proj')
            has_user_expand = hasattr(self, 'user_expand')
            print(f"   å¢å¼ºç”¨æˆ·æŠ•å½±: {'âœ…å¯ç”¨' if has_user_proj else 'âŒæœªå¯ç”¨'}")
            print(f"   ç”¨æˆ·ç‰¹å¾æ‰©å±•: {'âœ…å¯ç”¨' if has_user_expand else 'âŒæœªå¯ç”¨'}")
            print(f"   æ‰©å±•å› å­: {self.user_expansion_factor}")

        # æ£€æŸ¥GPT2äº¤å‰æ³¨æ„åŠ›
        gpt2_has_cross_attn = self.transformer.config.add_cross_attention
        print(f"   GPT2äº¤å‰æ³¨æ„åŠ›: {'âœ…å¯ç”¨' if gpt2_has_cross_attn else 'âŒæœªå¯ç”¨'}")
    
    def prepare_inputs(
        self, 
        user_ids: torch.Tensor, 
        token_sequences: Optional[torch.Tensor] = None,
        max_length: Optional[int] = None,
    ) -> Dict[str, torch.Tensor]:
        """
        å‡†å¤‡æ¨¡å‹è¾“å…¥
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
            token_sequences: [batch_size, seq_len] tokenåºåˆ— (è®­ç»ƒæ—¶æä¾›)
            max_length: ç”Ÿæˆæ—¶çš„æœ€å¤§é•¿åº¦
        """
        batch_size = user_ids.size(0)
        device = user_ids.device
        
        if token_sequences is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šæ„é€ è¾“å…¥åºåˆ—
            seq_len = token_sequences.size(1)
            
            # è‡ªå›å½’è®­ç»ƒï¼šç”¨æˆ·tokené¢„æµ‹ç¬¬ä¸€ä¸ªå›¾åƒtokenï¼Œæ¯ä¸ªå›¾åƒtokené¢„æµ‹ä¸‹ä¸€ä¸ª
            user_tokens = torch.full((batch_size, 1), self.user_token_id, device=device)

            # è¾“å…¥åºåˆ—ï¼š[user_token] + [token1, token2, ..., token_n-1]
            input_ids = torch.cat([user_tokens, token_sequences[:, :-1]], dim=1)  # [B, 1024]

            # ç›®æ ‡åºåˆ—ï¼š[token1] + [token2, token3, ..., token_n] (æ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªtoken)
            # ç”¨æˆ·tokenä½ç½®é¢„æµ‹token1ï¼Œtoken1ä½ç½®é¢„æµ‹token2ï¼Œ...ï¼Œtoken_n-1ä½ç½®é¢„æµ‹token_n
            labels = token_sequences  # [B, 1024]
            
            # æ³¨æ„åŠ›æ©ç 
            attention_mask = torch.ones_like(input_ids)
            
        else:
            # ç”Ÿæˆæ¨¡å¼ï¼šåªæœ‰ç”¨æˆ·token
            max_length = max_length or self.max_seq_len
            input_ids = torch.full((batch_size, 1), self.user_token_id, device=device)
            labels = None
            attention_mask = torch.ones_like(input_ids)
        
        # ç”¨æˆ·æ¡ä»¶ç¼–ç 
        user_embeds = self.user_encoder(user_ids)  # [batch_size, n_embd]
        
        if self.use_cross_attention:
            # å¢å¼ºçš„äº¤å‰æ³¨æ„åŠ›æ¨¡å¼ï¼šæ‰©å±•ç”¨æˆ·ç‰¹å¾è¡¨è¾¾
            projected_user_embeds = self.user_proj(user_embeds)  # [B, n_embd]

            # æ‰©å±•ç”¨æˆ·ç‰¹å¾ä¸ºå¤šä¸ªtokenä»¥å¢å¼ºè¡¨è¾¾èƒ½åŠ›
            expanded_user_features = self.user_expand(projected_user_embeds)  # [B, n_embd * 4]
            expanded_user_features = expanded_user_features.view(
                batch_size, self.user_expansion_factor, self.n_embd
            )  # [B, 4, n_embd]

            encoder_hidden_states = expanded_user_features
            encoder_attention_mask = torch.ones(batch_size, self.user_expansion_factor, device=device)
        else:
            # è‡ªæ³¨æ„åŠ›æ¨¡å¼ï¼šç”¨æˆ·åµŒå…¥æ›¿æ¢ç”¨æˆ·tokençš„åµŒå…¥
            encoder_hidden_states = None
            encoder_attention_mask = None
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'encoder_hidden_states': encoder_hidden_states,
            'encoder_attention_mask': encoder_attention_mask,
            'user_embeds': user_embeds,
        }
    
    def forward(
        self,
        user_ids: torch.Tensor,
        token_sequences: Optional[torch.Tensor] = None,
        return_dict: bool = True,
    ):
        """
        å‰å‘ä¼ æ’­
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
            token_sequences: [batch_size, seq_len] tokenåºåˆ—
        """
        inputs = self.prepare_inputs(user_ids, token_sequences)
        
        # å¦‚æœä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ï¼Œéœ€è¦æ‰‹åŠ¨æ›¿æ¢ç”¨æˆ·tokençš„åµŒå…¥
        if not self.use_cross_attention:
            # è·å–tokenåµŒå…¥
            inputs_embeds = self.transformer.transformer.wte(inputs['input_ids'])
            
            # æ›¿æ¢ç”¨æˆ·tokenä½ç½®çš„åµŒå…¥
            user_positions = (inputs['input_ids'] == self.user_token_id)
            inputs_embeds[user_positions] = inputs['user_embeds'].unsqueeze(1).expand(-1, user_positions.sum(1).max(), -1)[user_positions]
            
            # ä½¿ç”¨åµŒå…¥è€Œä¸æ˜¯token ID
            outputs = self.transformer(
                inputs_embeds=inputs_embeds,
                attention_mask=inputs['attention_mask'],
                labels=inputs['labels'],
                return_dict=return_dict,
            )
        else:
            # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
            outputs = self.transformer(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                encoder_hidden_states=inputs['encoder_hidden_states'],
                encoder_attention_mask=inputs['encoder_attention_mask'],
                labels=inputs['labels'],
                return_dict=return_dict,
            )
        
        return outputs

    @torch.no_grad()
    def generate(
        self,
        user_ids: torch.Tensor,
        max_length: int = 256,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        do_sample: bool = True,
        num_return_sequences: int = 1,
    ) -> torch.Tensor:
        """
        ç”Ÿæˆtokenåºåˆ—
        Args:
            user_ids: [batch_size] ç”¨æˆ·ID
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            do_sample: æ˜¯å¦é‡‡æ ·
            num_return_sequences: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„åºåˆ—æ•°
        Returns:
            generated_sequences: [batch_size * num_return_sequences, seq_len]
        """
        batch_size = user_ids.size(0)
        device = user_ids.device

        # æ‰©å±•ç”¨æˆ·IDä»¥æ”¯æŒå¤šåºåˆ—ç”Ÿæˆ
        if num_return_sequences > 1:
            user_ids = user_ids.unsqueeze(1).expand(-1, num_return_sequences).flatten()
            batch_size = user_ids.size(0)

        # å‡†å¤‡åˆå§‹è¾“å…¥
        inputs = self.prepare_inputs(user_ids, max_length=max_length)
        input_ids = inputs['input_ids']  # [batch_size, 1] åªæœ‰ç”¨æˆ·token

        # ç”Ÿæˆå¾ªç¯
        for _ in range(max_length):
            # å‰å‘ä¼ æ’­
            if not self.use_cross_attention:
                # æ‰‹åŠ¨å¤„ç†ç”¨æˆ·åµŒå…¥
                inputs_embeds = self.transformer.transformer.wte(input_ids)
                user_positions = (input_ids == self.user_token_id)
                if user_positions.any():
                    user_embeds = self.user_encoder(user_ids % self.num_users)  # å¤„ç†æ‰©å±•çš„user_ids
                    inputs_embeds[user_positions] = user_embeds.unsqueeze(1).expand(-1, user_positions.sum(1).max(), -1)[user_positions]

                outputs = self.transformer(
                    inputs_embeds=inputs_embeds,
                    attention_mask=inputs['attention_mask'],
                    return_dict=True,
                )
            else:
                outputs = self.transformer(
                    input_ids=input_ids,
                    attention_mask=inputs['attention_mask'],
                    encoder_hidden_states=inputs['encoder_hidden_states'],
                    encoder_attention_mask=inputs['encoder_attention_mask'],
                    return_dict=True,
                )

            # è·å–ä¸‹ä¸€ä¸ªtokençš„logits
            next_token_logits = outputs.logits[:, -1, :] / temperature

            # è¿‡æ»¤è¯æ±‡è¡¨ï¼Œåªä¿ç•™æœ‰æ•ˆçš„VQ token
            next_token_logits[:, self.vocab_size:] = -float('inf')  # å±è”½ç‰¹æ®Štoken

            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            if do_sample:
                # Top-ké‡‡æ ·
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = -float('inf')

                # Top-p (nucleus)é‡‡æ ·
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    next_token_logits[indices_to_remove] = -float('inf')

                # é‡‡æ ·
                probs = F.softmax(next_token_logits, dim=-1)
                next_tokens = torch.multinomial(probs, num_samples=1)
            else:
                # è´ªå¿ƒè§£ç 
                next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)

            # æ·»åŠ åˆ°åºåˆ—
            input_ids = torch.cat([input_ids, next_tokens], dim=-1)

            # æ›´æ–°æ³¨æ„åŠ›æ©ç 
            inputs['attention_mask'] = torch.cat([
                inputs['attention_mask'],
                torch.ones(batch_size, 1, device=device)
            ], dim=-1)

        # ç§»é™¤ç”¨æˆ·tokenï¼Œåªè¿”å›ç”Ÿæˆçš„VQ tokenåºåˆ—
        generated_sequences = input_ids[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªç”¨æˆ·token

        return generated_sequences
