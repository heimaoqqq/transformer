#!/usr/bin/env python3
"""
ç»„ä»¶è¯Šæ–­è„šæœ¬ - åˆ†ç¦»æµ‹è¯•VQ-VAEå’ŒTransformer
åˆ¤æ–­ç”Ÿæˆæ¨¡å¼å´©æºƒæ˜¯å“ªä¸ªç»„ä»¶çš„é—®é¢˜
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.data_loader import MicroDopplerDataset
from torch.utils.data import DataLoader

class ComponentDiagnostic:
    """ç»„ä»¶è¯Šæ–­å™¨"""
    
    def __init__(self, vqvae_path, transformer_path=None, data_dir="data/processed"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.vqvae_path = Path(vqvae_path)
        self.transformer_path = Path(transformer_path) if transformer_path else None
        self.data_dir = Path(data_dir)
        
        print(f"ğŸ” ç»„ä»¶è¯Šæ–­å™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   VQ-VAEè·¯å¾„: {self.vqvae_path}")
        print(f"   Transformerè·¯å¾„: {self.transformer_path}")
        
        # åŠ è½½æ¨¡å‹
        self.vqvae_model = self._load_vqvae()
        self.transformer_model = self._load_transformer() if self.transformer_path else None
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        self.test_data = self._load_test_data()
    
    def _load_vqvae(self):
        """åŠ è½½VQ-VAEæ¨¡å‹"""
        try:
            from diffusers import VQModel
            
            # å°è¯•åŠ è½½diffusersæ ¼å¼
            if (self.vqvae_path / "config.json").exists():
                print("ğŸ“ åŠ è½½diffusersæ ¼å¼VQ-VAE...")
                vqvae = VQModel.from_pretrained(str(self.vqvae_path))
            else:
                # å°è¯•åŠ è½½checkpointæ ¼å¼
                print("ğŸ“ åŠ è½½checkpointæ ¼å¼VQ-VAE...")
                checkpoint_files = list(self.vqvae_path.glob("*.pth"))
                if not checkpoint_files:
                    raise FileNotFoundError(f"æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹æ–‡ä»¶: {self.vqvae_path}")
                
                checkpoint_path = checkpoint_files[0]
                checkpoint = torch.load(checkpoint_path, map_location=self.device)
                
                # åˆ›å»ºVQ-VAEæ¨¡å‹
                vqvae = VQModel(
                    in_channels=1,
                    out_channels=1,
                    down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D"],
                    up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D"],
                    block_out_channels=[128, 256],
                    layers_per_block=2,
                    act_fn="silu",
                    latent_channels=256,
                    sample_size=128,
                    num_vq_embeddings=1024,
                    vq_embed_dim=256,
                )
                
                vqvae.load_state_dict(checkpoint['model_state_dict'])
            
            vqvae.to(self.device)
            vqvae.eval()
            print("âœ… VQ-VAEåŠ è½½æˆåŠŸ")
            return vqvae
            
        except Exception as e:
            print(f"âŒ VQ-VAEåŠ è½½å¤±è´¥: {e}")
            return None
    
    def _load_transformer(self):
        """åŠ è½½Transformeræ¨¡å‹"""
        try:
            from models.transformer_model import MicroDopplerTransformer
            
            checkpoint = torch.load(self.transformer_path, map_location=self.device)
            
            # åˆ›å»ºTransformeræ¨¡å‹
            transformer = MicroDopplerTransformer(
                vocab_size=1024,
                max_seq_len=1024,
                num_users=31,
                d_model=256,
                nhead=8,
                num_layers=6,
                dim_feedforward=1024,
                dropout=0.1
            )
            
            transformer.load_state_dict(checkpoint['model_state_dict'])
            transformer.to(self.device)
            transformer.eval()
            print("âœ… TransformeråŠ è½½æˆåŠŸ")
            return transformer
            
        except Exception as e:
            print(f"âŒ TransformeråŠ è½½å¤±è´¥: {e}")
            return None
    
    def _load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        try:
            dataset = MicroDopplerDataset(
                data_dir=str(self.data_dir),
                split='test',
                transform=None
            )
            
            dataloader = DataLoader(
                dataset,
                batch_size=4,
                shuffle=False,
                num_workers=0
            )
            
            # è·å–ä¸€ä¸ªbatchçš„æµ‹è¯•æ•°æ®
            test_batch = next(iter(dataloader))
            print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸ: {test_batch['image'].shape}")
            return test_batch
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def diagnose_vqvae(self):
        """è¯Šæ–­VQ-VAEç»„ä»¶"""
        print("\n" + "="*60)
        print("ğŸ” VQ-VAEç»„ä»¶è¯Šæ–­")
        print("="*60)
        
        if self.vqvae_model is None or self.test_data is None:
            print("âŒ æ— æ³•è¿›è¡ŒVQ-VAEè¯Šæ–­ï¼šæ¨¡å‹æˆ–æ•°æ®æœªåŠ è½½")
            return False
        
        images = self.test_data['image'].to(self.device)
        user_ids = self.test_data['user_id']
        
        with torch.no_grad():
            # 1. ç¼–ç æµ‹è¯•
            print("1ï¸âƒ£ ç¼–ç æµ‹è¯•...")
            encoded = self.vqvae_model.encode(images)
            if hasattr(encoded, 'latents'):
                latents = encoded.latents
            else:
                latents = encoded
            print(f"   ç¼–ç è¾“å‡ºå½¢çŠ¶: {latents.shape}")
            
            # 2. é‡åŒ–æµ‹è¯•
            print("2ï¸âƒ£ é‡åŒ–æµ‹è¯•...")
            quantized_output = self.vqvae_model.quantize(latents)
            if hasattr(quantized_output, 'quantized'):
                quantized = quantized_output.quantized
                indices = quantized_output.indices
            else:
                quantized = quantized_output
                indices = None
            
            print(f"   é‡åŒ–è¾“å‡ºå½¢çŠ¶: {quantized.shape}")
            if indices is not None:
                print(f"   ç´¢å¼•å½¢çŠ¶: {indices.shape}")
                print(f"   ç´¢å¼•èŒƒå›´: [{indices.min().item()}, {indices.max().item()}]")
                
                # åˆ†æç æœ¬ä½¿ç”¨
                unique_indices = torch.unique(indices)
                usage_ratio = len(unique_indices) / 1024
                print(f"   ç æœ¬ä½¿ç”¨ç‡: {len(unique_indices)}/1024 ({usage_ratio:.2%})")
            
            # 3. è§£ç æµ‹è¯•
            print("3ï¸âƒ£ è§£ç æµ‹è¯•...")
            decoded = self.vqvae_model.decode(quantized)
            if hasattr(decoded, 'sample'):
                reconstructed = decoded.sample
            else:
                reconstructed = decoded
            print(f"   é‡å»ºè¾“å‡ºå½¢çŠ¶: {reconstructed.shape}")
            
            # 4. é‡å»ºè´¨é‡è¯„ä¼°
            print("4ï¸âƒ£ é‡å»ºè´¨é‡è¯„ä¼°...")
            mse_loss = F.mse_loss(reconstructed, images)
            psnr = 20 * torch.log10(1.0 / torch.sqrt(mse_loss))
            
            print(f"   MSEæŸå¤±: {mse_loss.item():.6f}")
            print(f"   PSNR: {psnr.item():.2f} dB")
            
            # 5. ç”¨æˆ·ç‰¹å¼‚æ€§æµ‹è¯•
            print("5ï¸âƒ£ ç”¨æˆ·ç‰¹å¼‚æ€§æµ‹è¯•...")
            user_reconstructions = {}
            for i, uid in enumerate(user_ids):
                user_reconstructions[uid.item()] = reconstructed[i:i+1]
            
            if len(user_reconstructions) > 1:
                users = list(user_reconstructions.keys())
                img1 = user_reconstructions[users[0]]
                img2 = user_reconstructions[users[1]]
                user_diff = F.mse_loss(img1, img2)
                print(f"   ç”¨æˆ·é—´å·®å¼‚: {user_diff.item():.6f}")
            
            # 6. ä¿å­˜è¯Šæ–­ç»“æœ
            self._save_vqvae_diagnosis(images, reconstructed, user_ids)
            
            # åˆ¤æ–­VQ-VAEè´¨é‡
            vqvae_quality = self._evaluate_vqvae_quality(mse_loss.item(), usage_ratio, psnr.item())
            return vqvae_quality
    
    def diagnose_transformer(self):
        """è¯Šæ–­Transformerç»„ä»¶"""
        print("\n" + "="*60)
        print("ğŸ” Transformerç»„ä»¶è¯Šæ–­")
        print("="*60)
        
        if self.transformer_model is None or self.vqvae_model is None or self.test_data is None:
            print("âŒ æ— æ³•è¿›è¡ŒTransformerè¯Šæ–­ï¼šæ¨¡å‹æˆ–æ•°æ®æœªåŠ è½½")
            return False
        
        images = self.test_data['image'].to(self.device)
        user_ids = self.test_data['user_id'].to(self.device)
        
        with torch.no_grad():
            # 1. è·å–çœŸå®tokens
            print("1ï¸âƒ£ è·å–çœŸå®tokens...")
            encoded = self.vqvae_model.encode(images)
            if hasattr(encoded, 'latents'):
                latents = encoded.latents
            else:
                latents = encoded
            
            quantized_output = self.vqvae_model.quantize(latents)
            if hasattr(quantized_output, 'indices'):
                real_tokens = quantized_output.indices.flatten(1)  # [B, H*W]
            else:
                print("âŒ æ— æ³•è·å–tokenç´¢å¼•")
                return False
            
            print(f"   çœŸå®tokenså½¢çŠ¶: {real_tokens.shape}")
            print(f"   tokensèŒƒå›´: [{real_tokens.min().item()}, {real_tokens.max().item()}]")
            
            # 2. Transformerç”Ÿæˆæµ‹è¯•
            print("2ï¸âƒ£ Transformerç”Ÿæˆæµ‹è¯•...")
            generated_tokens = self._generate_tokens(user_ids, max_length=real_tokens.shape[1])
            
            if generated_tokens is not None:
                print(f"   ç”Ÿæˆtokenså½¢çŠ¶: {generated_tokens.shape}")
                print(f"   ç”ŸæˆtokensèŒƒå›´: [{generated_tokens.min().item()}, {generated_tokens.max().item()}]")
                
                # 3. Tokenåˆ†å¸ƒåˆ†æ
                print("3ï¸âƒ£ Tokenåˆ†å¸ƒåˆ†æ...")
                self._analyze_token_distribution(real_tokens, generated_tokens)
                
                # 4. ç”¨æˆ·æ¡ä»¶æµ‹è¯•
                print("4ï¸âƒ£ ç”¨æˆ·æ¡ä»¶æµ‹è¯•...")
                self._test_user_conditioning(user_ids)
                
                # 5. ç”Ÿæˆå›¾åƒè´¨é‡æµ‹è¯•
                print("5ï¸âƒ£ ç”Ÿæˆå›¾åƒè´¨é‡æµ‹è¯•...")
                generated_images = self._decode_tokens(generated_tokens)
                if generated_images is not None:
                    self._save_transformer_diagnosis(images, generated_images, user_ids)
                    
                    # åˆ¤æ–­Transformerè´¨é‡
                    transformer_quality = self._evaluate_transformer_quality(real_tokens, generated_tokens, images, generated_images)
                    return transformer_quality
        
        return False
    
    def _generate_tokens(self, user_ids, max_length=1024):
        """ç”Ÿæˆtokens"""
        try:
            batch_size = user_ids.shape[0]
            generated = torch.zeros((batch_size, 1), dtype=torch.long, device=self.device)
            
            for i in range(max_length):
                outputs = self.transformer_model(
                    input_ids=generated,
                    user_ids=user_ids
                )
                
                next_token_logits = outputs.logits[:, -1, :]
                next_token = torch.multinomial(torch.softmax(next_token_logits, dim=-1), 1)
                generated = torch.cat([generated, next_token], dim=1)
                
                if i % 100 == 0:
                    print(f"   ç”Ÿæˆè¿›åº¦: {i}/{max_length}")
            
            return generated[:, 1:]  # ç§»é™¤èµ·å§‹token
            
        except Exception as e:
            print(f"âŒ Tokenç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _decode_tokens(self, tokens):
        """è§£ç tokensä¸ºå›¾åƒ"""
        try:
            # é‡å¡‘ä¸º2D
            batch_size = tokens.shape[0]
            tokens_2d = tokens.view(batch_size, 32, 32)  # å‡è®¾32x32
            
            # åˆ›å»ºé‡åŒ–ç‰¹å¾
            quantized_features = self.vqvae_model.quantize.get_codebook_entry(
                tokens_2d.flatten(), tokens_2d.shape
            )
            
            # è§£ç 
            decoded = self.vqvae_model.decode(quantized_features)
            if hasattr(decoded, 'sample'):
                return decoded.sample
            else:
                return decoded
                
        except Exception as e:
            print(f"âŒ Tokenè§£ç å¤±è´¥: {e}")
            return None
    
    def _analyze_token_distribution(self, real_tokens, generated_tokens):
        """åˆ†ætokenåˆ†å¸ƒ"""
        real_unique = torch.unique(real_tokens)
        gen_unique = torch.unique(generated_tokens)
        
        print(f"   çœŸå®tokenså”¯ä¸€å€¼: {len(real_unique)}")
        print(f"   ç”Ÿæˆtokenså”¯ä¸€å€¼: {len(gen_unique)}")
        
        # è®¡ç®—åˆ†å¸ƒå·®å¼‚
        real_hist = torch.histc(real_tokens.float(), bins=1024, min=0, max=1023)
        gen_hist = torch.histc(generated_tokens.float(), bins=1024, min=0, max=1023)
        
        # å½’ä¸€åŒ–
        real_hist = real_hist / real_hist.sum()
        gen_hist = gen_hist / gen_hist.sum()
        
        # KLæ•£åº¦
        kl_div = F.kl_div(gen_hist.log(), real_hist, reduction='sum')
        print(f"   åˆ†å¸ƒKLæ•£åº¦: {kl_div.item():.4f}")
    
    def _test_user_conditioning(self, user_ids):
        """æµ‹è¯•ç”¨æˆ·æ¡ä»¶"""
        if len(torch.unique(user_ids)) < 2:
            print("   âš ï¸ éœ€è¦è‡³å°‘2ä¸ªä¸åŒç”¨æˆ·è¿›è¡Œæµ‹è¯•")
            return
        
        # ç”Ÿæˆä¸åŒç”¨æˆ·çš„tokens
        user1_id = user_ids[:1]
        user2_id = user_ids[1:2] if len(user_ids) > 1 else user_ids[:1]
        
        tokens1 = self._generate_tokens(user1_id, max_length=100)
        tokens2 = self._generate_tokens(user2_id, max_length=100)
        
        if tokens1 is not None and tokens2 is not None:
            # è®¡ç®—å·®å¼‚
            diff_ratio = (tokens1 != tokens2).float().mean()
            print(f"   ç”¨æˆ·é—´tokenå·®å¼‚ç‡: {diff_ratio.item():.2%}")
            
            if diff_ratio < 0.1:
                print("   âš ï¸ è­¦å‘Šï¼šç”¨æˆ·é—´å·®å¼‚è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¨¡å¼å´©æºƒ")
    
    def _evaluate_vqvae_quality(self, mse_loss, usage_ratio, psnr):
        """è¯„ä¼°VQ-VAEè´¨é‡"""
        print("\nğŸ“Š VQ-VAEè´¨é‡è¯„ä¼°:")
        
        issues = []
        if mse_loss > 0.1:
            issues.append("é‡å»ºè¯¯å·®è¿‡é«˜")
        if usage_ratio < 0.1:
            issues.append("ç æœ¬ä½¿ç”¨ç‡è¿‡ä½")
        if psnr < 15:
            issues.append("PSNRè¿‡ä½")
        
        if not issues:
            print("   âœ… VQ-VAEè´¨é‡è‰¯å¥½")
            return True
        else:
            print("   âŒ VQ-VAEå­˜åœ¨é—®é¢˜:")
            for issue in issues:
                print(f"      - {issue}")
            return False
    
    def _evaluate_transformer_quality(self, real_tokens, generated_tokens, real_images, generated_images):
        """è¯„ä¼°Transformerè´¨é‡"""
        print("\nğŸ“Š Transformerè´¨é‡è¯„ä¼°:")
        
        issues = []
        
        # Tokenå¤šæ ·æ€§æ£€æŸ¥
        gen_unique_ratio = len(torch.unique(generated_tokens)) / 1024
        if gen_unique_ratio < 0.05:
            issues.append("ç”Ÿæˆtokenå¤šæ ·æ€§ä¸è¶³")
        
        # å›¾åƒè´¨é‡æ£€æŸ¥
        if generated_images is not None:
            img_mse = F.mse_loss(generated_images, real_images)
            if img_mse > 0.5:
                issues.append("ç”Ÿæˆå›¾åƒè´¨é‡å·®")
        
        if not issues:
            print("   âœ… Transformerè´¨é‡è‰¯å¥½")
            return True
        else:
            print("   âŒ Transformerå­˜åœ¨é—®é¢˜:")
            for issue in issues:
                print(f"      - {issue}")
            return False
    
    def _save_vqvae_diagnosis(self, original, reconstructed, user_ids):
        """ä¿å­˜VQ-VAEè¯Šæ–­ç»“æœ"""
        try:
            fig, axes = plt.subplots(2, 4, figsize=(16, 8))
            
            for i in range(min(4, original.shape[0])):
                # åŸå§‹å›¾åƒ
                axes[0, i].imshow(original[i, 0].cpu().numpy(), cmap='viridis')
                axes[0, i].set_title(f'åŸå§‹ (User {user_ids[i].item()})')
                axes[0, i].axis('off')
                
                # é‡å»ºå›¾åƒ
                axes[1, i].imshow(reconstructed[i, 0].cpu().numpy(), cmap='viridis')
                axes[1, i].set_title(f'é‡å»º (User {user_ids[i].item()})')
                axes[1, i].axis('off')
            
            plt.tight_layout()
            plt.savefig('vqvae_diagnosis.png', dpi=150, bbox_inches='tight')
            plt.close()
            print("ğŸ’¾ VQ-VAEè¯Šæ–­å›¾åƒå·²ä¿å­˜: vqvae_diagnosis.png")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜VQ-VAEè¯Šæ–­å›¾åƒå¤±è´¥: {e}")
    
    def _save_transformer_diagnosis(self, original, generated, user_ids):
        """ä¿å­˜Transformerè¯Šæ–­ç»“æœ"""
        try:
            fig, axes = plt.subplots(2, 4, figsize=(16, 8))
            
            for i in range(min(4, original.shape[0])):
                # åŸå§‹å›¾åƒ
                axes[0, i].imshow(original[i, 0].cpu().numpy(), cmap='viridis')
                axes[0, i].set_title(f'çœŸå® (User {user_ids[i].item()})')
                axes[0, i].axis('off')
                
                # ç”Ÿæˆå›¾åƒ
                axes[1, i].imshow(generated[i, 0].cpu().numpy(), cmap='viridis')
                axes[1, i].set_title(f'ç”Ÿæˆ (User {user_ids[i].item()})')
                axes[1, i].axis('off')
            
            plt.tight_layout()
            plt.savefig('transformer_diagnosis.png', dpi=150, bbox_inches='tight')
            plt.close()
            print("ğŸ’¾ Transformerè¯Šæ–­å›¾åƒå·²ä¿å­˜: transformer_diagnosis.png")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜Transformerè¯Šæ–­å›¾åƒå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç»„ä»¶è¯Šæ–­")
    parser.add_argument("--vqvae_path", type=str, default="models/vqvae_model", help="VQ-VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--transformer_path", type=str, help="Transformeræ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_dir", type=str, default="data/processed", help="æ•°æ®ç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯Šæ–­å™¨
    diagnostic = ComponentDiagnostic(
        vqvae_path=args.vqvae_path,
        transformer_path=args.transformer_path,
        data_dir=args.data_dir
    )
    
    # è¯Šæ–­VQ-VAE
    vqvae_ok = diagnostic.diagnose_vqvae()
    
    # è¯Šæ–­Transformerï¼ˆå¦‚æœæä¾›äº†è·¯å¾„ï¼‰
    transformer_ok = True
    if args.transformer_path:
        transformer_ok = diagnostic.diagnose_transformer()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ¯ è¯Šæ–­æ€»ç»“")
    print("="*60)
    
    if vqvae_ok and transformer_ok:
        print("âœ… ä¸¤ä¸ªç»„ä»¶éƒ½æ­£å¸¸ï¼Œé—®é¢˜å¯èƒ½åœ¨è®­ç»ƒç­–ç•¥æˆ–å‚æ•°è®¾ç½®")
    elif not vqvae_ok and transformer_ok:
        print("âŒ VQ-VAEå­˜åœ¨é—®é¢˜ï¼Œå»ºè®®é‡æ–°è®­ç»ƒVQ-VAE")
    elif vqvae_ok and not transformer_ok:
        print("âŒ Transformerå­˜åœ¨é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨æ”¹è¿›çš„è®­ç»ƒè„šæœ¬")
    else:
        print("âŒ ä¸¤ä¸ªç»„ä»¶éƒ½å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®ä»VQ-VAEå¼€å§‹é‡æ–°è®­ç»ƒ")

if __name__ == "__main__":
    main()
