#!/usr/bin/env python3
"""
ç»„ä»¶è¯Šæ–­è„šæœ¬ - åˆ†ç¦»æµ‹è¯•VQ-VAEå’ŒTransformer
åˆ¤æ–­ç”Ÿæˆæ¨¡å¼å´©æºƒæ˜¯å“ªä¸ªç»„ä»¶çš„é—®é¢˜
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import os
import torchvision.transforms as transforms

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.data_loader import MicroDopplerDataset
from torch.utils.data import DataLoader

class ComponentDiagnostic:
    """ç»„ä»¶è¯Šæ–­å™¨"""
    
    def __init__(self, vqvae_path, transformer_path=None, data_dir="data/processed"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.vqvae_path = Path(vqvae_path)
        self.transformer_path = Path(transformer_path) if transformer_path else None
        self.data_dir = Path(data_dir)
        
        print(f"ğŸ” ç»„ä»¶è¯Šæ–­å™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   VQ-VAEè·¯å¾„: {self.vqvae_path}")
        print(f"   Transformerè·¯å¾„: {self.transformer_path}")
        
        # åŠ è½½æ¨¡å‹
        self.vqvae_model = self._load_vqvae()
        self.transformer_model = self._load_transformer() if self.transformer_path else None
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        self.test_data = self._load_test_data()
    
    def _load_vqvae(self):
        """åŠ è½½VQ-VAEæ¨¡å‹"""
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not self.vqvae_path.exists():
                print(f"âŒ VQ-VAEè·¯å¾„ä¸å­˜åœ¨: {self.vqvae_path}")
                print("ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„")
                return None

            # å°è¯•åŠ è½½diffusersæ ¼å¼
            try:
                from diffusers import VQModel

                if (self.vqvae_path / "config.json").exists():
                    print("ğŸ“ åŠ è½½diffusersæ ¼å¼VQ-VAE...")
                    vqvae = VQModel.from_pretrained(str(self.vqvae_path))
                else:
                    # å°è¯•åŠ è½½checkpointæ ¼å¼
                    print("ğŸ“ åŠ è½½checkpointæ ¼å¼VQ-VAE...")
                    checkpoint_files = list(self.vqvae_path.glob("*.pth"))
                    if not checkpoint_files:
                        raise FileNotFoundError(f"æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹æ–‡ä»¶: {self.vqvae_path}")

                    checkpoint_path = checkpoint_files[0]
                    checkpoint = torch.load(checkpoint_path, map_location=self.device)

                    # åˆ›å»ºVQ-VAEæ¨¡å‹
                    vqvae = VQModel(
                        in_channels=1,
                        out_channels=1,
                        down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D"],
                        up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D"],
                        block_out_channels=[128, 256],
                        layers_per_block=2,
                        act_fn="silu",
                        latent_channels=256,
                        sample_size=128,
                        num_vq_embeddings=1024,
                        vq_embed_dim=256,
                    )

                    vqvae.load_state_dict(checkpoint['model_state_dict'])

                vqvae.to(self.device)
                vqvae.eval()
                print("âœ… VQ-VAEåŠ è½½æˆåŠŸ")
                return vqvae

            except ImportError:
                print("âŒ ç¼ºå°‘diffusersæ¨¡å—ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°VQ-VAEå®ç°...")
                try:
                    from models.vqvae_model import MicroDopplerVQVAE

                    # å°è¯•åŠ è½½æœ¬åœ°VQ-VAEæ¨¡å‹
                    checkpoint_files = list(self.vqvae_path.glob("*.pth"))
                    if not checkpoint_files:
                        print("âŒ æœªæ‰¾åˆ°æœ¬åœ°VQ-VAEæ¨¡å‹æ–‡ä»¶")
                        return None

                    checkpoint_path = checkpoint_files[0]
                    print(f"ğŸ“ åŠ è½½æœ¬åœ°VQ-VAE: {checkpoint_path}")

                    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æœ¬åœ°VQ-VAEå®ç°æ¥è°ƒæ•´
                    vqvae = MicroDopplerVQVAE.from_pretrained(str(self.vqvae_path))
                    vqvae.to(self.device)
                    vqvae.eval()
                    print("âœ… æœ¬åœ°VQ-VAEåŠ è½½æˆåŠŸ")
                    return vqvae

                except Exception as local_e:
                    print(f"âŒ æœ¬åœ°VQ-VAEåŠ è½½ä¹Ÿå¤±è´¥: {local_e}")
                    return None

        except Exception as e:
            print(f"âŒ VQ-VAEåŠ è½½å¤±è´¥: {e}")
            return None
    
    def _load_transformer(self):
        """åŠ è½½Transformeræ¨¡å‹"""
        try:
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not self.transformer_path.exists():
                print(f"âŒ Transformerè·¯å¾„ä¸å­˜åœ¨: {self.transformer_path}")
                print("ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„")
                return None

            from models.transformer_model import MicroDopplerTransformer

            # ä¿®å¤PyTorch 2.6çš„weights_onlyé—®é¢˜
            checkpoint = torch.load(self.transformer_path, map_location=self.device, weights_only=False)

            # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if self._is_vqvae_checkpoint(checkpoint):
                print("âŒ é”™è¯¯ï¼šæä¾›çš„æ˜¯VQ-VAEæ¨¡å‹ï¼Œä¸æ˜¯Transformeræ¨¡å‹")
                print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                print("   1. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
                print("   2. ç¡®ä¿æŒ‡å‘çš„æ˜¯Transformeræ¨¡å‹æ–‡ä»¶")
                print("   3. VQ-VAEæ¨¡å‹é€šå¸¸åŒ…å«encoder/decoderæƒé‡")
                print("   4. Transformeræ¨¡å‹åº”åŒ…å«transformer.transformeræƒé‡")
                print("   5. å¦‚æœåªæƒ³è¯Šæ–­VQ-VAEï¼Œè¯·ä¸è¦æä¾›--transformer_pathå‚æ•°")
                print(f"   6. å½“å‰æä¾›çš„è·¯å¾„: {self.transformer_path}")
                print("   7. è¯·æ£€æŸ¥è¯¥è·¯å¾„æ˜¯å¦æŒ‡å‘æ­£ç¡®çš„Transformeræ¨¡å‹æ–‡ä»¶")
                return None

            # ä»checkpointä¸­è·å–æ¨¡å‹å‚æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'args' in checkpoint:
                args = checkpoint['args']
                print(f"ğŸ“‹ ä»checkpointè¯»å–å‚æ•°:")
                print(f"   vocab_size: {getattr(args, 'vocab_size', 1024)}")
                print(f"   num_users: {getattr(args, 'num_users', 31)}")
                print(f"   n_embd: {getattr(args, 'n_embd', 256)}")

                # ä½¿ç”¨checkpointä¸­çš„å‚æ•°
                transformer = MicroDopplerTransformer(
                    vocab_size=getattr(args, 'vocab_size', 1024),
                    max_seq_len=getattr(args, 'max_seq_len', 1024),
                    num_users=getattr(args, 'num_users', 31),
                    n_embd=getattr(args, 'n_embd', 256),
                    n_layer=getattr(args, 'n_layer', 6),
                    n_head=getattr(args, 'n_head', 8),
                    dropout=getattr(args, 'dropout', 0.1),
                    use_cross_attention=getattr(args, 'use_cross_attention', True)
                )
            else:
                print("âš ï¸ checkpointä¸­æ²¡æœ‰argsï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
                # åˆ›å»ºTransformeræ¨¡å‹ - ä½¿ç”¨é»˜è®¤å‚æ•°
                transformer = MicroDopplerTransformer(
                    vocab_size=1024,
                    max_seq_len=1024,
                    num_users=31,
                    n_embd=256,  # ä¿®æ­£ï¼šä½¿ç”¨n_embdè€Œä¸æ˜¯d_model
                    n_layer=6,   # ä¿®æ­£ï¼šä½¿ç”¨n_layerè€Œä¸æ˜¯num_layers
                    n_head=8,    # ä¿®æ­£ï¼šä½¿ç”¨n_headè€Œä¸æ˜¯nhead
                    dropout=0.1,
                    use_cross_attention=True
                )

            transformer.load_state_dict(checkpoint['model_state_dict'])
            transformer.to(self.device)
            transformer.eval()
            print("âœ… TransformeråŠ è½½æˆåŠŸ")
            return transformer

        except Exception as e:
            print(f"âŒ TransformeråŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å¸¸è§é—®é¢˜:")
            print("   - æ¨¡å‹ç±»å‹é”™è¯¯ï¼šæä¾›äº†VQ-VAEè€ŒéTransformeræ¨¡å‹")
            print("   - å‚æ•°åç§°ä¸åŒ¹é… (å·²ä¿®å¤)")
            print("   - æ¨¡å‹æ–‡ä»¶æŸåæˆ–æ ¼å¼ä¸æ­£ç¡®")
            print("   - ç¼ºå°‘å¿…è¦çš„ä¾èµ–æ¨¡å—")
            return None

    def _is_vqvae_checkpoint(self, checkpoint):
        """æ£€æŸ¥æ˜¯å¦ä¸ºVQ-VAEæ¨¡å‹checkpoint"""
        if 'model_state_dict' not in checkpoint:
            return False

        state_dict = checkpoint['model_state_dict']

        # VQ-VAEç‰¹å¾é”®
        vqvae_keys = [
            'encoder.conv_in.weight',
            'decoder.conv_out.weight',
            'quantize.embedding.weight',
            'quant_conv.weight',
            'post_quant_conv.weight'
        ]

        # Transformerç‰¹å¾é”®
        transformer_keys = [
            'transformer.transformer.wte.weight',
            'user_encoder.user_embedding.weight',
            'transformer.lm_head.weight'
        ]

        # æ£€æŸ¥VQ-VAEç‰¹å¾
        vqvae_count = sum(1 for key in vqvae_keys if key in state_dict)
        transformer_count = sum(1 for key in transformer_keys if key in state_dict)

        return vqvae_count > transformer_count
    
    def _load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        try:
            # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
            if not self.data_dir.exists():
                print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
                print("ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨æœ¬åœ°æ•°æ®è·¯å¾„")
                print("ğŸ’¡ å¯ç”¨çš„æœ¬åœ°è·¯å¾„ç¤ºä¾‹:")
                print("   - data/processed")
                print("   - ../data/processed")
                print("   - æˆ–å…¶ä»–åŒ…å«å¾®å¤šæ™®å‹’æ•°æ®çš„ç›®å½•")
                return self._create_dummy_data()

            # åˆ›å»ºå›¾åƒå˜æ¢ - ç¡®ä¿è¾“å‡ºtensoræ ¼å¼
            transform = transforms.Compose([
                transforms.Resize((128, 128)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # å½’ä¸€åŒ–åˆ°[-1, 1]
            ])

            # å°è¯•ä¸åŒçš„æ•°æ®åŠ è½½æ–¹å¼
            dataset = None

            # æ–¹å¼1ï¼šå°è¯•å¸¦splitå‚æ•°
            try:
                dataset = MicroDopplerDataset(
                    data_dir=str(self.data_dir),
                    split='test',
                    transform=transform,
                    return_user_id=True
                )
            except TypeError:
                # æ–¹å¼2ï¼šä¸å¸¦splitå‚æ•°
                try:
                    dataset = MicroDopplerDataset(
                        data_dir=str(self.data_dir),
                        transform=transform,
                        return_user_id=True
                    )
                except Exception:
                    # æ–¹å¼3ï¼šå°è¯•å…¶ä»–å¯èƒ½çš„å‚æ•°
                    dataset = MicroDopplerDataset(
                        str(self.data_dir),
                        transform=transform
                    )

            if dataset is None:
                print("âŒ æ— æ³•åˆ›å»ºæ•°æ®é›†ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return self._create_dummy_data()

            print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ:")
            print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")

            # è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯
            if hasattr(dataset, 'get_user_statistics'):
                user_stats = dataset.get_user_statistics()
                print(f"   ç”¨æˆ·æ•°é‡: {len(user_stats)}")

            dataloader = DataLoader(
                dataset,
                batch_size=4,
                shuffle=False,
                num_workers=0,
                collate_fn=self._custom_collate_fn  # ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
            )

            # è·å–ä¸€ä¸ªbatchçš„æµ‹è¯•æ•°æ®
            test_batch = next(iter(dataloader))
            print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸ: {test_batch['image'].shape}")
            return test_batch

        except Exception as e:
            print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
            print(f"   è¯·æ£€æŸ¥æ•°æ®ç›®å½•: {self.data_dir}")
            print("ğŸ’¡ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œè¯Šæ–­...")
            return self._create_dummy_data()

    def _create_dummy_data(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        print("ğŸ”§ åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºè¯Šæ–­...")

        # åˆ›å»ºæ¨¡æ‹Ÿçš„å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®
        batch_size = 4
        channels = 3  # RGB
        height, width = 128, 128

        # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒ - æ¨¡æ‹Ÿå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾çš„ç‰¹å¾
        images = torch.randn(batch_size, channels, height, width)
        images = torch.tanh(images)  # å½’ä¸€åŒ–åˆ°[-1, 1]

        # ç”Ÿæˆæ¨¡æ‹Ÿç”¨æˆ·ID
        user_ids = torch.randint(0, 31, (batch_size,), dtype=torch.long)

        dummy_data = {
            'image': images,
            'user_id': user_ids
        }

        print(f"âœ… æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºæˆåŠŸ:")
        print(f"   å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"   ç”¨æˆ·ID: {user_ids.tolist()}")

        return dummy_data

    def _custom_collate_fn(self, batch):
        """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼"""
        try:
            if isinstance(batch[0], tuple):
                # å¦‚æœè¿”å›çš„æ˜¯(image, user_id)å…ƒç»„
                images = []
                user_ids = []
                for item in batch:
                    if len(item) == 2:
                        image, user_id = item
                        images.append(image)
                        user_ids.append(user_id)
                    else:
                        images.append(item[0])
                        user_ids.append(0)  # é»˜è®¤ç”¨æˆ·ID

                return {
                    'image': torch.stack(images),
                    'user_id': torch.tensor(user_ids, dtype=torch.long)
                }
            else:
                # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªå›¾åƒ
                images = torch.stack(batch)
                return {
                    'image': images,
                    'user_id': torch.zeros(len(batch), dtype=torch.long)
                }
        except Exception as e:
            print(f"âŒ Collateå‡½æ•°é”™è¯¯: {e}")
            # è¿”å›é»˜è®¤æ ¼å¼
            return {
                'image': torch.zeros(4, 3, 128, 128),
                'user_id': torch.zeros(4, dtype=torch.long)
            }
    
    def diagnose_vqvae(self):
        """è¯Šæ–­VQ-VAEç»„ä»¶"""
        print("\n" + "="*60)
        print("ğŸ” VQ-VAEç»„ä»¶è¯Šæ–­")
        print("="*60)
        
        if self.vqvae_model is None or self.test_data is None:
            print("âŒ æ— æ³•è¿›è¡ŒVQ-VAEè¯Šæ–­ï¼šæ¨¡å‹æˆ–æ•°æ®æœªåŠ è½½")
            return False
        
        images = self.test_data['image'].to(self.device)
        user_ids = self.test_data['user_id']
        
        with torch.no_grad():
            # 1. ç¼–ç æµ‹è¯•
            print("1ï¸âƒ£ ç¼–ç æµ‹è¯•...")
            encoded = self.vqvae_model.encode(images)
            if hasattr(encoded, 'latents'):
                latents = encoded.latents
            else:
                latents = encoded
            print(f"   ç¼–ç è¾“å‡ºå½¢çŠ¶: {latents.shape}")
            
            # 2. é‡åŒ–æµ‹è¯•
            print("2ï¸âƒ£ é‡åŒ–æµ‹è¯•...")
            quantized_output = self.vqvae_model.quantize(latents)

            # å¤„ç†ä¸åŒçš„é‡åŒ–è¾“å‡ºæ ¼å¼
            quantized = None
            indices = None

            if hasattr(quantized_output, 'quantized'):
                # å¦‚æœæ˜¯å‘½åå…ƒç»„æˆ–å¯¹è±¡
                quantized = quantized_output.quantized
                indices = quantized_output.indices
            elif isinstance(quantized_output, tuple):
                # å¦‚æœæ˜¯æ™®é€šå…ƒç»„
                if len(quantized_output) >= 2:
                    quantized = quantized_output[0]  # é‡åŒ–ç‰¹å¾
                    indices = quantized_output[1]    # ç´¢å¼•
                else:
                    quantized = quantized_output[0]
                    indices = None
            else:
                # å¦‚æœæ˜¯å•ä¸ªtensor
                quantized = quantized_output
                indices = None

            print(f"   é‡åŒ–è¾“å‡ºå½¢çŠ¶: {quantized.shape}")
            print(f"   é‡åŒ–è¾“å‡ºç±»å‹: {type(quantized_output)}")

            if indices is not None:
                print(f"   ç´¢å¼•å½¢çŠ¶: {indices.shape}")
                print(f"   ç´¢å¼•èŒƒå›´: [{indices.min().item()}, {indices.max().item()}]")

                # åˆ†æç æœ¬ä½¿ç”¨
                unique_indices = torch.unique(indices)
                usage_ratio = len(unique_indices) / 1024
                print(f"   ç æœ¬ä½¿ç”¨ç‡: {len(unique_indices)}/1024 ({usage_ratio:.2%})")
            else:
                print("   âš ï¸ æœªè·å–åˆ°é‡åŒ–ç´¢å¼•ï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„è®¿é—®æ–¹å¼")
            
            # 3. è§£ç æµ‹è¯•
            print("3ï¸âƒ£ è§£ç æµ‹è¯•...")
            decoded = self.vqvae_model.decode(quantized)
            if hasattr(decoded, 'sample'):
                reconstructed = decoded.sample
            else:
                reconstructed = decoded
            print(f"   é‡å»ºè¾“å‡ºå½¢çŠ¶: {reconstructed.shape}")
            
            # 4. é‡å»ºè´¨é‡è¯„ä¼°
            print("4ï¸âƒ£ é‡å»ºè´¨é‡è¯„ä¼°...")
            mse_loss = F.mse_loss(reconstructed, images)
            psnr = 20 * torch.log10(1.0 / torch.sqrt(mse_loss))
            
            print(f"   MSEæŸå¤±: {mse_loss.item():.6f}")
            print(f"   PSNR: {psnr.item():.2f} dB")
            
            # 5. ç”¨æˆ·ç‰¹å¼‚æ€§æµ‹è¯•
            print("5ï¸âƒ£ ç”¨æˆ·ç‰¹å¼‚æ€§æµ‹è¯•...")
            user_reconstructions = {}
            for i, uid in enumerate(user_ids):
                user_reconstructions[uid.item()] = reconstructed[i:i+1]
            
            if len(user_reconstructions) > 1:
                users = list(user_reconstructions.keys())
                img1 = user_reconstructions[users[0]]
                img2 = user_reconstructions[users[1]]
                user_diff = F.mse_loss(img1, img2)
                print(f"   ç”¨æˆ·é—´å·®å¼‚: {user_diff.item():.6f}")
            
            # 6. ä¿å­˜è¯Šæ–­ç»“æœ
            self._save_vqvae_diagnosis(images, reconstructed, user_ids)
            
            # åˆ¤æ–­VQ-VAEè´¨é‡
            vqvae_quality = self._evaluate_vqvae_quality(mse_loss.item(), usage_ratio, psnr.item())
            return vqvae_quality
    
    def diagnose_transformer(self):
        """è¯Šæ–­Transformerç»„ä»¶"""
        print("\n" + "="*60)
        print("ğŸ” Transformerç»„ä»¶è¯Šæ–­")
        print("="*60)
        
        if self.transformer_model is None or self.vqvae_model is None or self.test_data is None:
            print("âŒ æ— æ³•è¿›è¡ŒTransformerè¯Šæ–­ï¼šæ¨¡å‹æˆ–æ•°æ®æœªåŠ è½½")
            return False
        
        images = self.test_data['image'].to(self.device)
        user_ids = self.test_data['user_id'].to(self.device)
        
        with torch.no_grad():
            # 1. è·å–çœŸå®tokens
            print("1ï¸âƒ£ è·å–çœŸå®tokens...")
            encoded = self.vqvae_model.encode(images)
            if hasattr(encoded, 'latents'):
                latents = encoded.latents
            else:
                latents = encoded
            
            quantized_output = self.vqvae_model.quantize(latents)
            if hasattr(quantized_output, 'indices'):
                real_tokens = quantized_output.indices.flatten(1)  # [B, H*W]
            else:
                print("âŒ æ— æ³•è·å–tokenç´¢å¼•")
                return False
            
            print(f"   çœŸå®tokenså½¢çŠ¶: {real_tokens.shape}")
            print(f"   tokensèŒƒå›´: [{real_tokens.min().item()}, {real_tokens.max().item()}]")
            
            # 2. Transformerç”Ÿæˆæµ‹è¯•
            print("2ï¸âƒ£ Transformerç”Ÿæˆæµ‹è¯•...")
            generated_tokens = self._generate_tokens(user_ids, max_length=real_tokens.shape[1])
            
            if generated_tokens is not None:
                print(f"   ç”Ÿæˆtokenså½¢çŠ¶: {generated_tokens.shape}")
                print(f"   ç”ŸæˆtokensèŒƒå›´: [{generated_tokens.min().item()}, {generated_tokens.max().item()}]")
                
                # 3. Tokenåˆ†å¸ƒåˆ†æ
                print("3ï¸âƒ£ Tokenåˆ†å¸ƒåˆ†æ...")
                self._analyze_token_distribution(real_tokens, generated_tokens)
                
                # 4. ç”¨æˆ·æ¡ä»¶æµ‹è¯•
                print("4ï¸âƒ£ ç”¨æˆ·æ¡ä»¶æµ‹è¯•...")
                self._test_user_conditioning(user_ids)
                
                # 5. ç”Ÿæˆå›¾åƒè´¨é‡æµ‹è¯•
                print("5ï¸âƒ£ ç”Ÿæˆå›¾åƒè´¨é‡æµ‹è¯•...")
                generated_images = self._decode_tokens(generated_tokens)
                if generated_images is not None:
                    self._save_transformer_diagnosis(images, generated_images, user_ids)
                    
                    # åˆ¤æ–­Transformerè´¨é‡
                    transformer_quality = self._evaluate_transformer_quality(real_tokens, generated_tokens, images, generated_images)
                    return transformer_quality
        
        return False
    
    def _generate_tokens(self, user_ids, max_length=1024):
        """ç”Ÿæˆtokens"""
        try:
            batch_size = user_ids.shape[0]
            generated = torch.zeros((batch_size, 1), dtype=torch.long, device=self.device)
            
            for i in range(max_length):
                outputs = self.transformer_model(
                    input_ids=generated,
                    user_ids=user_ids
                )
                
                next_token_logits = outputs.logits[:, -1, :]
                next_token = torch.multinomial(torch.softmax(next_token_logits, dim=-1), 1)
                generated = torch.cat([generated, next_token], dim=1)
                
                if i % 100 == 0:
                    print(f"   ç”Ÿæˆè¿›åº¦: {i}/{max_length}")
            
            return generated[:, 1:]  # ç§»é™¤èµ·å§‹token
            
        except Exception as e:
            print(f"âŒ Tokenç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _decode_tokens(self, tokens):
        """è§£ç tokensä¸ºå›¾åƒ"""
        try:
            # é‡å¡‘ä¸º2D
            batch_size = tokens.shape[0]
            tokens_2d = tokens.view(batch_size, 32, 32)  # å‡è®¾32x32
            
            # åˆ›å»ºé‡åŒ–ç‰¹å¾
            quantized_features = self.vqvae_model.quantize.get_codebook_entry(
                tokens_2d.flatten(), tokens_2d.shape
            )
            
            # è§£ç 
            decoded = self.vqvae_model.decode(quantized_features)
            if hasattr(decoded, 'sample'):
                return decoded.sample
            else:
                return decoded
                
        except Exception as e:
            print(f"âŒ Tokenè§£ç å¤±è´¥: {e}")
            return None
    
    def _analyze_token_distribution(self, real_tokens, generated_tokens):
        """åˆ†ætokenåˆ†å¸ƒ"""
        real_unique = torch.unique(real_tokens)
        gen_unique = torch.unique(generated_tokens)
        
        print(f"   çœŸå®tokenså”¯ä¸€å€¼: {len(real_unique)}")
        print(f"   ç”Ÿæˆtokenså”¯ä¸€å€¼: {len(gen_unique)}")
        
        # è®¡ç®—åˆ†å¸ƒå·®å¼‚
        real_hist = torch.histc(real_tokens.float(), bins=1024, min=0, max=1023)
        gen_hist = torch.histc(generated_tokens.float(), bins=1024, min=0, max=1023)
        
        # å½’ä¸€åŒ–
        real_hist = real_hist / real_hist.sum()
        gen_hist = gen_hist / gen_hist.sum()
        
        # KLæ•£åº¦
        kl_div = F.kl_div(gen_hist.log(), real_hist, reduction='sum')
        print(f"   åˆ†å¸ƒKLæ•£åº¦: {kl_div.item():.4f}")
    
    def _test_user_conditioning(self, user_ids):
        """æµ‹è¯•ç”¨æˆ·æ¡ä»¶"""
        if len(torch.unique(user_ids)) < 2:
            print("   âš ï¸ éœ€è¦è‡³å°‘2ä¸ªä¸åŒç”¨æˆ·è¿›è¡Œæµ‹è¯•")
            return
        
        # ç”Ÿæˆä¸åŒç”¨æˆ·çš„tokens
        user1_id = user_ids[:1]
        user2_id = user_ids[1:2] if len(user_ids) > 1 else user_ids[:1]
        
        tokens1 = self._generate_tokens(user1_id, max_length=100)
        tokens2 = self._generate_tokens(user2_id, max_length=100)
        
        if tokens1 is not None and tokens2 is not None:
            # è®¡ç®—å·®å¼‚
            diff_ratio = (tokens1 != tokens2).float().mean()
            print(f"   ç”¨æˆ·é—´tokenå·®å¼‚ç‡: {diff_ratio.item():.2%}")
            
            if diff_ratio < 0.1:
                print("   âš ï¸ è­¦å‘Šï¼šç”¨æˆ·é—´å·®å¼‚è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¨¡å¼å´©æºƒ")
    
    def _evaluate_vqvae_quality(self, mse_loss, usage_ratio, psnr):
        """è¯„ä¼°VQ-VAEè´¨é‡"""
        print("\nğŸ“Š VQ-VAEè´¨é‡è¯„ä¼°:")
        
        issues = []
        if mse_loss > 0.1:
            issues.append("é‡å»ºè¯¯å·®è¿‡é«˜")
        if usage_ratio < 0.1:
            issues.append("ç æœ¬ä½¿ç”¨ç‡è¿‡ä½")
        if psnr < 15:
            issues.append("PSNRè¿‡ä½")
        
        if not issues:
            print("   âœ… VQ-VAEè´¨é‡è‰¯å¥½")
            return True
        else:
            print("   âŒ VQ-VAEå­˜åœ¨é—®é¢˜:")
            for issue in issues:
                print(f"      - {issue}")
            return False
    
    def _evaluate_transformer_quality(self, real_tokens, generated_tokens, real_images, generated_images):
        """è¯„ä¼°Transformerè´¨é‡"""
        print("\nğŸ“Š Transformerè´¨é‡è¯„ä¼°:")
        
        issues = []
        
        # Tokenå¤šæ ·æ€§æ£€æŸ¥
        gen_unique_ratio = len(torch.unique(generated_tokens)) / 1024
        if gen_unique_ratio < 0.05:
            issues.append("ç”Ÿæˆtokenå¤šæ ·æ€§ä¸è¶³")
        
        # å›¾åƒè´¨é‡æ£€æŸ¥
        if generated_images is not None:
            img_mse = F.mse_loss(generated_images, real_images)
            if img_mse > 0.5:
                issues.append("ç”Ÿæˆå›¾åƒè´¨é‡å·®")
        
        if not issues:
            print("   âœ… Transformerè´¨é‡è‰¯å¥½")
            return True
        else:
            print("   âŒ Transformerå­˜åœ¨é—®é¢˜:")
            for issue in issues:
                print(f"      - {issue}")
            return False
    
    def _save_vqvae_diagnosis(self, original, reconstructed, user_ids):
        """ä¿å­˜VQ-VAEè¯Šæ–­ç»“æœ"""
        try:
            fig, axes = plt.subplots(2, 4, figsize=(16, 8))
            
            for i in range(min(4, original.shape[0])):
                # åŸå§‹å›¾åƒ
                axes[0, i].imshow(original[i, 0].cpu().numpy(), cmap='viridis')
                axes[0, i].set_title(f'åŸå§‹ (User {user_ids[i].item()})')
                axes[0, i].axis('off')
                
                # é‡å»ºå›¾åƒ
                axes[1, i].imshow(reconstructed[i, 0].cpu().numpy(), cmap='viridis')
                axes[1, i].set_title(f'é‡å»º (User {user_ids[i].item()})')
                axes[1, i].axis('off')
            
            plt.tight_layout()
            plt.savefig('vqvae_diagnosis.png', dpi=150, bbox_inches='tight')
            plt.close()
            print("ğŸ’¾ VQ-VAEè¯Šæ–­å›¾åƒå·²ä¿å­˜: vqvae_diagnosis.png")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜VQ-VAEè¯Šæ–­å›¾åƒå¤±è´¥: {e}")
    
    def _save_transformer_diagnosis(self, original, generated, user_ids):
        """ä¿å­˜Transformerè¯Šæ–­ç»“æœ"""
        try:
            fig, axes = plt.subplots(2, 4, figsize=(16, 8))
            
            for i in range(min(4, original.shape[0])):
                # åŸå§‹å›¾åƒ
                axes[0, i].imshow(original[i, 0].cpu().numpy(), cmap='viridis')
                axes[0, i].set_title(f'çœŸå® (User {user_ids[i].item()})')
                axes[0, i].axis('off')
                
                # ç”Ÿæˆå›¾åƒ
                axes[1, i].imshow(generated[i, 0].cpu().numpy(), cmap='viridis')
                axes[1, i].set_title(f'ç”Ÿæˆ (User {user_ids[i].item()})')
                axes[1, i].axis('off')
            
            plt.tight_layout()
            plt.savefig('transformer_diagnosis.png', dpi=150, bbox_inches='tight')
            plt.close()
            print("ğŸ’¾ Transformerè¯Šæ–­å›¾åƒå·²ä¿å­˜: transformer_diagnosis.png")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜Transformerè¯Šæ–­å›¾åƒå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç»„ä»¶è¯Šæ–­")
    parser.add_argument("--vqvae_path", type=str, default="models/vqvae_model", help="VQ-VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--transformer_path", type=str, help="Transformeræ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_dir", type=str, default="data/processed", help="æ•°æ®ç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯Šæ–­å™¨
    diagnostic = ComponentDiagnostic(
        vqvae_path=args.vqvae_path,
        transformer_path=args.transformer_path,
        data_dir=args.data_dir
    )
    
    # è¯Šæ–­VQ-VAE
    vqvae_ok = diagnostic.diagnose_vqvae()
    
    # è¯Šæ–­Transformerï¼ˆå¦‚æœæä¾›äº†è·¯å¾„ï¼‰
    transformer_ok = True
    if args.transformer_path:
        transformer_ok = diagnostic.diagnose_transformer()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ¯ è¯Šæ–­æ€»ç»“")
    print("="*60)
    
    if vqvae_ok and transformer_ok:
        print("âœ… ä¸¤ä¸ªç»„ä»¶éƒ½æ­£å¸¸ï¼Œé—®é¢˜å¯èƒ½åœ¨è®­ç»ƒç­–ç•¥æˆ–å‚æ•°è®¾ç½®")
    elif not vqvae_ok and transformer_ok:
        print("âŒ VQ-VAEå­˜åœ¨é—®é¢˜ï¼Œå»ºè®®é‡æ–°è®­ç»ƒVQ-VAE")
    elif vqvae_ok and not transformer_ok:
        print("âŒ Transformerå­˜åœ¨é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨æ”¹è¿›çš„è®­ç»ƒè„šæœ¬")
    else:
        print("âŒ ä¸¤ä¸ªç»„ä»¶éƒ½å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®ä»VQ-VAEå¼€å§‹é‡æ–°è®­ç»ƒ")

if __name__ == "__main__":
    main()
