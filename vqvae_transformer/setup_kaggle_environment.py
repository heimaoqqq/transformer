#!/usr/bin/env python3
"""
Kaggleç¯å¢ƒä¸€é”®é…ç½®è„šæœ¬
æ•´åˆæ‰€æœ‰ç¯å¢ƒé…ç½®åŠŸèƒ½ï¼šGPUä¼˜åŒ–ã€ä¾èµ–å®‰è£…ã€å…¼å®¹æ€§æ£€æŸ¥
"""

import subprocess
import sys
import os
import importlib
import time

def run_command(cmd, description="", timeout=120):
    """è¿è¡Œå‘½ä»¤"""
    print(f"ğŸ”„ {description}")
    print(f"   å‘½ä»¤: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
        if result.returncode == 0:
            print(f"âœ… {description} æˆåŠŸ")
            return True
        else:
            print(f"âŒ {description} å¤±è´¥")
            if result.stderr.strip():
                print(f"   é”™è¯¯: {result.stderr.strip()}")
            return False
    except subprocess.TimeoutExpired:
        print(f"â° {description} è¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ {description} å¼‚å¸¸: {e}")
        return False

def check_kaggle_environment():
    """æ£€æŸ¥Kaggleç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥Kaggleç¯å¢ƒ...")
    
    kaggle_indicators = [
        ("/kaggle", "Kaggleç›®å½•"),
        ("/opt/conda", "Condaç¯å¢ƒ"),
    ]
    
    is_kaggle = False
    for indicator, desc in kaggle_indicators:
        if os.path.exists(indicator):
            print(f"âœ… æ£€æµ‹åˆ° {desc}: {indicator}")
            is_kaggle = True
    
    if is_kaggle:
        print("âœ… ç¡®è®¤åœ¨Kaggleç¯å¢ƒä¸­")
    else:
        print("âš ï¸ å¯èƒ½ä¸åœ¨Kaggleç¯å¢ƒä¸­ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
    
    return True

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("\nğŸ” æ£€æŸ¥GPUç¯å¢ƒ...")
    
    # æ£€æŸ¥nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smiå¯ç”¨")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'CUDA Version:' in line:
                    cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                    print(f"âœ… CUDAç‰ˆæœ¬: {cuda_version}")
                    return True, cuda_version
        else:
            print("âŒ nvidia-smiå¤±è´¥")
            return False, None
    except Exception as e:
        print(f"âŒ nvidia-smiå¼‚å¸¸: {e}")
        return False, None

def clean_environment():
    """æ¸…ç†ç¯å¢ƒ"""
    print("\nğŸ—‘ï¸ æ¸…ç†ç¯å¢ƒ...")
    
    # æ¸…ç†Pythonæ¨¡å—ç¼“å­˜
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub',
            'accelerate', 'tokenizers', 'safetensors'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    print(f"âœ… æ¸…ç†äº† {len(modules_to_clear)} ä¸ªPythonæ¨¡å—")
    
    # å¸è½½å¯èƒ½å†²çªçš„åŒ…
    packages_to_remove = [
        "torch", "torchvision", "torchaudio",
        "transformers", "diffusers", "accelerate",
        "huggingface_hub", "tokenizers", "safetensors"
    ]
    
    for package in packages_to_remove:
        run_command(f"pip uninstall {package} -y", f"å¸è½½ {package}")
    
    # æ¸…ç†pipç¼“å­˜
    run_command("pip cache purge", "æ¸…ç†pipç¼“å­˜")
    
    return True

def install_pytorch_gpu():
    """å®‰è£…GPUç‰ˆæœ¬PyTorch"""
    print("\nğŸ”¥ å®‰è£…GPUç‰ˆæœ¬PyTorch...")
    
    # é’ˆå¯¹Kaggle GPUç¯å¢ƒçš„PyTorchå®‰è£…ç­–ç•¥
    pytorch_options = [
        "pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 --index-url https://download.pytorch.org/whl/cu116",
        "pip install torch torchvision torchaudio --upgrade",
        "pip install torch==2.0.1 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
        "pip install torch==2.0.1 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu",
    ]
    
    for i, cmd in enumerate(pytorch_options, 1):
        print(f"\nå°è¯•PyTorchæ–¹æ¡ˆ {i}...")
        if run_command(cmd, f"PyTorchæ–¹æ¡ˆ {i}"):
            print(f"âœ… PyTorchæ–¹æ¡ˆ {i} æˆåŠŸ")
            return True
    
    print("âŒ æ‰€æœ‰PyTorchå®‰è£…æ–¹æ¡ˆéƒ½å¤±è´¥")
    return False

def install_huggingface_stack():
    """å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ"""
    print("\nğŸ¤— å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ...")
    
    # å®Œå…¨æŒ‰ç…§diffusers 0.24.0å®˜æ–¹è¦æ±‚
    hf_packages = [
        ("huggingface_hub>=0.19.4", "HuggingFace Hub (diffuserså®˜æ–¹è¦æ±‚)"),
        ("tokenizers>=0.11.1,!=0.11.3", "Tokenizers (diffuserså®˜æ–¹è¦æ±‚)"),
        ("safetensors>=0.3.1", "SafeTensors (diffuserså®˜æ–¹è¦æ±‚)"),
        ("transformers>=4.25.1", "Transformers (diffuserså®˜æ–¹è¦æ±‚)"),
        ("accelerate>=0.11.0", "Accelerate (diffuserså®˜æ–¹è¦æ±‚)"),
        ("diffusers==0.24.0", "Diffusers (ç›®æ ‡ç‰ˆæœ¬)"),
    ]
    
    success_count = 0
    for package, description in hf_packages:
        if run_command(f"pip install '{package}'", f"å®‰è£… {description}"):
            success_count += 1
    
    print(f"\nğŸ“Š HuggingFaceåŒ…å®‰è£…ç»“æœ: {success_count}/{len(hf_packages)} æˆåŠŸ")
    return success_count >= len(hf_packages) - 1  # å…è®¸1ä¸ªå¤±è´¥

def install_other_dependencies():
    """å®‰è£…å…¶ä»–ä¾èµ–"""
    print("\nğŸ“š å®‰è£…å…¶ä»–ä¾èµ–...")
    
    other_deps = [
        "numpy==1.26.4",
        "scipy==1.11.4", 
        "scikit-learn==1.3.0",
        "matplotlib==3.7.2",
        "opencv-python==4.8.1.78",
        "einops==0.7.0",
        "lpips==0.1.4",
    ]
    
    for dep in other_deps:
        run_command(f"pip install {dep}", f"å®‰è£… {dep}")
    
    return True

def test_installation():
    """æµ‹è¯•å®‰è£…ç»“æœ"""
    print("\nğŸ§ª æµ‹è¯•å®‰è£…ç»“æœ...")
    
    # æ¸…ç†æ¨¡å—ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°å¯¼å…¥
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    # æµ‹è¯•å…³é”®å¯¼å…¥
    tests = [
        ("torch", "PyTorch"),
        ("transformers", "Transformers"),
        ("diffusers", "Diffusers"),
        ("huggingface_hub", "HuggingFace Hub"),
        ("accelerate", "Accelerate"),
    ]
    
    success_count = 0
    
    for module_name, display_name in tests:
        try:
            module = importlib.import_module(module_name)
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {display_name}: {version}")
            success_count += 1
        except Exception as e:
            print(f"âŒ {display_name}: å¯¼å…¥å¤±è´¥ - {e}")
    
    # æµ‹è¯•GPU
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… CUDAå¯ç”¨: {torch.version.cuda}")
            print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
            
            # æµ‹è¯•GPUæ“ä½œ
            try:
                device = torch.device('cuda:0')
                test_tensor = torch.randn(10, device=device)
                result = test_tensor + 1
                print("âœ… GPUæ“ä½œæ­£å¸¸")
            except Exception as e:
                print(f"âš ï¸ GPUæ“ä½œå¤±è´¥: {e}")
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUç‰ˆæœ¬")
    except Exception as e:
        print(f"âŒ PyTorchæµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•ä¸‹è½½API
    try:
        from huggingface_hub import hf_hub_download
        print("âœ… hf_hub_download: å¯ç”¨ (diffusersä½¿ç”¨çš„API)")
    except Exception as e:
        print(f"âŒ hf_hub_download: ä¸å¯ç”¨ - {e}")
    
    # æµ‹è¯•VQModel
    try:
        from diffusers.models.autoencoders.vq_model import VQModel
        print("âœ… VQModel: å¯ç”¨ (æ–°ç‰ˆAPI)")
    except ImportError:
        try:
            from diffusers.models.vq_model import VQModel
            print("âœ… VQModel: å¯ç”¨ (æ—§ç‰ˆAPI)")
        except ImportError:
            try:
                from diffusers import VQModel
                print("âœ… VQModel: å¯ç”¨ (ç›´æ¥å¯¼å…¥)")
            except ImportError:
                print("âŒ VQModel: æ‰€æœ‰å¯¼å…¥è·¯å¾„éƒ½å¤±è´¥")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(tests)} åŸºç¡€åŒ…æˆåŠŸ")
    
    return success_count >= len(tests) - 1  # å…è®¸1ä¸ªå¤±è´¥

def get_gpu_config():
    """è·å–GPUè®­ç»ƒé…ç½®"""
    try:
        import torch
        if not torch.cuda.is_available():
            return {"device": "cpu", "batch_size": 8}
        
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ¯ æ£€æµ‹åˆ°GPU: {gpu_name}")
        
        # æ ¹æ®GPUç±»å‹ä¼˜åŒ–é…ç½®
        if "T4" in gpu_name:
            config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
            print("ğŸ¯ Tesla T4é…ç½®ï¼šbatch_size=16, æ··åˆç²¾åº¦=True")
        elif "P100" in gpu_name:
            config = {"device": "cuda", "batch_size": 12, "mixed_precision": False}
            print("ğŸ¯ Tesla P100é…ç½®ï¼šbatch_size=12, æ··åˆç²¾åº¦=False")
        elif "V100" in gpu_name:
            config = {"device": "cuda", "batch_size": 32, "mixed_precision": True}
            print("ğŸ¯ Tesla V100é…ç½®ï¼šbatch_size=32, æ··åˆç²¾åº¦=True")
        else:
            config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
            print("ğŸ¯ é€šç”¨GPUé…ç½®ï¼šbatch_size=16, æ··åˆç²¾åº¦=True")
        
        return config
    except:
        return {"device": "cpu", "batch_size": 8}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Kaggleç¯å¢ƒä¸€é”®é…ç½®è„šæœ¬")
    print("=" * 50)
    print("ğŸ¯ GPUä¼˜åŒ– + ä¾èµ–å®‰è£… + å…¼å®¹æ€§æ£€æŸ¥")
    
    # æ‰§è¡Œé…ç½®æµç¨‹
    steps = [
        ("æ£€æŸ¥Kaggleç¯å¢ƒ", check_kaggle_environment),
        ("æ¸…ç†ç¯å¢ƒ", clean_environment),
        ("å®‰è£…PyTorch GPUç‰ˆæœ¬", install_pytorch_gpu),
        ("å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ", install_huggingface_stack),
        ("å®‰è£…å…¶ä»–ä¾èµ–", install_other_dependencies),
        ("æµ‹è¯•å®‰è£…ç»“æœ", test_installation),
    ]
    
    for step_name, step_func in steps:
        print(f"\n{'='*20} {step_name} {'='*20}")
        if not step_func():
            print(f"âŒ {step_name} å¤±è´¥")
            if step_name == "æµ‹è¯•å®‰è£…ç»“æœ":
                print("âš ï¸ éƒ¨åˆ†ç»„ä»¶å¯èƒ½ä»æœ‰é—®é¢˜ï¼Œä½†å¯ä»¥å°è¯•ä½¿ç”¨")
            else:
                print("ğŸ’¥ å…³é”®æ­¥éª¤å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
        else:
            print(f"âœ… {step_name} æˆåŠŸ")
    
    # è·å–GPUé…ç½®
    print(f"\n{'='*20} GPUè®­ç»ƒé…ç½® {'='*20}")
    gpu_config = get_gpu_config()
    print(f"ğŸ“‹ æ¨èé…ç½®: {gpu_config}")
    
    print("\nğŸ‰ Kaggleç¯å¢ƒé…ç½®å®Œæˆ!")
    print("âœ… æ‰€æœ‰ç»„ä»¶å·²å®‰è£…å¹¶éªŒè¯")
    print("\nğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒ:")
    print(f"   python train_main.py --data_dir /kaggle/input/dataset --device {gpu_config['device']}")
    print(f"   æ¨èbatch_size: {gpu_config['batch_size']}")
    
    return True

if __name__ == "__main__":
    main()
