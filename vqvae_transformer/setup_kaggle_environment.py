#!/usr/bin/env python3
"""
Kaggle环境一键配置脚本
整合所有环境配置功能：GPU优化、依赖安装、兼容性检查
"""

import subprocess
import sys
import os
import importlib
import time

def run_command(cmd, description="", timeout=120):
    """运行命令"""
    print(f"🔄 {description}")
    print(f"   命令: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
        if result.returncode == 0:
            print(f"✅ {description} 成功")
            return True
        else:
            print(f"❌ {description} 失败")
            if result.stderr.strip():
                print(f"   错误: {result.stderr.strip()}")
            return False
    except subprocess.TimeoutExpired:
        print(f"⏰ {description} 超时")
        return False
    except Exception as e:
        print(f"❌ {description} 异常: {e}")
        return False

def check_kaggle_environment():
    """检查Kaggle环境"""
    print("🔍 检查Kaggle环境...")
    
    kaggle_indicators = [
        ("/kaggle", "Kaggle目录"),
        ("/opt/conda", "Conda环境"),
    ]
    
    is_kaggle = False
    for indicator, desc in kaggle_indicators:
        if os.path.exists(indicator):
            print(f"✅ 检测到 {desc}: {indicator}")
            is_kaggle = True
    
    if is_kaggle:
        print("✅ 确认在Kaggle环境中")
    else:
        print("⚠️ 可能不在Kaggle环境中，但继续执行")
    
    return True

def check_gpu_environment():
    """检查GPU环境"""
    print("\n🔍 检查GPU环境...")
    
    # 检查nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("✅ nvidia-smi可用")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'CUDA Version:' in line:
                    cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                    print(f"✅ CUDA版本: {cuda_version}")
                    return True, cuda_version
        else:
            print("❌ nvidia-smi失败")
            return False, None
    except Exception as e:
        print(f"❌ nvidia-smi异常: {e}")
        return False, None

def clean_environment():
    """清理环境"""
    print("\n🗑️ 清理环境...")
    
    # 清理Python模块缓存
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub',
            'accelerate', 'tokenizers', 'safetensors'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    print(f"✅ 清理了 {len(modules_to_clear)} 个Python模块")
    
    # 卸载可能冲突的包
    packages_to_remove = [
        "torch", "torchvision", "torchaudio",
        "transformers", "diffusers", "accelerate",
        "huggingface_hub", "tokenizers", "safetensors"
    ]
    
    for package in packages_to_remove:
        run_command(f"pip uninstall {package} -y", f"卸载 {package}")
    
    # 清理pip缓存
    run_command("pip cache purge", "清理pip缓存")
    
    return True

def install_pytorch_gpu():
    """安装GPU版本PyTorch"""
    print("\n🔥 安装GPU版本PyTorch...")
    
    # 针对Kaggle GPU环境的PyTorch安装策略
    pytorch_options = [
        "pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 --index-url https://download.pytorch.org/whl/cu116",
        "pip install torch torchvision torchaudio --upgrade",
        "pip install torch==2.0.1 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118",
        "pip install torch==2.0.1 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu",
    ]
    
    for i, cmd in enumerate(pytorch_options, 1):
        print(f"\n尝试PyTorch方案 {i}...")
        if run_command(cmd, f"PyTorch方案 {i}"):
            print(f"✅ PyTorch方案 {i} 成功")
            return True
    
    print("❌ 所有PyTorch安装方案都失败")
    return False

def install_huggingface_stack():
    """安装HuggingFace技术栈"""
    print("\n🤗 安装HuggingFace技术栈...")
    
    # 完全按照diffusers 0.24.0官方要求
    hf_packages = [
        ("huggingface_hub>=0.19.4", "HuggingFace Hub (diffusers官方要求)"),
        ("tokenizers>=0.11.1,!=0.11.3", "Tokenizers (diffusers官方要求)"),
        ("safetensors>=0.3.1", "SafeTensors (diffusers官方要求)"),
        ("transformers>=4.25.1", "Transformers (diffusers官方要求)"),
        ("accelerate>=0.11.0", "Accelerate (diffusers官方要求)"),
        ("diffusers==0.24.0", "Diffusers (目标版本)"),
    ]
    
    success_count = 0
    for package, description in hf_packages:
        if run_command(f"pip install '{package}'", f"安装 {description}"):
            success_count += 1
    
    print(f"\n📊 HuggingFace包安装结果: {success_count}/{len(hf_packages)} 成功")
    return success_count >= len(hf_packages) - 1  # 允许1个失败

def install_other_dependencies():
    """安装其他依赖"""
    print("\n📚 安装其他依赖...")
    
    other_deps = [
        "numpy==1.26.4",
        "scipy==1.11.4", 
        "scikit-learn==1.3.0",
        "matplotlib==3.7.2",
        "opencv-python==4.8.1.78",
        "einops==0.7.0",
        "lpips==0.1.4",
    ]
    
    for dep in other_deps:
        run_command(f"pip install {dep}", f"安装 {dep}")
    
    return True

def test_installation():
    """测试安装结果"""
    print("\n🧪 测试安装结果...")
    
    # 清理模块缓存，强制重新导入
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    # 测试关键导入
    tests = [
        ("torch", "PyTorch"),
        ("transformers", "Transformers"),
        ("diffusers", "Diffusers"),
        ("huggingface_hub", "HuggingFace Hub"),
        ("accelerate", "Accelerate"),
    ]
    
    success_count = 0
    
    for module_name, display_name in tests:
        try:
            module = importlib.import_module(module_name)
            version = getattr(module, '__version__', 'unknown')
            print(f"✅ {display_name}: {version}")
            success_count += 1
        except Exception as e:
            print(f"❌ {display_name}: 导入失败 - {e}")
    
    # 测试GPU
    try:
        import torch
        if torch.cuda.is_available():
            print(f"✅ CUDA可用: {torch.version.cuda}")
            print(f"✅ GPU数量: {torch.cuda.device_count()}")
            
            # 测试GPU操作
            try:
                device = torch.device('cuda:0')
                test_tensor = torch.randn(10, device=device)
                result = test_tensor + 1
                print("✅ GPU操作正常")
            except Exception as e:
                print(f"⚠️ GPU操作失败: {e}")
        else:
            print("⚠️ CUDA不可用，使用CPU版本")
    except Exception as e:
        print(f"❌ PyTorch测试失败: {e}")
    
    # 测试下载API
    try:
        from huggingface_hub import hf_hub_download
        print("✅ hf_hub_download: 可用 (diffusers使用的API)")
    except Exception as e:
        print(f"❌ hf_hub_download: 不可用 - {e}")
    
    # 测试VQModel
    try:
        from diffusers.models.autoencoders.vq_model import VQModel
        print("✅ VQModel: 可用 (新版API)")
    except ImportError:
        try:
            from diffusers.models.vq_model import VQModel
            print("✅ VQModel: 可用 (旧版API)")
        except ImportError:
            try:
                from diffusers import VQModel
                print("✅ VQModel: 可用 (直接导入)")
            except ImportError:
                print("❌ VQModel: 所有导入路径都失败")
    
    print(f"\n📊 测试结果: {success_count}/{len(tests)} 基础包成功")
    
    return success_count >= len(tests) - 1  # 允许1个失败

def get_gpu_config():
    """获取GPU训练配置"""
    try:
        import torch
        if not torch.cuda.is_available():
            return {"device": "cpu", "batch_size": 8}
        
        gpu_name = torch.cuda.get_device_name(0)
        print(f"🎯 检测到GPU: {gpu_name}")
        
        # 根据GPU类型优化配置
        if "T4" in gpu_name:
            config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
            print("🎯 Tesla T4配置：batch_size=16, 混合精度=True")
        elif "P100" in gpu_name:
            config = {"device": "cuda", "batch_size": 12, "mixed_precision": False}
            print("🎯 Tesla P100配置：batch_size=12, 混合精度=False")
        elif "V100" in gpu_name:
            config = {"device": "cuda", "batch_size": 32, "mixed_precision": True}
            print("🎯 Tesla V100配置：batch_size=32, 混合精度=True")
        else:
            config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
            print("🎯 通用GPU配置：batch_size=16, 混合精度=True")
        
        return config
    except:
        return {"device": "cpu", "batch_size": 8}

def main():
    """主函数"""
    print("🔧 Kaggle环境一键配置脚本")
    print("=" * 50)
    print("🎯 GPU优化 + 依赖安装 + 兼容性检查")
    
    # 执行配置流程
    steps = [
        ("检查Kaggle环境", check_kaggle_environment),
        ("清理环境", clean_environment),
        ("安装PyTorch GPU版本", install_pytorch_gpu),
        ("安装HuggingFace技术栈", install_huggingface_stack),
        ("安装其他依赖", install_other_dependencies),
        ("测试安装结果", test_installation),
    ]
    
    for step_name, step_func in steps:
        print(f"\n{'='*20} {step_name} {'='*20}")
        if not step_func():
            print(f"❌ {step_name} 失败")
            if step_name == "测试安装结果":
                print("⚠️ 部分组件可能仍有问题，但可以尝试使用")
            else:
                print("💥 关键步骤失败，但继续执行...")
        else:
            print(f"✅ {step_name} 成功")
    
    # 获取GPU配置
    print(f"\n{'='*20} GPU训练配置 {'='*20}")
    gpu_config = get_gpu_config()
    print(f"📋 推荐配置: {gpu_config}")
    
    print("\n🎉 Kaggle环境配置完成!")
    print("✅ 所有组件已安装并验证")
    print("\n🚀 现在可以开始训练:")
    print(f"   python train_main.py --data_dir /kaggle/input/dataset --device {gpu_config['device']}")
    print(f"   推荐batch_size: {gpu_config['batch_size']}")
    
    return True

if __name__ == "__main__":
    main()
