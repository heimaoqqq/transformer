#!/usr/bin/env python3
"""
Kaggleç¯å¢ƒä¸€é”®é…ç½®è„šæœ¬
æ•´åˆæ‰€æœ‰ç¯å¢ƒé…ç½®åŠŸèƒ½ï¼šGPUä¼˜åŒ–ã€ä¾èµ–å®‰è£…ã€å…¼å®¹æ€§æ£€æŸ¥
"""

import subprocess
import sys
import os
import importlib
import time

def run_command(cmd, description="", timeout=120):
    """è¿è¡Œå‘½ä»¤"""
    print(f"ğŸ”„ {description}")
    print(f"   å‘½ä»¤: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
        if result.returncode == 0:
            print(f"âœ… {description} æˆåŠŸ")
            return True
        else:
            print(f"âŒ {description} å¤±è´¥")
            if result.stderr.strip():
                print(f"   é”™è¯¯: {result.stderr.strip()}")
            return False
    except subprocess.TimeoutExpired:
        print(f"â° {description} è¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ {description} å¼‚å¸¸: {e}")
        return False

def check_kaggle_environment():
    """æ£€æŸ¥Kaggleç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥Kaggleç¯å¢ƒ...")
    
    kaggle_indicators = [
        ("/kaggle", "Kaggleç›®å½•"),
        ("/opt/conda", "Condaç¯å¢ƒ"),
    ]
    
    is_kaggle = False
    for indicator, desc in kaggle_indicators:
        if os.path.exists(indicator):
            print(f"âœ… æ£€æµ‹åˆ° {desc}: {indicator}")
            is_kaggle = True
    
    if is_kaggle:
        print("âœ… ç¡®è®¤åœ¨Kaggleç¯å¢ƒä¸­")
    else:
        print("âš ï¸ å¯èƒ½ä¸åœ¨Kaggleç¯å¢ƒä¸­ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
    
    return True

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("\nğŸ” æ£€æŸ¥GPUç¯å¢ƒ...")
    
    # æ£€æŸ¥nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smiå¯ç”¨")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'CUDA Version:' in line:
                    cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                    print(f"âœ… CUDAç‰ˆæœ¬: {cuda_version}")
                    return True, cuda_version
        else:
            print("âŒ nvidia-smiå¤±è´¥")
            return False, None
    except Exception as e:
        print(f"âŒ nvidia-smiå¼‚å¸¸: {e}")
        return False, None

def clean_environment():
    """æ¸…ç†ç¯å¢ƒ"""
    print("\nğŸ—‘ï¸ æ¸…ç†ç¯å¢ƒ...")
    
    # æ¸…ç†Pythonæ¨¡å—ç¼“å­˜
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub',
            'accelerate', 'tokenizers', 'safetensors'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    print(f"âœ… æ¸…ç†äº† {len(modules_to_clear)} ä¸ªPythonæ¨¡å—")
    
    # å¸è½½å¯èƒ½å†²çªçš„åŒ…
    packages_to_remove = [
        "torch", "torchvision", "torchaudio",
        "transformers", "diffusers", "accelerate",
        "huggingface_hub", "tokenizers", "safetensors"
    ]
    
    for package in packages_to_remove:
        run_command(f"pip uninstall {package} -y", f"å¸è½½ {package}")
    
    # æ¸…ç†pipç¼“å­˜
    run_command("pip cache purge", "æ¸…ç†pipç¼“å­˜")
    
    return True

def install_pytorch_gpu():
    """å®‰è£…GPUç‰ˆæœ¬PyTorch"""
    print("\nğŸ”¥ å®‰è£…GPUç‰ˆæœ¬PyTorch...")
    
    # é’ˆå¯¹Kaggle GPUç¯å¢ƒçš„PyTorchå®‰è£…ç­–ç•¥ (ä¿®å¤NCCLå…¼å®¹æ€§é—®é¢˜)
    pytorch_options = [
        # æ–¹æ¡ˆ1: ä½¿ç”¨CPUç‰ˆæœ¬é¿å…CUDAå…¼å®¹æ€§é—®é¢˜
        "pip install torch==2.0.1 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu",

        # æ–¹æ¡ˆ2: è¾ƒæ—§çš„ç¨³å®šCUDAç‰ˆæœ¬
        "pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 --index-url https://download.pytorch.org/whl/cu116",

        # æ–¹æ¡ˆ3: å°è¯•é¢„è£…ç‰ˆæœ¬ä½†å¯èƒ½æœ‰NCCLé—®é¢˜
        "pip install torch torchvision torchaudio --upgrade",

        # æ–¹æ¡ˆ4: å…¶ä»–CUDAç‰ˆæœ¬
        "pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 --index-url https://download.pytorch.org/whl/cu117",

        # æ–¹æ¡ˆ5: æœ€æ–°CPUç‰ˆæœ¬ä½œä¸ºæœ€åå¤‡ç”¨
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu",
    ]
    
    for i, cmd in enumerate(pytorch_options, 1):
        print(f"\nå°è¯•PyTorchæ–¹æ¡ˆ {i}...")
        if run_command(cmd, f"PyTorchæ–¹æ¡ˆ {i}"):
            print(f"âœ… PyTorchæ–¹æ¡ˆ {i} æˆåŠŸ")
            return True
    
    print("âŒ æ‰€æœ‰PyTorchå®‰è£…æ–¹æ¡ˆéƒ½å¤±è´¥")
    return False

def install_huggingface_stack():
    """å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ"""
    print("\nğŸ¤— å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ...")
    
    # ä½¿ç”¨å…¼å®¹cached_downloadçš„ç‰ˆæœ¬èŒƒå›´ (è§£å†³APIå…¼å®¹æ€§é—®é¢˜)
    hf_packages = [
        ("huggingface_hub>=0.19.4,<0.25.0", "HuggingFace Hub (å…¼å®¹cached_download)"),
        ("tokenizers>=0.11.1,!=0.11.3,<0.15.0", "Tokenizers"),
        ("safetensors>=0.3.1,<0.5.0", "SafeTensors"),
        ("transformers>=4.25.1,<4.40.0", "Transformers"),
        ("accelerate>=0.11.0,<0.30.0", "Accelerate"),
        ("diffusers==0.24.0", "Diffusers (ç›®æ ‡ç‰ˆæœ¬)"),
    ]
    
    success_count = 0
    for package, description in hf_packages:
        # ä½¿ç”¨--force-reinstallç¡®ä¿ç‰ˆæœ¬æ­£ç¡®
        if run_command(f"pip install '{package}' --force-reinstall --no-cache-dir", f"å®‰è£… {description}"):
            success_count += 1
    
    print(f"\nğŸ“Š HuggingFaceåŒ…å®‰è£…ç»“æœ: {success_count}/{len(hf_packages)} æˆåŠŸ")
    return success_count >= len(hf_packages) - 1  # å…è®¸1ä¸ªå¤±è´¥

def install_other_dependencies():
    """å®‰è£…å…¶ä»–ä¾èµ–"""
    print("\nğŸ“š å®‰è£…å…¶ä»–ä¾èµ–...")
    
    other_deps = [
        "numpy==1.26.4",
        "scipy==1.11.4", 
        "scikit-learn==1.3.0",
        "matplotlib==3.7.2",
        "opencv-python==4.8.1.78",
        "einops==0.7.0",
        "lpips==0.1.4",
    ]
    
    for dep in other_deps:
        run_command(f"pip install {dep}", f"å®‰è£… {dep}")
    
    return True

def test_installation():
    """æµ‹è¯•å®‰è£…ç»“æœ"""
    print("\nğŸ§ª æµ‹è¯•å®‰è£…ç»“æœ...")
    
    # æ¸…ç†æ¨¡å—ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°å¯¼å…¥
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    # æµ‹è¯•å…³é”®å¯¼å…¥
    tests = [
        ("torch", "PyTorch"),
        ("transformers", "Transformers"),
        ("diffusers", "Diffusers"),
        ("huggingface_hub", "HuggingFace Hub"),
        ("accelerate", "Accelerate"),
    ]
    
    success_count = 0
    
    for module_name, display_name in tests:
        try:
            module = importlib.import_module(module_name)
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {display_name}: {version}")
            success_count += 1
        except Exception as e:
            print(f"âŒ {display_name}: å¯¼å…¥å¤±è´¥ - {e}")
    
    # æµ‹è¯•GPU
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… CUDAå¯ç”¨: {torch.version.cuda}")
            print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
            
            # æµ‹è¯•GPUæ“ä½œ
            try:
                device = torch.device('cuda:0')
                test_tensor = torch.randn(10, device=device)
                result = test_tensor + 1
                print("âœ… GPUæ“ä½œæ­£å¸¸")
            except Exception as e:
                print(f"âš ï¸ GPUæ“ä½œå¤±è´¥: {e}")
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUç‰ˆæœ¬")
    except Exception as e:
        print(f"âŒ PyTorchæµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•ä¸‹è½½API
    try:
        from huggingface_hub import hf_hub_download
        print("âœ… hf_hub_download: å¯ç”¨ (diffusersä½¿ç”¨çš„API)")
    except Exception as e:
        print(f"âŒ hf_hub_download: ä¸å¯ç”¨ - {e}")
    
    # æµ‹è¯•VQModel
    try:
        from diffusers.models.autoencoders.vq_model import VQModel
        print("âœ… VQModel: å¯ç”¨ (æ–°ç‰ˆAPI)")
    except ImportError:
        try:
            from diffusers.models.vq_model import VQModel
            print("âœ… VQModel: å¯ç”¨ (æ—§ç‰ˆAPI)")
        except ImportError:
            try:
                from diffusers import VQModel
                print("âœ… VQModel: å¯ç”¨ (ç›´æ¥å¯¼å…¥)")
            except ImportError:
                print("âŒ VQModel: æ‰€æœ‰å¯¼å…¥è·¯å¾„éƒ½å¤±è´¥")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(tests)} åŸºç¡€åŒ…æˆåŠŸ")
    
    return success_count >= len(tests) - 1  # å…è®¸1ä¸ªå¤±è´¥

def get_gpu_config():
    """è·å–GPUè®­ç»ƒé…ç½®"""
    try:
        import torch
        if not torch.cuda.is_available():
            print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒé…ç½®")
            return {"device": "cpu", "batch_size": 8, "mixed_precision": False}

        # æµ‹è¯•GPUæ˜¯å¦çœŸæ­£å¯ç”¨
        try:
            device = torch.device('cuda:0')
            test_tensor = torch.randn(10, device=device)
            _ = test_tensor + 1  # ç®€å•æµ‹è¯•

            gpu_name = torch.cuda.get_device_name(0)
            print(f"ğŸ¯ æ£€æµ‹åˆ°å¯ç”¨GPU: {gpu_name}")

            # æ ¹æ®GPUç±»å‹ä¼˜åŒ–é…ç½®
            if "T4" in gpu_name:
                config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
                print("ğŸ¯ Tesla T4é…ç½®ï¼šbatch_size=16, æ··åˆç²¾åº¦=True")
            elif "P100" in gpu_name:
                config = {"device": "cuda", "batch_size": 12, "mixed_precision": False}
                print("ğŸ¯ Tesla P100é…ç½®ï¼šbatch_size=12, æ··åˆç²¾åº¦=False")
            elif "V100" in gpu_name:
                config = {"device": "cuda", "batch_size": 32, "mixed_precision": True}
                print("ğŸ¯ Tesla V100é…ç½®ï¼šbatch_size=32, æ··åˆç²¾åº¦=True")
            else:
                config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
                print("ğŸ¯ é€šç”¨GPUé…ç½®ï¼šbatch_size=16, æ··åˆç²¾åº¦=True")

            return config

        except Exception as e:
            print(f"âš ï¸ GPUæµ‹è¯•å¤±è´¥: {e}")
            print("ğŸ’» é™çº§åˆ°CPUè®­ç»ƒé…ç½®")
            return {"device": "cpu", "batch_size": 8, "mixed_precision": False}

    except Exception:
        print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒé…ç½®")
        return {"device": "cpu", "batch_size": 8, "mixed_precision": False}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Kaggleç¯å¢ƒä¸€é”®é…ç½®è„šæœ¬")
    print("=" * 50)
    print("ğŸ¯ æ™ºèƒ½ç¯å¢ƒé…ç½® + ä¾èµ–å®‰è£… + å…¼å®¹æ€§æ£€æŸ¥")
    print("ğŸ’¡ ä¼˜å…ˆä½¿ç”¨CPUç‰ˆæœ¬PyTorché¿å…CUDAå…¼å®¹æ€§é—®é¢˜")
    
    # æ‰§è¡Œé…ç½®æµç¨‹
    steps = [
        ("æ£€æŸ¥Kaggleç¯å¢ƒ", check_kaggle_environment),
        ("æ¸…ç†ç¯å¢ƒ", clean_environment),
        ("å®‰è£…PyTorch GPUç‰ˆæœ¬", install_pytorch_gpu),
        ("å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ", install_huggingface_stack),
        ("å®‰è£…å…¶ä»–ä¾èµ–", install_other_dependencies),
        ("æµ‹è¯•å®‰è£…ç»“æœ", test_installation),
    ]
    
    for step_name, step_func in steps:
        print(f"\n{'='*20} {step_name} {'='*20}")
        if not step_func():
            print(f"âŒ {step_name} å¤±è´¥")
            if step_name == "æµ‹è¯•å®‰è£…ç»“æœ":
                print("âš ï¸ éƒ¨åˆ†ç»„ä»¶å¯èƒ½ä»æœ‰é—®é¢˜ï¼Œä½†å¯ä»¥å°è¯•ä½¿ç”¨")
            else:
                print("ğŸ’¥ å…³é”®æ­¥éª¤å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
        else:
            print(f"âœ… {step_name} æˆåŠŸ")
    
    # è·å–GPUé…ç½®
    print(f"\n{'='*20} GPUè®­ç»ƒé…ç½® {'='*20}")
    gpu_config = get_gpu_config()
    print(f"ğŸ“‹ æ¨èé…ç½®: {gpu_config}")
    
    print("\nğŸ‰ Kaggleç¯å¢ƒé…ç½®å®Œæˆ!")
    print("âœ… æ‰€æœ‰ç»„ä»¶å·²å®‰è£…å¹¶éªŒè¯")

    if gpu_config['device'] == 'cpu':
        print("ğŸ’» é…ç½®ä¸ºCPUè®­ç»ƒæ¨¡å¼ (é¿å…CUDAå…¼å®¹æ€§é—®é¢˜)")
        print("âš¡ CPUè®­ç»ƒç¨³å®šå¯é ï¼Œé€‚åˆKaggleç¯å¢ƒ")
    else:
        print("ğŸš€ é…ç½®ä¸ºGPUè®­ç»ƒæ¨¡å¼")

    print("\nğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒ:")
    print(f"   python train_main.py --data_dir /kaggle/input/dataset --device {gpu_config['device']}")
    print(f"   æ¨èbatch_size: {gpu_config['batch_size']}")

    if gpu_config['device'] == 'cpu':
        print("\nğŸ’¡ CPUè®­ç»ƒä¼˜åŠ¿:")
        print("   - é¿å…CUDAå…¼å®¹æ€§é—®é¢˜")
        print("   - ç¨³å®šå¯é ï¼Œä¸ä¼šå‡ºç°NCCLé”™è¯¯")
        print("   - å†…å­˜ä½¿ç”¨æ›´å¯æ§")
    
    return True

if __name__ == "__main__":
    main()
