#!/usr/bin/env python3
"""
Kaggleç¯å¢ƒä¸€é”®é…ç½®è„šæœ¬
æ•´åˆæ‰€æœ‰ç¯å¢ƒé…ç½®åŠŸèƒ½ï¼šGPUä¼˜åŒ–ã€ä¾èµ–å®‰è£…ã€å…¼å®¹æ€§æ£€æŸ¥
"""

import subprocess
import sys
import os
import importlib
import time

def run_command(cmd, description="", timeout=600):
    """è¿è¡Œå‘½ä»¤"""
    print(f"ğŸ”„ {description}")
    print(f"   å‘½ä»¤: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
        if result.returncode == 0:
            print(f"âœ… {description} æˆåŠŸ")
            return True
        else:
            print(f"âŒ {description} å¤±è´¥")
            if result.stderr.strip():
                print(f"   é”™è¯¯: {result.stderr.strip()}")
            return False
    except subprocess.TimeoutExpired:
        print(f"â° {description} è¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ {description} å¼‚å¸¸: {e}")
        return False

def check_kaggle_environment():
    """æ£€æŸ¥Kaggleç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥Kaggleç¯å¢ƒ...")
    
    kaggle_indicators = [
        ("/kaggle", "Kaggleç›®å½•"),
        ("/opt/conda", "Condaç¯å¢ƒ"),
    ]
    
    is_kaggle = False
    for indicator, desc in kaggle_indicators:
        if os.path.exists(indicator):
            print(f"âœ… æ£€æµ‹åˆ° {desc}: {indicator}")
            is_kaggle = True
    
    if is_kaggle:
        print("âœ… ç¡®è®¤åœ¨Kaggleç¯å¢ƒä¸­")
    else:
        print("âš ï¸ å¯èƒ½ä¸åœ¨Kaggleç¯å¢ƒä¸­ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
    
    return True

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("\nğŸ” æ£€æŸ¥GPUç¯å¢ƒ...")
    
    # æ£€æŸ¥nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… nvidia-smiå¯ç”¨")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'CUDA Version:' in line:
                    cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                    print(f"âœ… CUDAç‰ˆæœ¬: {cuda_version}")
                    return True, cuda_version
        else:
            print("âŒ nvidia-smiå¤±è´¥")
            return False, None
    except Exception as e:
        print(f"âŒ nvidia-smiå¼‚å¸¸: {e}")
        return False, None

def clean_environment():
    """æ¸…ç†ç¯å¢ƒ"""
    print("\nğŸ—‘ï¸ æ¸…ç†ç¯å¢ƒ...")
    
    # æ¸…ç†Pythonæ¨¡å—ç¼“å­˜
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub',
            'accelerate', 'tokenizers', 'safetensors'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    print(f"âœ… æ¸…ç†äº† {len(modules_to_clear)} ä¸ªPythonæ¨¡å—")
    
    # å¸è½½å¯èƒ½å†²çªçš„åŒ…
    packages_to_remove = [
        "torch", "torchvision", "torchaudio",
        "transformers", "diffusers", "accelerate",
        "huggingface_hub", "tokenizers", "safetensors"
    ]
    
    for package in packages_to_remove:
        run_command(f"pip uninstall {package} -y", f"å¸è½½ {package}")
    
    # æ¸…ç†pipç¼“å­˜
    run_command("pip cache purge", "æ¸…ç†pipç¼“å­˜")
    
    return True

def install_pytorch_gpu():
    """å®‰è£…GPUç‰ˆæœ¬PyTorch"""
    print("\nğŸ”¥ å®‰è£…GPUç‰ˆæœ¬PyTorch...")
    
    # å€Ÿé‰´ultimate_fix_kaggle.pyçš„æˆåŠŸç­–ç•¥ - ä½¿ç”¨ä¸Kaggleå…¼å®¹çš„æ–°ç‰ˆPyTorch
    pytorch_options = [
        # æ–¹æ¡ˆ1: CUDA 12.1ç‰ˆæœ¬ (ä¸Kaggleæœ€æ–°ç¯å¢ƒåŒ¹é…)
        "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121",

        # æ–¹æ¡ˆ2: CUDA 11.8ç‰ˆæœ¬ (ç¨³å®šç‰ˆæœ¬)
        "pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118",

        # æ–¹æ¡ˆ3: ä½¿ç”¨Kaggleé¢„è£…ç‰ˆæœ¬ (é€šå¸¸å·²ä¼˜åŒ–)
        "pip install torch torchvision torchaudio --upgrade",

        # æ–¹æ¡ˆ4: é»˜è®¤æœ€æ–°ç‰ˆæœ¬
        "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0",

        # æ–¹æ¡ˆ5: CPUç‰ˆæœ¬ä½œä¸ºæœ€åå¤‡ç”¨
        "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cpu",
    ]
    
    for i, cmd in enumerate(pytorch_options, 1):
        print(f"\nå°è¯•PyTorchæ–¹æ¡ˆ {i}...")
        if run_command(cmd, f"PyTorchæ–¹æ¡ˆ {i}"):
            print(f"âœ… PyTorchæ–¹æ¡ˆ {i} æˆåŠŸ")
            return True
    
    print("âŒ æ‰€æœ‰PyTorchå®‰è£…æ–¹æ¡ˆéƒ½å¤±è´¥")
    return False

def install_huggingface_stack():
    """å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ"""
    print("\nğŸ¤— å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ...")
    
    # ä½¿ç”¨diffusersè¦æ±‚çš„æœ€ä½ç‰ˆæœ¬ï¼Œé¿å…æ–°ç‰ˆæœ¬APIå˜åŒ–é—®é¢˜
    hf_packages = [
        ("huggingface_hub==0.19.4", "HuggingFace Hub (diffusersè¦æ±‚çš„æœ€ä½ç‰ˆæœ¬ï¼Œå¯èƒ½ä»æœ‰cached_download)"),
        ("tokenizers>=0.11.1,!=0.11.3", "Tokenizers (diffuserså®˜æ–¹è¦æ±‚)"),
        ("safetensors>=0.3.1", "SafeTensors (diffuserså®˜æ–¹è¦æ±‚)"),
        ("transformers>=4.25.1", "Transformers (diffuserså®˜æ–¹è¦æ±‚)"),
        ("accelerate>=0.11.0", "Accelerate (diffuserså®˜æ–¹è¦æ±‚)"),
        ("diffusers==0.24.0", "Diffusers (ç›®æ ‡ç‰ˆæœ¬)"),
    ]
    
    success_count = 0
    for package, description in hf_packages:
        # ä½¿ç”¨--force-reinstallç¡®ä¿ç‰ˆæœ¬æ­£ç¡®
        if run_command(f"pip install '{package}' --force-reinstall --no-cache-dir", f"å®‰è£… {description}"):
            success_count += 1
    
    print(f"\nğŸ“Š HuggingFaceåŒ…å®‰è£…ç»“æœ: {success_count}/{len(hf_packages)} æˆåŠŸ")

    # å¼ºåˆ¶é‡æ–°å®‰è£…huggingface_hubåˆ°æŒ‡å®šç‰ˆæœ¬ï¼ˆè§£å†³ä¾èµ–å†²çªé—®é¢˜ï¼‰
    print("\nğŸ”§ å¼ºåˆ¶é”å®šhuggingface_hubç‰ˆæœ¬...")
    if run_command("pip install 'huggingface_hub==0.19.4' --force-reinstall --no-deps", "é”å®š HuggingFace Hub 0.19.4"):
        print("âœ… HuggingFace Hubç‰ˆæœ¬é”å®šæˆåŠŸ")
    else:
        print("âš ï¸ HuggingFace Hubç‰ˆæœ¬é”å®šå¤±è´¥")

    # å¦‚æœaccelerateå®‰è£…å¤±è´¥ï¼Œå•ç‹¬é‡è¯•
    if success_count < len(hf_packages):
        print("\nğŸ”§ é‡è¯•å¤±è´¥çš„åŒ…...")
        if run_command("pip install 'accelerate>=0.11.0' --no-cache-dir", "é‡è¯•å®‰è£… Accelerate"):
            success_count += 1
            print("âœ… Accelerateé‡è¯•å®‰è£…æˆåŠŸ")

    return success_count >= len(hf_packages) - 1  # å…è®¸1ä¸ªå¤±è´¥

def install_other_dependencies():
    """å®‰è£…å…¶ä»–ä¾èµ–"""
    print("\nğŸ“š å®‰è£…å…¶ä»–ä¾èµ–...")
    
    other_deps = [
        "numpy==1.26.4",
        "scipy==1.11.4", 
        "scikit-learn==1.3.0",
        "matplotlib==3.7.2",
        "opencv-python==4.8.1.78",
        "einops==0.7.0",
        "lpips==0.1.4",
    ]
    
    for dep in other_deps:
        run_command(f"pip install {dep}", f"å®‰è£… {dep}")
    
    return True

def test_installation():
    """æµ‹è¯•å®‰è£…ç»“æœ"""
    print("\nğŸ§ª æµ‹è¯•å®‰è£…ç»“æœ...")
    
    # æ¸…ç†æ¨¡å—ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°å¯¼å…¥
    modules_to_clear = []
    for module_name in list(sys.modules.keys()):
        if any(pattern in module_name for pattern in [
            'torch', 'transformers', 'diffusers', 'huggingface_hub'
        ]):
            modules_to_clear.append(module_name)
    
    for module_name in modules_to_clear:
        if module_name in sys.modules:
            del sys.modules[module_name]
    
    # æµ‹è¯•å…³é”®å¯¼å…¥
    tests = [
        ("torch", "PyTorch"),
        ("transformers", "Transformers"),
        ("diffusers", "Diffusers"),
        ("huggingface_hub", "HuggingFace Hub"),
        ("accelerate", "Accelerate"),
    ]
    
    success_count = 0
    
    for module_name, display_name in tests:
        try:
            module = importlib.import_module(module_name)
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {display_name}: {version}")
            success_count += 1
        except Exception as e:
            print(f"âŒ {display_name}: å¯¼å…¥å¤±è´¥ - {e}")
    
    # æµ‹è¯•GPU
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… CUDAå¯ç”¨: {torch.version.cuda}")
            print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
            
            # æµ‹è¯•GPUæ“ä½œ
            try:
                device = torch.device('cuda:0')
                test_tensor = torch.randn(10, device=device)
                result = test_tensor + 1
                print("âœ… GPUæ“ä½œæ­£å¸¸")
            except Exception as e:
                print(f"âš ï¸ GPUæ“ä½œå¤±è´¥: {e}")
        else:
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUç‰ˆæœ¬")
    except Exception as e:
        print(f"âŒ PyTorchæµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•ä¸‹è½½API
    try:
        from huggingface_hub import hf_hub_download
        print("âœ… hf_hub_download: å¯ç”¨ (diffusersä½¿ç”¨çš„API)")
    except Exception as e:
        print(f"âŒ hf_hub_download: ä¸å¯ç”¨ - {e}")
    
    # æµ‹è¯•VQModel (æŒ‰diffusers 0.24.0çš„æ­£ç¡®å¯¼å…¥é¡ºåº)
    try:
        from diffusers import VQModel
        print("âœ… VQModel: å¯ç”¨ (diffusers 0.24.0æ ‡å‡†å¯¼å…¥)")
    except ImportError:
        try:
            from diffusers.models.autoencoders.vq_model import VQModel
            print("âœ… VQModel: å¯ç”¨ (autoencodersè·¯å¾„)")
        except ImportError:
            try:
                from diffusers.models.vq_model import VQModel
                print("âœ… VQModel: å¯ç”¨ (æ—§ç‰ˆAPIè·¯å¾„)")
            except ImportError:
                print("âŒ VQModel: æ‰€æœ‰å¯¼å…¥è·¯å¾„éƒ½å¤±è´¥")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(tests)} åŸºç¡€åŒ…æˆåŠŸ")
    
    return success_count >= len(tests) - 1  # å…è®¸1ä¸ªå¤±è´¥

def get_gpu_config():
    """è·å–GPUè®­ç»ƒé…ç½®"""
    try:
        import torch
        if not torch.cuda.is_available():
            print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒé…ç½®")
            return {"device": "cpu", "batch_size": 8, "mixed_precision": False}

        # æµ‹è¯•GPUæ˜¯å¦çœŸæ­£å¯ç”¨
        try:
            device = torch.device('cuda:0')
            test_tensor = torch.randn(10, device=device)
            _ = test_tensor + 1  # ç®€å•æµ‹è¯•

            gpu_name = torch.cuda.get_device_name(0)
            print(f"ğŸš€ æ£€æµ‹åˆ°å¯ç”¨GPU: {gpu_name}")

            # æ ¹æ®GPUç±»å‹ä¼˜åŒ–é…ç½®
            if "T4" in gpu_name:
                config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
                print("ğŸ¯ Tesla T4é…ç½®ï¼šbatch_size=16, æ··åˆç²¾åº¦=True")
            elif "P100" in gpu_name:
                config = {"device": "cuda", "batch_size": 12, "mixed_precision": False}
                print("ğŸ¯ Tesla P100é…ç½®ï¼šbatch_size=12, æ··åˆç²¾åº¦=False")
            elif "V100" in gpu_name:
                config = {"device": "cuda", "batch_size": 32, "mixed_precision": True}
                print("ğŸ¯ Tesla V100é…ç½®ï¼šbatch_size=32, æ··åˆç²¾åº¦=True")
            else:
                config = {"device": "cuda", "batch_size": 16, "mixed_precision": True}
                print("ğŸ¯ é€šç”¨GPUé…ç½®ï¼šbatch_size=16, æ··åˆç²¾åº¦=True")

            print("ğŸš€ GPUè®­ç»ƒæ¨¡å¼ï¼šé€Ÿåº¦å¿«ï¼Œæ€§èƒ½ä¼˜")
            return config

        except Exception as e:
            print(f"âš ï¸ GPUæµ‹è¯•å¤±è´¥: {e}")
            print("ğŸ’» é™çº§åˆ°CPUè®­ç»ƒé…ç½®")
            return {"device": "cpu", "batch_size": 8, "mixed_precision": False}

    except Exception:
        print("ğŸ’» ä½¿ç”¨CPUè®­ç»ƒé…ç½®")
        return {"device": "cpu", "batch_size": 8, "mixed_precision": False}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Kaggleç¯å¢ƒä¸€é”®é…ç½®è„šæœ¬")
    print("=" * 50)
    print("ğŸ¯ GPUä¼˜åŒ–é…ç½® + ä¾èµ–å®‰è£… + å…¼å®¹æ€§æ£€æŸ¥")
    print("ğŸ’¡ ä½¿ç”¨ä¸Kaggleå…¼å®¹çš„æ–°ç‰ˆPyTorchè§£å†³CUDAé—®é¢˜")
    print("ğŸ“‹ ä¿æŒdiffusers 0.24.0ç‰ˆæœ¬ï¼ŒæŒ‰å®˜æ–¹è¦æ±‚é…ç½®")
    
    # æ‰§è¡Œé…ç½®æµç¨‹
    steps = [
        ("æ£€æŸ¥Kaggleç¯å¢ƒ", check_kaggle_environment),
        ("æ¸…ç†ç¯å¢ƒ", clean_environment),
        ("å®‰è£…PyTorch GPUç‰ˆæœ¬", install_pytorch_gpu),
        ("å®‰è£…HuggingFaceæŠ€æœ¯æ ˆ", install_huggingface_stack),
        ("å®‰è£…å…¶ä»–ä¾èµ–", install_other_dependencies),
        ("æµ‹è¯•å®‰è£…ç»“æœ", test_installation),
    ]
    
    for step_name, step_func in steps:
        print(f"\n{'='*20} {step_name} {'='*20}")
        if not step_func():
            print(f"âŒ {step_name} å¤±è´¥")
            if step_name == "æµ‹è¯•å®‰è£…ç»“æœ":
                print("âš ï¸ éƒ¨åˆ†ç»„ä»¶å¯èƒ½ä»æœ‰é—®é¢˜ï¼Œä½†å¯ä»¥å°è¯•ä½¿ç”¨")
            else:
                print("ğŸ’¥ å…³é”®æ­¥éª¤å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
        else:
            print(f"âœ… {step_name} æˆåŠŸ")
    
    # è·å–GPUé…ç½®
    print(f"\n{'='*20} GPUè®­ç»ƒé…ç½® {'='*20}")
    gpu_config = get_gpu_config()
    print(f"ğŸ“‹ æ¨èé…ç½®: {gpu_config}")
    
    print("\nğŸ‰ Kaggleç¯å¢ƒé…ç½®å®Œæˆ!")
    print("âœ… æ‰€æœ‰ç»„ä»¶å·²å®‰è£…å¹¶éªŒè¯")
    print("ğŸ“‹ diffusers 0.24.0 + å…¼å®¹PyTorchç‰ˆæœ¬")

    if gpu_config['device'] == 'cpu':
        print("ğŸ’» é…ç½®ä¸ºCPUè®­ç»ƒæ¨¡å¼ (GPUå…¼å®¹æ€§é—®é¢˜)")
        print("âš¡ CPUè®­ç»ƒç¨³å®šå¯é ï¼ŒåŠŸèƒ½å®Œæ•´")
    else:
        print("ğŸš€ é…ç½®ä¸ºGPUè®­ç»ƒæ¨¡å¼")
        print("âš¡ GPUè®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ€§èƒ½ä¼˜")

    print("\nğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒ:")
    print(f"   python train_main.py --data_dir /kaggle/input/dataset --device {gpu_config['device']}")
    print(f"   æ¨èbatch_size: {gpu_config['batch_size']}")

    if gpu_config['device'] == 'cuda':
        print("\nğŸ’¡ GPUè®­ç»ƒä¼˜åŠ¿:")
        print("   - è®­ç»ƒé€Ÿåº¦å¿«5-10å€")
        print("   - æ”¯æŒæ›´å¤§çš„batch size")
        print("   - æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ")
    else:
        print("\nğŸ’¡ CPUè®­ç»ƒè¯´æ˜:")
        print("   - åŠŸèƒ½å®Œæ•´ï¼Œç¨³å®šå¯é ")
        print("   - é€‚åˆå°è§„æ¨¡å®éªŒ")
        print("   - å¦‚éœ€GPUï¼Œè¯·æ£€æŸ¥CUDAå…¼å®¹æ€§")
    
    return True

if __name__ == "__main__":
    main()
