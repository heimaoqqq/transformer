#!/usr/bin/env python3
"""
å®Œæ•´çš„APIå…¼å®¹æ€§æ£€æŸ¥è„šæœ¬
éªŒè¯diffusersã€transformerså’Œè‡ªå®šä¹‰æ¨¡å‹çš„APIå…¼å®¹æ€§

âš ï¸ é‡è¦ï¼šè¯·åœ¨ç¯å¢ƒé…ç½®å®Œæˆåè¿è¡Œæ­¤è„šæœ¬
ä½¿ç”¨æ–¹æ³•ï¼š
1. å…ˆè¿è¡Œ: python setup_unified_environment.py
2. å†è¿è¡Œ: python test_api_compatibility.py
"""

import torch
import sys
import importlib
import inspect
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

class APICompatibilityChecker:
    """APIå…¼å®¹æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.results = {}
        self.warnings_captured = []
        
    def capture_warnings(self):
        """æ•è·è­¦å‘Šä¿¡æ¯"""
        def warning_handler(message, category, filename, lineno, file=None, line=None):
            self.warnings_captured.append({
                'message': str(message),
                'category': category.__name__,
                'filename': filename,
                'lineno': lineno
            })
        
        warnings.showwarning = warning_handler
        
    def check_module_versions(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å—ç‰ˆæœ¬"""
        print("ğŸ” æ£€æŸ¥æ¨¡å—ç‰ˆæœ¬...")
        
        modules = {
            'torch': 'PyTorch',
            'diffusers': 'Diffusers',
            'transformers': 'Transformers',
            'huggingface_hub': 'HuggingFace Hub',
            'accelerate': 'Accelerate',
            'safetensors': 'SafeTensors',
            'tokenizers': 'Tokenizers',
        }
        
        versions = {}
        for module_name, display_name in modules.items():
            try:
                module = importlib.import_module(module_name)
                version = getattr(module, '__version__', 'unknown')
                versions[module_name] = version
                print(f"âœ… {display_name}: {version}")
            except ImportError as e:
                versions[module_name] = f"ERROR: {e}"
                print(f"âŒ {display_name}: å¯¼å…¥å¤±è´¥ - {e}")
        
        return versions
    
    def check_diffusers_api(self) -> Dict[str, Any]:
        """æ£€æŸ¥diffusers APIå…¼å®¹æ€§"""
        print("\nğŸ” æ£€æŸ¥diffusers APIå…¼å®¹æ€§...")
        
        results = {}
        
        # 1. æ£€æŸ¥VQModelå¯¼å…¥
        try:
            from diffusers.models.autoencoders.vq_model import VQModel
            results['vqmodel_import'] = "SUCCESS"
            print("âœ… VQModelå¯¼å…¥æˆåŠŸ")
            
            # æ£€æŸ¥VQModelæ„é€ å‡½æ•°å‚æ•°
            sig = inspect.signature(VQModel.__init__)
            params = list(sig.parameters.keys())
            results['vqmodel_params'] = params
            print(f"âœ… VQModelå‚æ•°: {len(params)} ä¸ª")
            
            # æµ‹è¯•VQModelå®ä¾‹åŒ–
            try:
                model = VQModel(
                    in_channels=3,
                    out_channels=3,
                    latent_channels=4,
                    num_vq_embeddings=1024,
                    vq_embed_dim=256,
                )
                results['vqmodel_instantiation'] = "SUCCESS"
                print("âœ… VQModelå®ä¾‹åŒ–æˆåŠŸ")
                
                # æ£€æŸ¥VQModelæ–¹æ³•
                methods = [method for method in dir(model) if not method.startswith('_')]
                results['vqmodel_methods'] = methods
                print(f"âœ… VQModelæ–¹æ³•: {len(methods)} ä¸ª")
                
            except Exception as e:
                results['vqmodel_instantiation'] = f"ERROR: {e}"
                print(f"âŒ VQModelå®ä¾‹åŒ–å¤±è´¥: {e}")
                
        except ImportError as e:
            results['vqmodel_import'] = f"ERROR: {e}"
            print(f"âŒ VQModelå¯¼å…¥å¤±è´¥: {e}")
        
        # 2. æ£€æŸ¥VectorQuantizer
        try:
            from diffusers.models.autoencoders.vq_model import VectorQuantizer
            results['vectorquantizer_import'] = "SUCCESS"
            print("âœ… VectorQuantizerå¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            results['vectorquantizer_import'] = f"ERROR: {e}"
            print(f"âŒ VectorQuantizerå¯¼å…¥å¤±è´¥: {e}")
        
        # 3. æ£€æŸ¥å…¶ä»–diffusersç»„ä»¶
        try:
            from diffusers import AutoencoderKL
            results['autoencoder_import'] = "SUCCESS"
            print("âœ… AutoencoderKLå¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            results['autoencoder_import'] = f"ERROR: {e}"
            print(f"âŒ AutoencoderKLå¯¼å…¥å¤±è´¥: {e}")
        
        return results
    
    def check_transformers_api(self) -> Dict[str, Any]:
        """æ£€æŸ¥transformers APIå…¼å®¹æ€§"""
        print("\nğŸ” æ£€æŸ¥transformers APIå…¼å®¹æ€§...")

        results = {}

        # 1. æ£€æŸ¥GPT2
        try:
            from transformers import GPT2Config, GPT2LMHeadModel
            results['gpt2_import'] = "SUCCESS"
            print("âœ… GPT2å¯¼å…¥æˆåŠŸ")

            # æ£€æŸ¥GPT2Configå‚æ•°
            sig = inspect.signature(GPT2Config.__init__)
            params = list(sig.parameters.keys())
            results['gpt2_config_params'] = params
            print(f"âœ… GPT2Configå‚æ•°: {len(params)} ä¸ª")

            # æµ‹è¯•GPT2å®ä¾‹åŒ– - é‡ç‚¹æµ‹è¯•äº¤å‰æ³¨æ„åŠ›
            try:
                config = GPT2Config(
                    vocab_size=1025,  # 1024 + 1ä¸ªç‰¹æ®Štoken
                    n_positions=1025,  # 1024 + 1ä¸ªç”¨æˆ·token
                    n_embd=512,
                    n_layer=8,
                    n_head=8,
                    n_inner=2048,
                    activation_function="gelu_new",
                    resid_pdrop=0.1,
                    embd_pdrop=0.1,
                    attn_pdrop=0.1,
                    layer_norm_epsilon=1e-5,
                    initializer_range=0.02,
                    use_cache=False,
                    add_cross_attention=True,  # å…³é”®ï¼šäº¤å‰æ³¨æ„åŠ›
                    _name_or_path="",
                )
                model = GPT2LMHeadModel(config)
                results['gpt2_instantiation'] = "SUCCESS"
                print("âœ… GPT2å®ä¾‹åŒ–æˆåŠŸ")

                # æ£€æŸ¥äº¤å‰æ³¨æ„åŠ›å±‚æ˜¯å¦å­˜åœ¨
                has_cross_attn = hasattr(model.transformer.h[0], 'crossattention')
                results['cross_attention_exists'] = has_cross_attn
                print(f"{'âœ…' if has_cross_attn else 'âŒ'} äº¤å‰æ³¨æ„åŠ›å±‚: {'å­˜åœ¨' if has_cross_attn else 'ä¸å­˜åœ¨'}")

                # æµ‹è¯•äº¤å‰æ³¨æ„åŠ›å‰å‘ä¼ æ’­
                if has_cross_attn:
                    try:
                        batch_size = 2
                        seq_len = 10
                        input_ids = torch.randint(0, 1024, (batch_size, seq_len))
                        attention_mask = torch.ones_like(input_ids)
                        encoder_hidden_states = torch.randn(batch_size, 8, 512)
                        encoder_attention_mask = torch.ones(batch_size, 8)

                        with torch.no_grad():
                            # ä¸ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
                            output1 = model(input_ids=input_ids, attention_mask=attention_mask)

                            # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
                            output2 = model(
                                input_ids=input_ids,
                                attention_mask=attention_mask,
                                encoder_hidden_states=encoder_hidden_states,
                                encoder_attention_mask=encoder_attention_mask,
                            )

                        # æ£€æŸ¥è¾“å‡ºå·®å¼‚
                        diff = torch.abs(output1.logits - output2.logits).mean().item()
                        results['cross_attention_effect'] = diff

                        if diff > 1e-6:
                            print(f"âœ… äº¤å‰æ³¨æ„åŠ›æœ‰æ•ˆæœ: å·®å¼‚ {diff:.6f}")
                        else:
                            print(f"âŒ äº¤å‰æ³¨æ„åŠ›æ— æ•ˆæœ: å·®å¼‚ {diff:.6f}")

                    except Exception as e:
                        results['cross_attention_test'] = f"ERROR: {e}"
                        print(f"âŒ äº¤å‰æ³¨æ„åŠ›æµ‹è¯•å¤±è´¥: {e}")

            except Exception as e:
                results['gpt2_instantiation'] = f"ERROR: {e}"
                print(f"âŒ GPT2å®ä¾‹åŒ–å¤±è´¥: {e}")

        except ImportError as e:
            results['gpt2_import'] = f"ERROR: {e}"
            print(f"âŒ GPT2å¯¼å…¥å¤±è´¥: {e}")

        # 2. æ£€æŸ¥Tokenizer
        try:
            from transformers import AutoTokenizer
            results['tokenizer_import'] = "SUCCESS"
            print("âœ… AutoTokenizerå¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            results['tokenizer_import'] = f"ERROR: {e}"
            print(f"âŒ AutoTokenizerå¯¼å…¥å¤±è´¥: {e}")

        return results
    
    def check_custom_models_api(self) -> Dict[str, Any]:
        """æ£€æŸ¥è‡ªå®šä¹‰æ¨¡å‹APIå…¼å®¹æ€§"""
        print("\nğŸ” æ£€æŸ¥è‡ªå®šä¹‰æ¨¡å‹APIå…¼å®¹æ€§...")

        results = {}

        # 1. æ£€æŸ¥VQ-VAEæ¨¡å‹
        try:
            from models.vqvae_model import MicroDopplerVQVAE
            results['custom_vqvae_import'] = "SUCCESS"
            print("âœ… MicroDopplerVQVAEå¯¼å…¥æˆåŠŸ")

            # æ£€æŸ¥æ„é€ å‡½æ•°å‚æ•°
            sig = inspect.signature(MicroDopplerVQVAE.__init__)
            params = list(sig.parameters.keys())
            results['custom_vqvae_params'] = params
            print(f"âœ… MicroDopplerVQVAEå‚æ•°: {len(params)} ä¸ª")

            # æµ‹è¯•å®ä¾‹åŒ–
            try:
                model = MicroDopplerVQVAE(
                    in_channels=3,
                    out_channels=3,
                    latent_channels=4,
                    num_vq_embeddings=1024,
                    vq_embed_dim=256
                )
                results['custom_vqvae_instantiation'] = "SUCCESS"
                print("âœ… MicroDopplerVQVAEå®ä¾‹åŒ–æˆåŠŸ")

                # æ£€æŸ¥æ–¹æ³•
                methods = [method for method in dir(model) if not method.startswith('_')]
                results['custom_vqvae_methods'] = methods
                print(f"âœ… MicroDopplerVQVAEæ–¹æ³•: {len(methods)} ä¸ª")

            except Exception as e:
                results['custom_vqvae_instantiation'] = f"ERROR: {e}"
                print(f"âŒ MicroDopplerVQVAEå®ä¾‹åŒ–å¤±è´¥: {e}")

        except ImportError as e:
            results['custom_vqvae_import'] = f"ERROR: {e}"
            print(f"âŒ MicroDopplerVQVAEå¯¼å…¥å¤±è´¥: {e}")

        # 2. æ£€æŸ¥Transformeræ¨¡å‹
        try:
            from models.transformer_model import MicroDopplerTransformer
            results['custom_transformer_import'] = "SUCCESS"
            print("âœ… MicroDopplerTransformerå¯¼å…¥æˆåŠŸ")

            # æµ‹è¯•å®ä¾‹åŒ–
            try:
                transformer = MicroDopplerTransformer(
                    vocab_size=1024,
                    max_seq_len=1024,
                    num_users=31,
                    n_embd=512,
                    n_layer=8,
                    n_head=8,
                    dropout=0.1,
                    use_cross_attention=True,
                )
                results['custom_transformer_instantiation'] = "SUCCESS"
                print("âœ… MicroDopplerTransformerå®ä¾‹åŒ–æˆåŠŸ")

                # æµ‹è¯•å‰å‘ä¼ æ’­
                try:
                    batch_size = 2
                    user_ids = torch.tensor([1, 5])
                    token_sequences = torch.randint(0, 1024, (batch_size, 1024))

                    with torch.no_grad():
                        outputs = transformer(user_ids=user_ids, token_sequences=token_sequences)

                    results['transformer_forward'] = {
                        'loss': outputs.loss.item(),
                        'logits_shape': list(outputs.logits.shape),
                        'loss_valid': not (torch.isnan(outputs.loss) or torch.isinf(outputs.loss))
                    }

                    print(f"âœ… Transformerå‰å‘ä¼ æ’­æˆåŠŸ")
                    print(f"   æŸå¤±: {outputs.loss.item():.4f}")
                    print(f"   logitså½¢çŠ¶: {outputs.logits.shape}")

                    # æ£€æŸ¥æŸå¤±æ˜¯å¦åˆç†
                    if torch.isnan(outputs.loss) or torch.isinf(outputs.loss):
                        print(f"âŒ æŸå¤±å€¼æ— æ•ˆ: {outputs.loss.item()}")
                    elif outputs.loss.item() > 20:
                        print(f"âš ï¸ æŸå¤±å€¼è¿‡é«˜: {outputs.loss.item():.4f}")
                    else:
                        print(f"âœ… æŸå¤±å€¼æ­£å¸¸")

                    # æµ‹è¯•ç”Ÿæˆ
                    try:
                        with torch.no_grad():
                            generated = transformer.generate(
                                user_ids=torch.tensor([1]),
                                max_length=10,  # çŸ­åºåˆ—æµ‹è¯•
                                temperature=1.0,
                                do_sample=True,
                            )

                        results['transformer_generation'] = {
                            'generated_shape': list(generated.shape),
                            'token_range': [generated.min().item(), generated.max().item()],
                            'valid_tokens': (generated.min().item() >= 0 and generated.max().item() < 1024)
                        }

                        print(f"âœ… Transformerç”Ÿæˆæµ‹è¯•æˆåŠŸ")
                        print(f"   ç”Ÿæˆå½¢çŠ¶: {generated.shape}")
                        print(f"   TokenèŒƒå›´: [{generated.min().item()}, {generated.max().item()}]")

                        if generated.min().item() < 0 or generated.max().item() >= 1024:
                            print(f"âŒ ç”Ÿæˆçš„tokenè¶…å‡ºæœ‰æ•ˆèŒƒå›´")
                        else:
                            print(f"âœ… ç”Ÿæˆçš„tokenåœ¨æœ‰æ•ˆèŒƒå›´å†…")

                    except Exception as e:
                        results['transformer_generation'] = f"ERROR: {e}"
                        print(f"âŒ Transformerç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")

                except Exception as e:
                    results['transformer_forward'] = f"ERROR: {e}"
                    print(f"âŒ Transformerå‰å‘ä¼ æ’­å¤±è´¥: {e}")

            except Exception as e:
                results['custom_transformer_instantiation'] = f"ERROR: {e}"
                print(f"âŒ MicroDopplerTransformerå®ä¾‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        except ImportError as e:
            results['custom_transformer_import'] = f"ERROR: {e}"
            print(f"âŒ MicroDopplerTransformerå¯¼å…¥å¤±è´¥: {e}")

        return results
    
    def check_forward_compatibility(self) -> Dict[str, Any]:
        """æ£€æŸ¥å‰å‘ä¼ æ’­å…¼å®¹æ€§"""
        print("\nğŸ” æ£€æŸ¥å‰å‘ä¼ æ’­å…¼å®¹æ€§...")
        
        results = {}
        
        # 1. æµ‹è¯•VQModelå‰å‘ä¼ æ’­
        try:
            from diffusers.models.autoencoders.vq_model import VQModel
            model = VQModel(
                in_channels=3,
                out_channels=3,
                latent_channels=4,
                num_vq_embeddings=1024,
                vq_embed_dim=256,
            )
            
            x = torch.randn(1, 3, 64, 64)
            with torch.no_grad():
                output = model(x)
                
            results['vqmodel_forward'] = {
                'input_shape': list(x.shape),
                'output_shape': list(output.sample.shape),
                'output_type': type(output).__name__
            }
            print(f"âœ… VQModelå‰å‘ä¼ æ’­: {x.shape} -> {output.sample.shape}")
            
        except Exception as e:
            results['vqmodel_forward'] = f"ERROR: {e}"
            print(f"âŒ VQModelå‰å‘ä¼ æ’­å¤±è´¥: {e}")
        
        # 2. æµ‹è¯•GPT2å‰å‘ä¼ æ’­
        try:
            from transformers import GPT2Config, GPT2LMHeadModel
            config = GPT2Config(
                vocab_size=1024,
                n_positions=256,
                n_embd=512,
                n_layer=4,
                n_head=8
            )
            model = GPT2LMHeadModel(config)
            
            input_ids = torch.randint(0, 1024, (1, 10))
            with torch.no_grad():
                output = model(input_ids)
                
            results['gpt2_forward'] = {
                'input_shape': list(input_ids.shape),
                'output_shape': list(output.logits.shape),
                'output_type': type(output).__name__
            }
            print(f"âœ… GPT2å‰å‘ä¼ æ’­: {input_ids.shape} -> {output.logits.shape}")
            
        except Exception as e:
            results['gpt2_forward'] = f"ERROR: {e}"
            print(f"âŒ GPT2å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        
        # 3. æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹å‰å‘ä¼ æ’­
        try:
            from models.vqvae_model import MicroDopplerVQVAE
            model = MicroDopplerVQVAE()
            
            x = torch.randn(1, 3, 128, 128)
            with torch.no_grad():
                output = model(x)
                
            results['custom_vqvae_forward'] = {
                'input_shape': list(x.shape),
                'output_shape': list(output.sample.shape),
                'output_type': type(output).__name__
            }
            print(f"âœ… MicroDopplerVQVAEå‰å‘ä¼ æ’­: {x.shape} -> {output.sample.shape}")
            
        except Exception as e:
            results['custom_vqvae_forward'] = f"ERROR: {e}"
            print(f"âŒ MicroDopplerVQVAEå‰å‘ä¼ æ’­å¤±è´¥: {e}")
        
        return results
    
    def check_save_load_compatibility(self) -> Dict[str, Any]:
        """æ£€æŸ¥ä¿å­˜/åŠ è½½å…¼å®¹æ€§"""
        print("\nğŸ” æ£€æŸ¥ä¿å­˜/åŠ è½½å…¼å®¹æ€§...")
        
        results = {}
        
        try:
            from models.vqvae_model import MicroDopplerVQVAE
            model = MicroDopplerVQVAE()
            
            # æµ‹è¯•state_dict
            state_dict = model.state_dict()
            results['state_dict_keys'] = len(state_dict)
            print(f"âœ… state_dictè·å–æˆåŠŸ: {len(state_dict)} ä¸ªå‚æ•°")
            
            # æµ‹è¯•load_state_dict
            model.load_state_dict(state_dict)
            results['load_state_dict'] = "SUCCESS"
            print("âœ… load_state_dictæˆåŠŸ")
            
        except Exception as e:
            results['save_load'] = f"ERROR: {e}"
            print(f"âŒ ä¿å­˜/åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def run_full_check(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„APIå…¼å®¹æ€§æ£€æŸ¥"""
        print("ğŸ¨ å®Œæ•´APIå…¼å®¹æ€§æ£€æŸ¥")
        print("=" * 60)
        
        self.capture_warnings()
        
        # è¿è¡Œæ‰€æœ‰æ£€æŸ¥
        self.results['versions'] = self.check_module_versions()
        self.results['diffusers_api'] = self.check_diffusers_api()
        self.results['transformers_api'] = self.check_transformers_api()
        self.results['custom_models_api'] = self.check_custom_models_api()
        self.results['forward_compatibility'] = self.check_forward_compatibility()
        self.results['save_load_compatibility'] = self.check_save_load_compatibility()
        self.results['warnings'] = self.warnings_captured
        
        return self.results
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š"""
        report = []
        report.append("# APIå…¼å®¹æ€§æ£€æŸ¥æŠ¥å‘Š")
        report.append("=" * 60)
        
        # ç‰ˆæœ¬ä¿¡æ¯
        report.append("\n## ğŸ“¦ ç‰ˆæœ¬ä¿¡æ¯")
        for module, version in self.results.get('versions', {}).items():
            status = "âœ…" if not version.startswith("ERROR") else "âŒ"
            report.append(f"{status} {module}: {version}")
        
        # è­¦å‘Šä¿¡æ¯
        if self.warnings_captured:
            report.append(f"\n## âš ï¸ è­¦å‘Šä¿¡æ¯ ({len(self.warnings_captured)} ä¸ª)")
            for warning in self.warnings_captured:
                report.append(f"- {warning['category']}: {warning['message']}")
        
        # æ€»ç»“
        total_checks = 0
        passed_checks = 0
        
        for category, results in self.results.items():
            if category in ['versions', 'warnings']:
                continue
            if isinstance(results, dict):
                for key, value in results.items():
                    total_checks += 1
                    if not str(value).startswith("ERROR"):
                        passed_checks += 1
        
        report.append(f"\n## ğŸ“Š æ£€æŸ¥æ€»ç»“")
        report.append(f"- æ€»æ£€æŸ¥é¡¹: {total_checks}")
        report.append(f"- é€šè¿‡æ£€æŸ¥: {passed_checks}")
        report.append(f"- æˆåŠŸç‡: {passed_checks/total_checks*100:.1f}%")
        
        if passed_checks >= total_checks * 0.9:  # 90%é€šè¿‡ç‡
            report.append("\nğŸ‰ APIå…¼å®¹æ€§æ£€æŸ¥é€šè¿‡ï¼ç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
        else:
            report.append("\nâŒ APIå…¼å®¹æ€§æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®ã€‚")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    print("âš ï¸ é‡è¦æé†’ï¼šè¯·ç¡®ä¿å·²å®Œæˆç¯å¢ƒé…ç½®")
    print("   å¦‚æœå°šæœªé…ç½®ç¯å¢ƒï¼Œè¯·å…ˆè¿è¡Œ:")
    print("   python setup_unified_environment.py")
    print("   æˆ–")
    print("   python setup_vqvae_environment.py")
    print("   python setup_transformer_environment.py")
    print()

    checker = APICompatibilityChecker()
    results = checker.run_full_check()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š...")
    
    report = checker.generate_report()
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path(__file__).parent / "api_compatibility_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    return results

if __name__ == "__main__":
    results = main()
