#!/usr/bin/env python3
"""
Transformerè®­ç»ƒè„šæœ¬
ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒTransformerå­¦ä¹ ä»ç”¨æˆ·IDç”Ÿæˆtokenåºåˆ—
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm
import numpy as np
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.transformer_model import MicroDopplerTransformer
from models.vqvae_model import MicroDopplerVQVAE
from utils.data_loader import MicroDopplerDataset

class TransformerTrainer:
    """Transformerè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device(args.device if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ¤– Transformerè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   VQ-VAEè·¯å¾„: {args.vqvae_path}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        
        # åŠ è½½VQ-VAEæ¨¡å‹
        self.vqvae_model = self._load_vqvae_model()
        
        # åˆ›å»ºTransformeræ¨¡å‹
        self.transformer_model = self._create_transformer_model()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.transformer_model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # åˆ›å»ºå¸¦warmupçš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.warmup_steps = 1000  # çº¦2ä¸ªepoch
        self.current_step = 0
        # total_stepså°†åœ¨train()æ–¹æ³•ä¸­è®¡ç®—

        # ä½¿ç”¨CosineAnnealingLRä½œä¸ºä¸»è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
    def _load_vqvae_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹"""
        vqvae_path = Path(self.args.vqvae_path)

        # æ£€æŸ¥æ˜¯å¦ç›´æ¥åŒ…å«diffusersæ ¼å¼æ–‡ä»¶ (config.json + safetensors)
        config_file = vqvae_path / "config.json"
        safetensors_file = vqvae_path / "diffusion_pytorch_model.safetensors"

        if config_file.exists() and safetensors_file.exists():
            print(f"ğŸ“‚ åŠ è½½VQ-VAEæ¨¡å‹ (ç›´æ¥diffusersæ ¼å¼): {vqvae_path}")
            try:
                from models.vqvae_model import MicroDopplerVQVAE
                vqvae_model = MicroDopplerVQVAE.from_pretrained(vqvae_path)
                vqvae_model.to(self.device)
                vqvae_model.eval()
                print("âœ… æˆåŠŸåŠ è½½ç›´æ¥diffusersæ ¼å¼æ¨¡å‹")
                return vqvae_model
            except Exception as e:
                print(f"âš ï¸ ç›´æ¥diffusersæ ¼å¼åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•final_modelå­ç›®å½•...")

        # å°è¯•final_modelå­ç›®å½• (diffusersæ ¼å¼)
        final_model_path = vqvae_path / "final_model"
        if final_model_path.exists():
            print(f"ğŸ“‚ åŠ è½½VQ-VAEæ¨¡å‹ (final_modelå­ç›®å½•): {final_model_path}")
            try:
                from models.vqvae_model import MicroDopplerVQVAE
                vqvae_model = MicroDopplerVQVAE.from_pretrained(final_model_path)
                vqvae_model.to(self.device)
                vqvae_model.eval()
                print("âœ… æˆåŠŸåŠ è½½final_modelå­ç›®å½•æ ¼å¼æ¨¡å‹")
                return vqvae_model
            except Exception as e:
                print(f"âš ï¸ final_modelå­ç›®å½•æ ¼å¼åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•checkpointæ ¼å¼...")

        # å¤‡é€‰ï¼šä½¿ç”¨checkpointæ–‡ä»¶
        best_model_path = vqvae_path / "best_model.pth"
        if best_model_path.exists():
            model_file = best_model_path
        else:
            # æŸ¥æ‰¾å…¶ä»–checkpointæ–‡ä»¶
            model_files = list(vqvae_path.glob("*.pth"))
            if not model_files:
                raise FileNotFoundError(f"åœ¨ {vqvae_path} ä¸­æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹æ–‡ä»¶")
            model_file = model_files[0]

        print(f"ğŸ“‚ åŠ è½½VQ-VAEæ¨¡å‹ (checkpointæ ¼å¼): {model_file}")

        # åŠ è½½checkpoint
        checkpoint = torch.load(model_file, map_location=self.device)

        # é‡å»ºVQ-VAEæ¨¡å‹
        from models.vqvae_model import MicroDopplerVQVAE
        vqvae_model = MicroDopplerVQVAE(
            num_vq_embeddings=checkpoint['args'].codebook_size,
            commitment_cost=checkpoint['args'].commitment_cost,
            ema_decay=getattr(checkpoint['args'], 'ema_decay', 0.99),
        )

        # åŠ è½½æƒé‡
        vqvae_model.load_state_dict(checkpoint['model_state_dict'])
        vqvae_model.to(self.device)
        vqvae_model.eval()
        print("âœ… æˆåŠŸåŠ è½½checkpointæ ¼å¼æ¨¡å‹")
        
        print(f"âœ… VQ-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        return vqvae_model
        
    def _create_transformer_model(self):
        """åˆ›å»ºTransformeræ¨¡å‹"""
        model = MicroDopplerTransformer(
            vocab_size=self.args.codebook_size,
            max_seq_len=self.args.resolution * self.args.resolution // 16,  # VQ-VAEå®é™…æ˜¯4å€ä¸‹é‡‡æ ·: (128//4)^2 = 32^2 = 1024
            num_users=self.args.num_users,
            n_embd=self.args.n_embd,
            n_layer=self.args.n_layer,
            n_head=self.args.n_head,
            dropout=0.1,
            use_cross_attention=True,
        )
        model.to(self.device)

        print(f"âœ… Transformeræ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.args.codebook_size}")
        print(f"   åµŒå…¥ç»´åº¦: {self.args.n_embd}")
        print(f"   å±‚æ•°: {self.args.n_layer}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.args.n_head}")
        print(f"   åºåˆ—é•¿åº¦: {model.max_seq_len}")
        print(f"   ç”¨æˆ·æ•°é‡: {self.args.num_users}")

        # æµ‹è¯•å¢å¼ºåŠŸèƒ½æ˜¯å¦å·¥ä½œ
        self._test_enhanced_features(model)

        return model

    def _test_enhanced_features(self, model):
        """æµ‹è¯•å¢å¼ºåŠŸèƒ½æ˜¯å¦æ­£ç¡®å·¥ä½œ"""
        print(f"ğŸ§ª æµ‹è¯•å¢å¼ºåŠŸèƒ½:")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_user_ids = torch.tensor([1, 2], device=self.device)
        test_tokens = torch.randint(0, 1024, (2, 1024), device=self.device)

        # æµ‹è¯•ç”¨æˆ·ç¼–ç å™¨
        with torch.no_grad():
            user_embeds = model.user_encoder(test_user_ids)
            print(f"   ç”¨æˆ·åµŒå…¥å½¢çŠ¶: {user_embeds.shape} (åº”è¯¥æ˜¯[2, 512])")

            # æµ‹è¯•prepare_inputs
            inputs_dict = model.prepare_inputs(test_user_ids, test_tokens)

            print(f"   è¾“å…¥åºåˆ—å½¢çŠ¶: {inputs_dict['input_ids'].shape}")
            print(f"   æ ‡ç­¾å½¢çŠ¶: {inputs_dict['labels'].shape}")

            if inputs_dict['encoder_hidden_states'] is not None:
                print(f"   äº¤å‰æ³¨æ„åŠ›çŠ¶æ€å½¢çŠ¶: {inputs_dict['encoder_hidden_states'].shape} (åº”è¯¥æ˜¯[2, 8, 512])")
                print(f"   æ³¨æ„åŠ›æ©ç å½¢çŠ¶: {inputs_dict['encoder_attention_mask'].shape}")
            else:
                print(f"   äº¤å‰æ³¨æ„åŠ›: æœªä½¿ç”¨")

        print(f"âœ… å¢å¼ºåŠŸèƒ½æµ‹è¯•å®Œæˆ")

        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        self._benchmark_performance(model)

        return model

    def _benchmark_performance(self, model):
        """æ€§èƒ½åŸºå‡†æµ‹è¯• - éªŒè¯å¢å¼ºåŠŸèƒ½çš„å®é™…å½±å“"""
        print(f"âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•:")

        import time
        import torch.profiler

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_user_ids = torch.tensor([1, 2, 3, 4], device=self.device)
        test_tokens = torch.randint(0, 1024, (4, 1024), device=self.device)

        # é¢„çƒ­GPU
        for _ in range(3):
            with torch.no_grad():
                _ = model(test_user_ids, test_tokens)

        torch.cuda.synchronize()

        # æµ‹è¯•å‰å‘ä¼ æ’­æ—¶é—´
        num_runs = 10
        start_time = time.time()

        for _ in range(num_runs):
            with torch.no_grad():
                outputs = model(test_user_ids, test_tokens)

        torch.cuda.synchronize()
        end_time = time.time()

        avg_time = (end_time - start_time) / num_runs

        # æµ‹è¯•æ˜¾å­˜ä½¿ç”¨
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated(self.device)

        outputs = model(test_user_ids, test_tokens)
        peak_memory = torch.cuda.max_memory_allocated(self.device)

        memory_used = (peak_memory - initial_memory) / 1024**2  # MB

        print(f"   å‰å‘ä¼ æ’­æ—¶é—´: {avg_time*1000:.2f}ms")
        print(f"   æ˜¾å­˜ä½¿ç”¨: {memory_used:.1f}MB")
        print(f"   è¾“å‡ºæŸå¤±: {outputs.loss.item():.4f}")

        # éªŒè¯äº¤å‰æ³¨æ„åŠ›æ˜¯å¦çœŸæ­£ä½¿ç”¨
        self._verify_cross_attention_usage(model, test_user_ids, test_tokens)

    def _verify_cross_attention_usage(self, model, test_user_ids, test_tokens):
        """éªŒè¯äº¤å‰æ³¨æ„åŠ›æ˜¯å¦çœŸæ­£è¢«ä½¿ç”¨"""
        print(f"ğŸ” éªŒè¯äº¤å‰æ³¨æ„åŠ›ä½¿ç”¨:")

        # å‡†å¤‡è¾“å…¥
        inputs = model.prepare_inputs(test_user_ids, test_tokens)

        # æ£€æŸ¥æ˜¯å¦æœ‰encoder_hidden_states
        has_encoder_states = inputs['encoder_hidden_states'] is not None
        print(f"   encoder_hidden_stateså­˜åœ¨: {'âœ…' if has_encoder_states else 'âŒ'}")

        if has_encoder_states:
            encoder_shape = inputs['encoder_hidden_states'].shape
            print(f"   encoderçŠ¶æ€å½¢çŠ¶: {encoder_shape}")

            # éªŒè¯GPT2æ˜¯å¦çœŸæ­£ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
            # é€šè¿‡hookç›‘æ§äº¤å‰æ³¨æ„åŠ›å±‚çš„æ¿€æ´»
            cross_attn_activations = []

            def hook_fn(module, input, output):
                cross_attn_activations.append(output[0].shape if isinstance(output, tuple) else output.shape)

            # æ³¨å†Œhookåˆ°GPT2çš„äº¤å‰æ³¨æ„åŠ›å±‚
            hooks = []
            for name, module in model.transformer.transformer.h[0].named_modules():
                if 'crossattention' in name.lower():
                    hook = module.register_forward_hook(hook_fn)
                    hooks.append(hook)

            # æ‰§è¡Œå‰å‘ä¼ æ’­
            with torch.no_grad():
                _ = model.transformer(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    encoder_hidden_states=inputs['encoder_hidden_states'],
                    encoder_attention_mask=inputs['encoder_attention_mask'],
                    labels=inputs['labels'],
                )

            # æ¸…ç†hooks
            for hook in hooks:
                hook.remove()

            if cross_attn_activations:
                print(f"   äº¤å‰æ³¨æ„åŠ›æ¿€æ´»: âœ… {len(cross_attn_activations)}å±‚")
                print(f"   æ¿€æ´»å½¢çŠ¶: {cross_attn_activations[0] if cross_attn_activations else 'N/A'}")
            else:
                print(f"   äº¤å‰æ³¨æ„åŠ›æ¿€æ´»: âŒ æœªæ£€æµ‹åˆ°")

        # å¯¹æ¯”æœ‰æ— ç”¨æˆ·æ¡ä»¶çš„è¾“å‡ºå·®å¼‚
        print(f"ğŸ”¬ ç”¨æˆ·æ¡ä»¶å½±å“æµ‹è¯•:")

        # ä½¿ç”¨å®é™…çš„ç”¨æˆ·IDèŒƒå›´ [1-31]
        # ç›¸åŒç”¨æˆ·çš„è¾“å‡º
        same_user_ids = torch.tensor([5, 5, 5, 5], device=self.device)
        with torch.no_grad():
            same_outputs = model(same_user_ids, test_tokens)

        # ä¸åŒç”¨æˆ·çš„è¾“å‡º - ä½¿ç”¨åˆ†æ•£çš„ç”¨æˆ·ID
        diff_user_ids = torch.tensor([1, 10, 20, 31], device=self.device)
        with torch.no_grad():
            diff_outputs = model(diff_user_ids, test_tokens)

        # æç«¯å¯¹æ¯”ï¼šç”¨æˆ·1 vs ç”¨æˆ·31
        extreme_user1 = torch.tensor([1, 1, 1, 1], device=self.device)
        extreme_user31 = torch.tensor([31, 31, 31, 31], device=self.device)
        with torch.no_grad():
            extreme_outputs1 = model(extreme_user1, test_tokens)
            extreme_outputs31 = model(extreme_user31, test_tokens)

        # è®¡ç®—å¤šç§å·®å¼‚æŒ‡æ ‡
        same_logits_std = same_outputs.logits.std().item()
        diff_logits_std = diff_outputs.logits.std().item()

        # è®¡ç®—æç«¯ç”¨æˆ·å·®å¼‚
        extreme_diff = torch.abs(extreme_outputs1.logits - extreme_outputs31.logits).mean().item()

        # è®¡ç®—ç”¨æˆ·åµŒå…¥çš„å·®å¼‚
        user_embed1 = model.user_encoder(torch.tensor([1], device=self.device))
        user_embed31 = model.user_encoder(torch.tensor([31], device=self.device))
        user_embed_diff = torch.abs(user_embed1 - user_embed31).mean().item()

        print(f"   ç›¸åŒç”¨æˆ·è¾“å‡ºæ ‡å‡†å·®: {same_logits_std:.4f}")
        print(f"   ä¸åŒç”¨æˆ·è¾“å‡ºæ ‡å‡†å·®: {diff_logits_std:.4f}")
        print(f"   æç«¯ç”¨æˆ·è¾“å‡ºå·®å¼‚(1 vs 31): {extreme_diff:.4f}")
        print(f"   ç”¨æˆ·åµŒå…¥å·®å¼‚(1 vs 31): {user_embed_diff:.4f}")
        print(f"   ç”¨æˆ·ç¼©æ”¾å› å­: {model.user_scale_factor.item():.4f}")

        # æ›´ä¸¥æ ¼çš„åˆ¤æ–­æ ‡å‡†
        is_significant = (diff_logits_std > same_logits_std * 1.05) or (extreme_diff > 0.01)
        print(f"   ç”¨æˆ·æ¡ä»¶å½±å“: {'âœ…æ˜¾è‘—' if is_significant else 'âŒå¾®å¼±'}")

        return model

    def _update_learning_rate(self):
        """æ›´æ–°å­¦ä¹ ç‡ï¼ŒåŒ…å«warmupæœºåˆ¶"""
        self.current_step += 1

        if self.current_step <= self.warmup_steps:
            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿
            warmup_factor = self.current_step / self.warmup_steps
            current_lr = self.args.learning_rate * warmup_factor

            for param_group in self.optimizer.param_groups:
                param_group['lr'] = current_lr

            if self.current_step % 200 == 0:  # æ¯200æ­¥æ‰“å°ä¸€æ¬¡
                print(f"   Warmupæ­¥éª¤ {self.current_step}/{self.warmup_steps}, LR: {current_lr:.6f}")
        else:
            # Warmupå®Œæˆåï¼Œä½¿ç”¨cosine annealing
            # ç®€åŒ–ï¼šæ¯ä¸ªepochç»“æŸæ—¶è°ƒç”¨scheduler.step()
            pass  # scheduler.step()å°†åœ¨epochç»“æŸæ—¶è°ƒç”¨

    def _check_user_distribution(self, train_dataset, val_dataset, full_dataset):
        """æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç”¨æˆ·åˆ†å¸ƒ"""
        print(f"ğŸ‘¥ æ£€æŸ¥ç”¨æˆ·åˆ†å¸ƒ:")

        # è·å–ç”¨æˆ·åˆ†å¸ƒ - æ£€æŸ¥æ›´å¤šæ ·æœ¬ä»¥ç¡®ä¿å‡†ç¡®æ€§
        def get_user_ids(dataset, max_samples=500):
            user_ids = set()
            # ä½¿ç”¨æ­¥é•¿é‡‡æ ·ï¼Œç¡®ä¿è¦†ç›–æ•´ä¸ªæ•°æ®é›†
            step = max(1, len(dataset) // max_samples)
            indices = list(range(0, len(dataset), step))

            for i in indices:
                try:
                    sample = dataset[i]

                    # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                    if isinstance(sample, dict):
                        user_id = sample['user_id']
                    elif isinstance(sample, (list, tuple)) and len(sample) == 2:
                        _, user_id = sample
                    else:
                        continue

                    user_ids.add(user_id.item() if hasattr(user_id, 'item') else user_id)
                except Exception as e:
                    continue
            return user_ids

        train_users = get_user_ids(train_dataset)
        val_users = get_user_ids(val_dataset)

        print(f"   è®­ç»ƒé›†ç”¨æˆ·: {len(train_users)}ä¸ª {sorted(list(train_users))}")
        print(f"   éªŒè¯é›†ç”¨æˆ·: {len(val_users)}ä¸ª {sorted(list(val_users))}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ç¼ºå¤±
        missing_in_train = val_users - train_users
        missing_in_val = train_users - val_users

        if missing_in_train:
            print(f"   âš ï¸ è®­ç»ƒé›†ç¼ºå°‘ç”¨æˆ·: {sorted(list(missing_in_train))}")
        if missing_in_val:
            print(f"   âš ï¸ éªŒè¯é›†ç¼ºå°‘ç”¨æˆ·: {sorted(list(missing_in_val))}")

        if not missing_in_train and not missing_in_val:
            print(f"   âœ… è®­ç»ƒé›†å’ŒéªŒè¯é›†éƒ½åŒ…å«æ‰€æœ‰ç”¨æˆ·")
        else:
            print(f"   â„¹ï¸ æ³¨æ„ï¼šå¦‚æœä¸Šè¿°è­¦å‘Šå‡ºç°ï¼Œå¯èƒ½æ˜¯é‡‡æ ·æ£€æŸ¥çš„é™åˆ¶ï¼Œå®é™…åˆ†å±‚åˆ’åˆ†å·²ç¡®ä¿æ‰€æœ‰ç”¨æˆ·éƒ½è¢«æ­£ç¡®åˆ†é…")

        print()

    def _stratified_split(self, dataset, train_ratio=0.8):
        """æŒ‰ç”¨æˆ·åˆ†å±‚åˆ’åˆ†æ•°æ®é›†ï¼Œç¡®ä¿æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬éƒ½æŒ‰æ¯”ä¾‹åˆ†é…"""
        print(f"ğŸ”„ æ‰§è¡Œåˆ†å±‚åˆ’åˆ† (ç¡®ä¿æ¯ä¸ªç”¨æˆ·éƒ½åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­)...")

        # æ”¶é›†æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬ç´¢å¼•
        user_indices = {}
        for idx in range(len(dataset)):
            try:
                sample = dataset[idx]

                # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                if isinstance(sample, dict):
                    user_id = sample['user_id']
                elif isinstance(sample, (list, tuple)) and len(sample) == 2:
                    _, user_id = sample
                else:
                    print(f"âš ï¸ æœªçŸ¥çš„æ ·æœ¬æ ¼å¼: {type(sample)}")
                    continue

                user_id = user_id.item() if hasattr(user_id, 'item') else user_id

                if user_id not in user_indices:
                    user_indices[user_id] = []
                user_indices[user_id].append(idx)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ ·æœ¬{idx}æ—¶å‡ºé”™: {e}")
                continue

        print(f"   å‘ç° {len(user_indices)} ä¸ªç”¨æˆ·")

        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ†é…æ ·æœ¬åˆ°è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_indices = []
        val_indices = []

        import random
        random.seed(42)  # å›ºå®šéšæœºç§å­

        for user_id, indices in user_indices.items():
            # éšæœºæ‰“ä¹±è¯¥ç”¨æˆ·çš„æ ·æœ¬
            indices = indices.copy()
            random.shuffle(indices)

            # è®¡ç®—è®­ç»ƒé›†æ ·æœ¬æ•°ï¼ˆè‡³å°‘1ä¸ªï¼‰
            user_train_size = max(1, int(len(indices) * train_ratio))

            # å¦‚æœç”¨æˆ·åªæœ‰1ä¸ªæ ·æœ¬ï¼Œæ”¾åˆ°è®­ç»ƒé›†
            if len(indices) == 1:
                train_indices.extend(indices)
                print(f"   ç”¨æˆ·{user_id}: 1ä¸ªæ ·æœ¬ â†’ è®­ç»ƒé›†")
            else:
                # åˆ†é…æ ·æœ¬
                user_train_indices = indices[:user_train_size]
                user_val_indices = indices[user_train_size:]

                train_indices.extend(user_train_indices)
                val_indices.extend(user_val_indices)

                print(f"   ç”¨æˆ·{user_id}: {len(indices)}ä¸ªæ ·æœ¬ â†’ è®­ç»ƒé›†{len(user_train_indices)}ä¸ª, éªŒè¯é›†{len(user_val_indices)}ä¸ª")

        # éšæœºæ‰“ä¹±æœ€ç»ˆçš„ç´¢å¼•åˆ—è¡¨
        random.shuffle(train_indices)
        random.shuffle(val_indices)

        print(f"âœ… åˆ†å±‚åˆ’åˆ†å®Œæˆ")

        # éªŒè¯åˆ†å±‚åˆ’åˆ†ç»“æœ
        self._verify_stratified_split(train_indices, val_indices, user_indices)

        return train_indices, val_indices

    def _verify_stratified_split(self, train_indices, val_indices, user_indices):
        """éªŒè¯åˆ†å±‚åˆ’åˆ†çš„ç»“æœ"""
        print(f"ğŸ” éªŒè¯åˆ†å±‚åˆ’åˆ†ç»“æœ:")

        # æ£€æŸ¥æ¯ä¸ªç”¨æˆ·åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­çš„åˆ†å¸ƒ
        train_users = set()
        val_users = set()

        for user_id, indices in user_indices.items():
            user_train_count = len([idx for idx in indices if idx in train_indices])
            user_val_count = len([idx for idx in indices if idx in val_indices])

            if user_train_count > 0:
                train_users.add(user_id)
            if user_val_count > 0:
                val_users.add(user_id)

        print(f"   è®­ç»ƒé›†åŒ…å«ç”¨æˆ·: {len(train_users)}ä¸ª {sorted(list(train_users))}")
        print(f"   éªŒè¯é›†åŒ…å«ç”¨æˆ·: {len(val_users)}ä¸ª {sorted(list(val_users))}")

        # æ£€æŸ¥ç¼ºå¤±ç”¨æˆ·
        all_users = set(user_indices.keys())
        missing_in_train = all_users - train_users
        missing_in_val = all_users - val_users

        if missing_in_train:
            print(f"   âŒ è®­ç»ƒé›†ç¼ºå°‘ç”¨æˆ·: {sorted(list(missing_in_train))}")
        if missing_in_val:
            print(f"   âš ï¸ éªŒè¯é›†ç¼ºå°‘ç”¨æˆ·: {sorted(list(missing_in_val))} (å¯èƒ½æ˜¯åªæœ‰1ä¸ªæ ·æœ¬çš„ç”¨æˆ·)")

        if not missing_in_train and not missing_in_val:
            print(f"   âœ… å®Œç¾ï¼šæ‰€æœ‰ç”¨æˆ·éƒ½åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­")
        elif not missing_in_train:
            print(f"   âœ… è‰¯å¥½ï¼šæ‰€æœ‰ç”¨æˆ·éƒ½åœ¨è®­ç»ƒé›†ä¸­ï¼Œ{len(missing_in_val)}ä¸ªç”¨æˆ·åªåœ¨è®­ç»ƒé›†ä¸­")

        print()
        
    def train(self):
        """è®­ç»ƒTransformer"""
        print(f"\nğŸš€ å¼€å§‹Transformerè®­ç»ƒ")
        print(f"   è®­ç»ƒè½®æ•°: {self.args.num_epochs}")
        print(f"   å­¦ä¹ ç‡: {self.args.learning_rate}")
        print(f"   è¯„ä¼°é—´éš”: æ¯5ä¸ªepoch")
        print(f"   å¯è§†åŒ–ç”Ÿæˆ: æ¯5ä¸ªepoch")
        
        # åˆ›å»ºå›¾åƒå˜æ¢ - è½¬æ¢ä¸ºå¼ é‡
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.Resize((self.args.resolution, self.args.resolution)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # å½’ä¸€åŒ–åˆ°[-1, 1]
        ])

        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = MicroDopplerDataset(
            data_dir=self.args.data_dir,
            transform=transform,  # éœ€è¦å˜æ¢å°†PILå›¾åƒè½¬ä¸ºå¼ é‡
            return_user_id=True,  # éœ€è¦ç”¨æˆ·IDè¿›è¡Œæ¡ä»¶ç”Ÿæˆ
        )

        # åˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
        # ç¡®ä¿æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬éƒ½æŒ‰æ¯”ä¾‹åˆ†é…åˆ°è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_indices, val_indices = self._stratified_split(full_dataset, train_ratio=0.8)

        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(full_dataset)}")
        print(f"   è®­ç»ƒé›†: {len(train_indices)} ({len(train_indices)/len(full_dataset)*100:.1f}%)")
        print(f"   éªŒè¯é›†: {len(val_indices)} ({len(val_indices)/len(full_dataset)*100:.1f}%)")

        # åˆ›å»ºå­æ•°æ®é›†
        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)
        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)

        # æ£€æŸ¥ç”¨æˆ·åˆ†å¸ƒ
        self._check_user_distribution(train_dataset, val_dataset, full_dataset)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=self.args.num_workers,
            pin_memory=True
        )

        val_dataloader = DataLoader(
            val_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,  # éªŒè¯é›†ä¸éœ€è¦shuffle
            num_workers=self.args.num_workers,
            pin_memory=True
        )

        # æ£€æŸ¥VQ-VAEè´¨é‡
        self._check_vqvae_quality(train_dataloader)

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹...")

        best_loss = float('inf')
        best_psnr = 0.0

        for epoch in range(self.args.num_epochs):
            self.transformer_model.train()
            total_loss = 0
            
            pbar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{self.args.num_epochs}")
            
            for batch_idx, batch in enumerate(pbar):
                # å¤„ç†ä¸åŒçš„batchæ ¼å¼
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                    user_ids = batch['user_id'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                    images = images.to(self.device)
                    user_ids = user_ids.to(self.device)
                else:
                    print(f"âŒ æœªçŸ¥çš„batchæ ¼å¼: {type(batch)}")
                    continue

                # ç”¨æˆ·IDèŒƒå›´[1,31]ç›´æ¥ä½¿ç”¨ï¼ŒåµŒå…¥å±‚å·²è°ƒæ•´ä¸ºæ”¯æŒè¿™ä¸ªèŒƒå›´
                
                # ä½¿ç”¨VQ-VAEç¼–ç å›¾åƒä¸ºtokenåºåˆ—
                with torch.no_grad():
                    encoded = self.vqvae_model.encode(images, return_dict=True)
                    tokens = encoded['encoding_indices']  # [B, H, W] - VQ-VAEè¾“å‡ºçš„2D token map

                    # æ£€æŸ¥tokenå€¼èŒƒå›´
                    min_token = tokens.min().item()
                    max_token = tokens.max().item()
                    if min_token < 0 or max_token >= self.args.codebook_size:
                        print(f"âŒ Tokenå€¼è¶…å‡ºèŒƒå›´: [{min_token}, {max_token}], è·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue

                    # å±•å¹³ä¸ºåºåˆ— [B, H*W] - å¯¹äº128x128å›¾åƒï¼Œ4å€ä¸‹é‡‡æ ·åæ˜¯32x32=1024
                    batch_size = tokens.shape[0]
                    tokens = tokens.view(batch_size, -1)  # [B, 1024]
                
                # Transformerè®­ç»ƒ
                self.optimizer.zero_grad()
                
                # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡ - ç¡®ä¿é•¿åº¦åŒ¹é…
                # MicroDopplerTransformerä¼šåœ¨å†…éƒ¨æ·»åŠ ç”¨æˆ·tokenå¹¶å¤„ç†åºåˆ—
                # æˆ‘ä»¬ç›´æ¥ä¼ é€’å®Œæ•´çš„tokenåºåˆ—
                input_tokens = tokens  # å®Œæ•´çš„tokenåºåˆ— [B, 1024]
                
                # å‰å‘ä¼ æ’­
                outputs = self.transformer_model(
                    user_ids=user_ids,
                    token_sequences=input_tokens
                )
                
                # ä½¿ç”¨Transformerå†…éƒ¨è®¡ç®—çš„æŸå¤±
                loss = outputs.loss

                # æ·»åŠ ç©ºé—´ä¸€è‡´æ€§æŸå¤±
                spatial_loss = self._compute_spatial_consistency_loss(input_tokens)
                loss = loss + 0.1 * spatial_loss  # æƒé‡0.1
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.transformer_model.parameters(), 1.0)
                self.optimizer.step()

                # æ›´æ–°å­¦ä¹ ç‡ï¼ˆåŒ…å«warmupï¼‰
                self._update_learning_rate()
                
                total_loss += loss.item()
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
                
                # å®šæœŸä¿å­˜checkpointï¼ˆå‡å°‘é¢‘ç‡ï¼‰
                if batch_idx % 1000 == 0 and batch_idx > 0:
                    self._save_checkpoint(epoch, batch_idx, loss.item())
            
            # Warmupå®Œæˆåï¼Œæ¯ä¸ªepochç»“æŸæ—¶æ›´æ–°cosine scheduler
            if self.current_step > self.warmup_steps:
                self.scheduler.step()
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"   Cosine LR: {current_lr:.6f}")

            avg_loss = total_loss / len(train_dataloader)
            print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

            # æ¯5ä¸ªepochè¿›è¡Œè¯„ä¼°å’Œå¯è§†åŒ–
            if (epoch + 1) % 5 == 0:
                print(f"\nğŸ“Š ç¬¬{epoch+1}è½®è¯„ä¼°:")

                # è¯„ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰
                eval_metrics = self.evaluate(val_dataloader)
                print(f"   éªŒè¯æŸå¤±: {eval_metrics['loss']:.4f}")
                print(f"   PSNR: {eval_metrics['psnr']:.2f} dB")
                print(f"   è¯„ä¼°æ ·æœ¬æ•°: {eval_metrics['num_samples']}")

                # ç”Ÿæˆå¯è§†åŒ–æ ·æœ¬
                self.generate_and_save_samples(epoch)

                # ä¿å­˜æœ€ä½³PSNRæ¨¡å‹
                if eval_metrics['psnr'] > best_psnr:
                    best_psnr = eval_metrics['psnr']
                    self._save_best_model(epoch, eval_metrics['psnr'], eval_metrics['loss'])



                print()  # ç©ºè¡Œåˆ†éš”

            # ä¿å­˜åŸºäºè®­ç»ƒæŸå¤±çš„æœ€ä½³æ¨¡å‹ï¼ˆå¤‡ç”¨ï¼‰
            if avg_loss < best_loss:
                best_loss = avg_loss
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_model("final_model.pth", self.args.num_epochs-1, avg_loss)
        print(f"âœ… Transformerè®­ç»ƒå®Œæˆ")
        print(f"ğŸ† æœ€ä½³PSNR: {best_psnr:.2f} dB")
        
    def _save_checkpoint(self, epoch, batch_idx, loss):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'batch_idx': batch_idx,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'args': self.args,
        }
        
        checkpoint_path = self.output_dir / f"checkpoint_epoch_{epoch}_batch_{batch_idx}.pth"
        torch.save(checkpoint, checkpoint_path)
        
    def _save_model(self, filename, epoch, loss):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'epoch': epoch,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'args': self.args,
        }
        
        model_path = self.output_dir / filename
        torch.save(model_data, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.transformer_model.eval()
        total_loss = 0.0
        num_batches = 0

        # ç”¨äºè®¡ç®—PSNRçš„æ ·æœ¬
        original_images = []
        generated_images = []

        print("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")

        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx >= 50:  # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡
                    break

                # å¤„ç†ä¸åŒçš„batchæ ¼å¼
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                    user_ids = batch['user_id'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                    images = images.to(self.device)
                    user_ids = user_ids.to(self.device)
                else:
                    print(f"âŒ æœªçŸ¥çš„batchæ ¼å¼: {type(batch)}")
                    continue

                # æ£€æŸ¥tokenå€¼èŒƒå›´
                encoded = self.vqvae_model.encode(images, return_dict=True)
                tokens = encoded['encoding_indices']

                min_token = tokens.min().item()
                max_token = tokens.max().item()
                if min_token < 0 or max_token >= self.args.codebook_size:
                    continue

                # å±•å¹³tokens
                batch_size = tokens.shape[0]
                tokens = tokens.view(batch_size, -1)
                input_tokens = tokens

                # è®¡ç®—æŸå¤±
                outputs = self.transformer_model(
                    user_ids=user_ids,
                    token_sequences=input_tokens
                )

                total_loss += outputs.loss.item()
                num_batches += 1

                # æ”¶é›†å‰å‡ ä¸ªæ ·æœ¬ç”¨äºPSNRè®¡ç®—
                if batch_idx < 5:
                    # ç”Ÿæˆå›¾åƒ
                    generated_tokens = self._generate_images(user_ids[:4])
                    if generated_tokens is not None:
                        # è§£ç ç”Ÿæˆçš„tokens
                        generated_imgs = self._decode_tokens(generated_tokens)
                        if generated_imgs is not None:
                            original_images.append(images[:4].cpu())
                            generated_images.append(generated_imgs.cpu())

        self.transformer_model.train()

        avg_loss = total_loss / max(num_batches, 1)

        # è®¡ç®—PSNR
        psnr = self._calculate_psnr(original_images, generated_images)

        return {
            'loss': avg_loss,
            'psnr': psnr,
            'num_samples': num_batches * self.args.batch_size
        }

    def _generate_images(self, user_ids, max_length=1024):
        """ç”Ÿæˆå›¾åƒtokens"""
        try:
            with torch.no_grad():
                # ä½¿ç”¨Transformerç”Ÿæˆtokens
                batch_size = user_ids.shape[0]
                device = user_ids.device

                # å¼€å§‹tokenï¼ˆç”¨æˆ·tokenï¼‰
                generated = torch.full((batch_size, 1), self.transformer_model.user_token_id, device=device)

                # é€æ­¥ç”Ÿæˆ
                for step in range(max_length):
                    # å‡†å¤‡è¾“å…¥
                    inputs = self.transformer_model.prepare_inputs(user_ids, None)

                    # æ›´æ–°input_idsä¸ºå½“å‰ç”Ÿæˆçš„åºåˆ—
                    inputs['input_ids'] = generated
                    inputs['attention_mask'] = torch.ones_like(generated)

                    # å‰å‘ä¼ æ’­
                    if self.transformer_model.use_cross_attention:
                        outputs = self.transformer_model.transformer(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            encoder_hidden_states=inputs['encoder_hidden_states'],
                            encoder_attention_mask=inputs['encoder_attention_mask'],
                        )
                    else:
                        outputs = self.transformer_model.transformer(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                        )

                    # è·å–ä¸‹ä¸€ä¸ªtokençš„logits - ä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥
                    next_token_logits = outputs.logits[:, -1, :]

                    # é™åˆ¶logitsåˆ°æœ‰æ•ˆçš„tokenèŒƒå›´ [0, codebook_size-1]
                    if next_token_logits.shape[-1] > self.args.codebook_size:
                        next_token_logits = next_token_logits[:, :self.args.codebook_size]

                    # ä½¿ç”¨æ›´ä½çš„æ¸©åº¦ä»¥å‡å°‘éšæœºæ€§
                    temperature = max(0.3, self.args.generation_temperature * 0.5)
                    next_token_logits = next_token_logits / temperature

                    # Top-ké‡‡æ ·ä»¥é¿å…æç«¯token
                    k = min(50, next_token_logits.shape[-1] // 4)  # é™åˆ¶é€‰æ‹©èŒƒå›´
                    top_k_logits, top_k_indices = torch.topk(next_token_logits, k, dim=-1)

                    # åœ¨top-kä¸­é‡‡æ ·
                    top_k_probs = F.softmax(top_k_logits, dim=-1)
                    sampled_indices = torch.multinomial(top_k_probs, num_samples=1)  # [batch_size, 1]
                    next_token = torch.gather(top_k_indices, -1, sampled_indices)  # [batch_size, 1]

                    # ç¡®ä¿tokenåœ¨æœ‰æ•ˆèŒƒå›´å†…
                    next_token = torch.clamp(next_token, 0, self.args.codebook_size - 1)

                    # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                    generated = torch.cat([generated, next_token], dim=1)

                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡é•¿åº¦
                    if generated.shape[1] >= max_length + 1:
                        break

                # ç§»é™¤ç”¨æˆ·tokenï¼Œè¿”å›å›¾åƒtokens
                image_tokens = generated[:, 1:]  # å»æ‰ç¬¬ä¸€ä¸ªç”¨æˆ·token

                # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
                if image_tokens.shape[1] < max_length:
                    # å¡«å……åˆ°æ­£ç¡®é•¿åº¦
                    padding = torch.zeros(batch_size, max_length - image_tokens.shape[1], device=device, dtype=torch.long)
                    image_tokens = torch.cat([image_tokens, padding], dim=1)
                elif image_tokens.shape[1] > max_length:
                    # æˆªæ–­åˆ°æ­£ç¡®é•¿åº¦
                    image_tokens = image_tokens[:, :max_length]

                return image_tokens

        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆå›¾åƒå¤±è´¥: {e}")
            return None

    def _decode_tokens(self, tokens):
        """å°†tokensè§£ç ä¸ºå›¾åƒ"""
        try:
            with torch.no_grad():
                # ç¡®ä¿tokensæ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹
                if tokens.dtype != torch.long:
                    tokens = tokens.long()

                # æ£€æŸ¥å’Œä¿®å¤tokenå€¼èŒƒå›´
                min_token = tokens.min().item()
                max_token = tokens.max().item()

                # è¿‡æ»¤æ‰ç‰¹æ®Štokenï¼ˆç”¨æˆ·token ID = codebook_sizeï¼‰
                special_token_mask = tokens >= self.args.codebook_size
                if special_token_mask.any():
                    print(f"âš ï¸ å‘ç°{special_token_mask.sum().item()}ä¸ªç‰¹æ®Štokenï¼Œå°†æ›¿æ¢ä¸ºéšæœºæœ‰æ•ˆtoken")
                    # å°†ç‰¹æ®Štokenæ›¿æ¢ä¸ºéšæœºçš„æœ‰æ•ˆtoken
                    random_tokens = torch.randint(0, self.args.codebook_size,
                                                 special_token_mask.shape,
                                                 device=tokens.device)
                    tokens = torch.where(special_token_mask, random_tokens, tokens)

                # å†æ¬¡æ£€æŸ¥èŒƒå›´
                min_token = tokens.min().item()
                max_token = tokens.max().item()
                if min_token < 0 or max_token >= self.args.codebook_size:
                    print(f"âš ï¸ Tokenå€¼ä»è¶…å‡ºèŒƒå›´: [{min_token}, {max_token}]")
                    return None

                print(f"âœ… TokenèŒƒå›´æ­£å¸¸: [{min_token}, {max_token}]")

                # é‡å¡‘ä¸º2D token map
                batch_size = tokens.shape[0]
                tokens_2d = tokens.view(batch_size, 32, 32)  # 32x32 token map

                # å°†token indicesè½¬æ¢ä¸ºembeddings
                # è·å–VQ-VAEçš„é‡åŒ–å™¨
                quantizer = self.vqvae_model.quantize

                # å°†token indicesè½¬æ¢ä¸ºembeddings
                # tokens_2d: [B, H, W] -> embeddings: [B, H, W, D]
                embeddings = quantizer.embedding(tokens_2d)  # [B, 32, 32, 256]

                # è½¬æ¢ä¸ºVQ-VAEæœŸæœ›çš„æ ¼å¼: [B, D, H, W]
                embeddings = embeddings.permute(0, 3, 1, 2)  # [B, 256, 32, 32]

                # ä½¿ç”¨VQ-VAEè§£ç  - è·³è¿‡é‡æ–°é‡åŒ–ï¼
                decoded_images = self.vqvae_model.decode(embeddings, force_not_quantize=True)

                return decoded_images

        except Exception as e:
            print(f"âš ï¸ è§£ç tokenså¤±è´¥: {e}")
            # æ‰“å°æ›´å¤šè°ƒè¯•ä¿¡æ¯
            if 'tokens' in locals():
                print(f"   tokenså½¢çŠ¶: {tokens.shape}")
                print(f"   tokensç±»å‹: {tokens.dtype}")
                print(f"   tokensèŒƒå›´: [{tokens.min().item()}, {tokens.max().item()}]")
            return None

    def _calculate_psnr(self, original_images, generated_images):
        """è®¡ç®—PSNR"""
        if not original_images or not generated_images:
            return 0.0

        try:
            import torch.nn.functional as F

            # åˆå¹¶æ‰€æœ‰å›¾åƒ
            orig = torch.cat(original_images, dim=0)
            gen = torch.cat(generated_images, dim=0)

            # ç¡®ä¿å½¢çŠ¶åŒ¹é…
            if orig.shape != gen.shape:
                gen = F.interpolate(gen, size=orig.shape[-2:], mode='bilinear', align_corners=False)

            # å½’ä¸€åŒ–åˆ°[0,1]
            orig = (orig + 1) / 2  # ä»[-1,1]åˆ°[0,1]
            gen = (gen + 1) / 2

            # è®¡ç®—MSE
            mse = F.mse_loss(gen, orig)

            # è®¡ç®—PSNR
            if mse > 0:
                psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))
                return psnr.item()
            else:
                return 100.0  # å®Œç¾åŒ¹é…

        except Exception as e:
            print(f"âš ï¸ PSNRè®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def generate_and_save_samples(self, epoch, num_users=4):
        """ç”Ÿæˆå¹¶ä¿å­˜æ ·æœ¬å›¾åƒ"""
        print(f"ğŸ¨ ç”Ÿæˆç¬¬{epoch+1}è½®æ ·æœ¬å›¾åƒ...")

        self.transformer_model.eval()

        try:
            # é€‰æ‹©ä¸åŒçš„ç”¨æˆ·IDè¿›è¡Œç”Ÿæˆ
            user_ids = torch.tensor([1, 8, 16, 31], device=self.device)[:num_users]

            with torch.no_grad():
                # ç”Ÿæˆå›¾åƒtokens
                generated_tokens = self._generate_images(user_ids)

                if generated_tokens is not None:
                    # è§£ç ä¸ºå›¾åƒ
                    generated_images = self._decode_tokens(generated_tokens)

                    if generated_images is not None:
                        # ä¿å­˜å›¾åƒ
                        self._save_sample_images(generated_images, user_ids, epoch)
                        print(f"âœ… æ ·æœ¬å›¾åƒå·²ä¿å­˜")
                    else:
                        print(f"âŒ å›¾åƒè§£ç å¤±è´¥")
                else:
                    print(f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥")

        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ ·æœ¬å¤±è´¥: {e}")

        self.transformer_model.train()

    def _save_sample_images(self, images, user_ids, epoch):
        """ä¿å­˜æ ·æœ¬å›¾åƒ"""
        import matplotlib.pyplot as plt
        import numpy as np

        # åˆ›å»ºæ ·æœ¬ç›®å½•
        samples_dir = self.output_dir / "samples"
        samples_dir.mkdir(exist_ok=True)

        # è½¬æ¢å›¾åƒæ ¼å¼
        images = images.cpu().numpy()
        images = (images + 1) / 2  # ä»[-1,1]åˆ°[0,1]
        images = np.clip(images, 0, 1)

        # åˆ›å»ºç½‘æ ¼å›¾åƒ
        fig, axes = plt.subplots(1, len(user_ids), figsize=(4*len(user_ids), 4))
        if len(user_ids) == 1:
            axes = [axes]

        for i, (img, user_id) in enumerate(zip(images, user_ids)):
            # è½¬æ¢ä¸ºç°åº¦å›¾åƒï¼ˆå¦‚æœæ˜¯3é€šé“ï¼‰
            if img.shape[0] == 3:
                img = np.mean(img, axis=0)
            else:
                img = img[0]

            axes[i].imshow(img, cmap='viridis')
            axes[i].set_title(f'User {user_id.item()}')
            axes[i].axis('off')

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        save_path = samples_dir / f"epoch_{epoch+1:03d}.png"
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()

        print(f"ğŸ“¸ æ ·æœ¬å›¾åƒä¿å­˜è‡³: {save_path}")

    def _save_best_model(self, epoch, psnr, loss):
        """ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œåˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹"""
        # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹
        old_best_files = list(self.output_dir.glob("best_model_*.pth"))
        for old_file in old_best_files:
            try:
                old_file.unlink()
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹: {old_file.name}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤æ—§æ¨¡å‹å¤±è´¥: {e}")

        # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
        model_data = {
            'epoch': epoch,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'psnr': psnr,
            'loss': loss,
            'args': self.args,
        }

        best_model_path = self.output_dir / f"best_model_epoch_{epoch+1:03d}_psnr_{psnr:.2f}.pth"
        torch.save(model_data, best_model_path)
        print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path.name} (PSNR: {psnr:.2f} dB)")

    def _save_checkpoint_to_kaggle(self, epoch, loss):
        """ä¿å­˜checkpointåˆ°Kaggleå·¥ä½œç›®å½•"""
        try:
            kaggle_output_dir = Path("/kaggle/working")
            if kaggle_output_dir.exists():
                checkpoint_data = {
                    'epoch': epoch,
                    'model_state_dict': self.transformer_model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'loss': loss,
                    'args': self.args,
                }

                checkpoint_path = kaggle_output_dir / f"transformer_checkpoint_epoch_{epoch+1:03d}.pth"
                torch.save(checkpoint_data, checkpoint_path)
                print(f"ğŸ’¾ Checkpointå·²ä¿å­˜: {checkpoint_path.name}")

                # åªä¿ç•™æœ€è¿‘çš„3ä¸ªcheckpoint
                checkpoints = sorted(kaggle_output_dir.glob("transformer_checkpoint_*.pth"))
                if len(checkpoints) > 3:
                    for old_checkpoint in checkpoints[:-3]:
                        old_checkpoint.unlink()
                        print(f"ğŸ—‘ï¸ åˆ é™¤æ—§checkpoint: {old_checkpoint.name}")

        except Exception as e:
            print(f"âš ï¸ Kaggle checkpointä¿å­˜å¤±è´¥: {e}")

    def _compute_spatial_consistency_loss(self, tokens):
        """è®¡ç®—ç©ºé—´ä¸€è‡´æ€§æŸå¤±ï¼Œé¼“åŠ±ç›¸é‚»tokençš„ç›¸ä¼¼æ€§"""
        try:
            batch_size = tokens.shape[0]
            # é‡å¡‘ä¸º2D: [B, 32, 32]
            tokens_2d = tokens.view(batch_size, 32, 32).float()

            # è®¡ç®—æ°´å¹³æ–¹å‘çš„å·®å¼‚
            horizontal_diff = torch.abs(tokens_2d[:, :, 1:] - tokens_2d[:, :, :-1])

            # è®¡ç®—å‚ç›´æ–¹å‘çš„å·®å¼‚
            vertical_diff = torch.abs(tokens_2d[:, 1:, :] - tokens_2d[:, :-1, :])

            # æ€»çš„ç©ºé—´ä¸€è‡´æ€§æŸå¤±ï¼ˆé¼“åŠ±å¹³æ»‘æ€§ï¼‰
            spatial_loss = torch.mean(horizontal_diff) + torch.mean(vertical_diff)

            return spatial_loss

        except Exception as e:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›0
            return torch.tensor(0.0, device=tokens.device)

    def _check_vqvae_quality(self, dataloader):
        """æ£€æŸ¥VQ-VAEçš„ç¼–ç è´¨é‡å’Œç æœ¬ä½¿ç”¨æƒ…å†µ"""
        print(f"ğŸ” æ£€æŸ¥VQ-VAEè´¨é‡:")

        self.vqvae_model.eval()
        all_tokens = []
        reconstruction_errors = []

        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                if i >= 10:  # åªæ£€æŸ¥å‰10ä¸ªbatch
                    break

                # å¤„ç†ä¸åŒçš„batchæ ¼å¼
                if isinstance(batch, dict):
                    # å­—å…¸æ ¼å¼
                    images = batch['image'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    # å…ƒç»„æ ¼å¼ (image, user_id)
                    images, user_ids = batch
                    images = images.to(self.device)
                else:
                    print(f"   âš ï¸ æœªçŸ¥çš„batchæ ¼å¼: {type(batch)}")
                    continue

                # VQ-VAEç¼–ç å’Œè§£ç 
                encoded = self.vqvae_model.encode(images, return_dict=True)

                # å¤„ç†ç¼–ç ç»“æœ
                if isinstance(encoded, dict):
                    latents = encoded['latents']
                    indices = encoded['encoding_indices']
                elif hasattr(encoded, 'latents'):
                    latents = encoded.latents
                    indices = getattr(encoded, 'encoding_indices', None)
                else:
                    latents = encoded
                    indices = None

                # è§£ç ï¼ˆè·³è¿‡é‡æ–°é‡åŒ–ï¼Œå› ä¸ºlatentså·²ç»æ˜¯é‡åŒ–åçš„ï¼‰
                reconstructed = self.vqvae_model.decode(latents, force_not_quantize=True)

                # è®¡ç®—é‡å»ºè¯¯å·®
                mse = torch.nn.functional.mse_loss(reconstructed, images)
                reconstruction_errors.append(mse.item())

                # æ”¶é›†tokens
                if indices is not None:
                    all_tokens.extend(indices.flatten().cpu().numpy())

        # åˆ†æç»“æœ
        avg_reconstruction_error = np.mean(reconstruction_errors)
        print(f"   å¹³å‡é‡å»ºè¯¯å·®: {avg_reconstruction_error:.6f}")

        if all_tokens:
            unique_tokens = len(set(all_tokens))
            total_possible = self.args.codebook_size
            usage_ratio = unique_tokens / total_possible
            print(f"   ç æœ¬ä½¿ç”¨ç‡: {unique_tokens}/{total_possible} ({usage_ratio:.2%})")

            # æ£€æŸ¥tokenåˆ†å¸ƒ
            token_counts = np.bincount(all_tokens, minlength=total_possible)
            most_used = np.argmax(token_counts)
            least_used = np.argmin(token_counts[token_counts > 0]) if np.any(token_counts > 0) else 0

            print(f"   æœ€å¸¸ç”¨token: {most_used} (ä½¿ç”¨{token_counts[most_used]}æ¬¡)")
            print(f"   æœ€å°‘ç”¨token: {least_used} (ä½¿ç”¨{token_counts[least_used]}æ¬¡)")

            # è­¦å‘Š
            if usage_ratio < 0.1:
                print(f"   âš ï¸ è­¦å‘Šï¼šç æœ¬ä½¿ç”¨ç‡è¿‡ä½ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆå¤šæ ·æ€§ä¸è¶³")
            if avg_reconstruction_error > 0.1:
                print(f"   âš ï¸ è­¦å‘Šï¼šé‡å»ºè¯¯å·®è¿‡é«˜ï¼ŒVQ-VAEè´¨é‡å¯èƒ½æœ‰é—®é¢˜")

        self.vqvae_model.train()

def main():
    parser = argparse.ArgumentParser(description="Transformerè®­ç»ƒè„šæœ¬")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--vqvae_path", type=str, required=True, help="VQ-VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--resolution", type=int, default=128, help="å›¾åƒåˆ†è¾¨ç‡")
    parser.add_argument("--codebook_size", type=int, default=1024, help="ç æœ¬å¤§å°")
    parser.add_argument("--num_users", type=int, default=31, help="ç”¨æˆ·æ•°é‡")

    # Transformeræ¶æ„å‚æ•°
    parser.add_argument("--n_embd", type=int, default=512, help="TransformeråµŒå…¥ç»´åº¦")
    parser.add_argument("--n_layer", type=int, default=8, help="Transformerå±‚æ•°")
    parser.add_argument("--n_head", type=int, default=8, help="æ³¨æ„åŠ›å¤´æ•°")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--device", type=str, default="cuda", help="è®­ç»ƒè®¾å¤‡")

    # ä¿å­˜å’Œé‡‡æ ·å‚æ•°
    parser.add_argument("--save_interval", type=int, default=10, help="æ¨¡å‹ä¿å­˜é—´éš”")
    parser.add_argument("--sample_interval", type=int, default=10, help="æ ·æœ¬ç”Ÿæˆé—´éš”")
    parser.add_argument("--generation_temperature", type=float, default=1.0, help="ç”Ÿæˆæ¸©åº¦")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = TransformerTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
