#!/usr/bin/env python3
"""
Transformerè®­ç»ƒè„šæœ¬
ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒTransformerå­¦ä¹ ä»ç”¨æˆ·IDç”Ÿæˆtokenåºåˆ—
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm
import numpy as np
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from vqvae_transformer.models.transformer_model import MicroDopplerTransformer
from vqvae_transformer.models.vqvae_model import MicroDopplerVQVAE
from vqvae_transformer.utils.data_loader import MicroDopplerDataset

class TransformerTrainer:
    """Transformerè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device(args.device if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ¤– Transformerè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   VQ-VAEè·¯å¾„: {args.vqvae_path}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        
        # åŠ è½½VQ-VAEæ¨¡å‹
        self.vqvae_model = self._load_vqvae_model()
        
        # åˆ›å»ºTransformeræ¨¡å‹
        self.transformer_model = self._create_transformer_model()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.transformer_model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
    def _load_vqvae_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹"""
        vqvae_path = Path(self.args.vqvae_path)
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        model_files = list(vqvae_path.glob("*.pth"))
        if not model_files:
            raise FileNotFoundError(f"åœ¨ {vqvae_path} ä¸­æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹æ–‡ä»¶")
        
        model_file = model_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ¨¡å‹æ–‡ä»¶
        print(f"ğŸ“‚ åŠ è½½VQ-VAEæ¨¡å‹: {model_file}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(model_file, map_location=self.device)
        
        # é‡å»ºVQ-VAEæ¨¡å‹
        vqvae_model = MicroDopplerVQVAE(
            num_vq_embeddings=checkpoint['args'].codebook_size,
            commitment_cost=checkpoint['args'].commitment_cost,
            ema_decay=getattr(checkpoint['args'], 'ema_decay', 0.99),
        )
        
        # åŠ è½½æƒé‡
        vqvae_model.load_state_dict(checkpoint['model_state_dict'])
        vqvae_model.to(self.device)
        vqvae_model.eval()
        
        print(f"âœ… VQ-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        return vqvae_model
        
    def _create_transformer_model(self):
        """åˆ›å»ºTransformeræ¨¡å‹"""
        model = MicroDopplerTransformer(
            vocab_size=self.args.codebook_size,
            max_seq_len=self.args.resolution * self.args.resolution // 16,  # å‡è®¾16å€ä¸‹é‡‡æ ·
            d_model=512,
            nhead=8,
            num_layers=6,
            num_users=self.args.num_users,
        )
        model.to(self.device)
        
        print(f"âœ… Transformeræ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.args.codebook_size}")
        print(f"   åºåˆ—é•¿åº¦: {model.max_seq_len}")
        print(f"   ç”¨æˆ·æ•°é‡: {self.args.num_users}")
        
        return model
        
    def train(self):
        """è®­ç»ƒTransformer"""
        print(f"\nğŸš€ å¼€å§‹Transformerè®­ç»ƒ")
        print(f"   è®­ç»ƒè½®æ•°: {self.args.num_epochs}")
        print(f"   å­¦ä¹ ç‡: {self.args.learning_rate}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = MicroDopplerDataset(
            data_dir=self.args.data_dir,
            resolution=self.args.resolution,
            num_users=self.args.num_users
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=self.args.num_workers,
            pin_memory=True
        )
        
        best_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            self.transformer_model.train()
            total_loss = 0
            
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{self.args.num_epochs}")
            
            for batch_idx, (images, user_ids) in enumerate(pbar):
                images = images.to(self.device)
                user_ids = user_ids.to(self.device)
                
                # ä½¿ç”¨VQ-VAEç¼–ç å›¾åƒä¸ºtokenåºåˆ—
                with torch.no_grad():
                    encoded = self.vqvae_model.encode(images)
                    tokens = encoded['encoding_indices']  # [B, H*W]
                
                # Transformerè®­ç»ƒ
                self.optimizer.zero_grad()
                
                # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                input_tokens = tokens[:, :-1]  # é™¤äº†æœ€åä¸€ä¸ªtoken
                target_tokens = tokens[:, 1:]  # é™¤äº†ç¬¬ä¸€ä¸ªtoken
                
                # å‰å‘ä¼ æ’­
                outputs = self.transformer_model(
                    input_ids=input_tokens,
                    user_ids=user_ids
                )
                
                # è®¡ç®—æŸå¤±
                loss = nn.CrossEntropyLoss()(
                    outputs.logits.reshape(-1, self.args.codebook_size),
                    target_tokens.reshape(-1)
                )
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.transformer_model.parameters(), 1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
                
                # å®šæœŸä¿å­˜
                if batch_idx % 500 == 0:
                    self._save_checkpoint(epoch, batch_idx, loss.item())
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self._save_model("best_model.pth", epoch, avg_loss)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_model("final_model.pth", self.args.num_epochs-1, avg_loss)
        print(f"âœ… Transformerè®­ç»ƒå®Œæˆ")
        
    def _save_checkpoint(self, epoch, batch_idx, loss):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'batch_idx': batch_idx,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'args': self.args,
        }
        
        checkpoint_path = self.output_dir / f"checkpoint_epoch_{epoch}_batch_{batch_idx}.pth"
        torch.save(checkpoint, checkpoint_path)
        
    def _save_model(self, filename, epoch, loss):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'epoch': epoch,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'args': self.args,
        }
        
        model_path = self.output_dir / filename
        torch.save(model_data, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

def main():
    parser = argparse.ArgumentParser(description="Transformerè®­ç»ƒè„šæœ¬")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--vqvae_path", type=str, required=True, help="VQ-VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--resolution", type=int, default=128, help="å›¾åƒåˆ†è¾¨ç‡")
    parser.add_argument("--codebook_size", type=int, default=1024, help="ç æœ¬å¤§å°")
    parser.add_argument("--num_users", type=int, default=31, help="ç”¨æˆ·æ•°é‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--device", type=str, default="cuda", help="è®­ç»ƒè®¾å¤‡")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = TransformerTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
