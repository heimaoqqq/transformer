#!/usr/bin/env python3
"""
Transformerè®­ç»ƒè„šæœ¬
ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒTransformerå­¦ä¹ ä»ç”¨æˆ·IDç”Ÿæˆtokenåºåˆ—
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm
import numpy as np
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.transformer_model import MicroDopplerTransformer
from models.vqvae_model import MicroDopplerVQVAE
from utils.data_loader import MicroDopplerDataset

class TransformerTrainer:
    """Transformerè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device(args.device if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ¤– Transformerè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   VQ-VAEè·¯å¾„: {args.vqvae_path}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        
        # åŠ è½½VQ-VAEæ¨¡å‹
        self.vqvae_model = self._load_vqvae_model()
        
        # åˆ›å»ºTransformeræ¨¡å‹
        self.transformer_model = self._create_transformer_model()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.transformer_model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
    def _load_vqvae_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹"""
        vqvae_path = Path(self.args.vqvae_path)

        # æ£€æŸ¥æ˜¯å¦ç›´æ¥åŒ…å«diffusersæ ¼å¼æ–‡ä»¶ (config.json + safetensors)
        config_file = vqvae_path / "config.json"
        safetensors_file = vqvae_path / "diffusion_pytorch_model.safetensors"

        if config_file.exists() and safetensors_file.exists():
            print(f"ğŸ“‚ åŠ è½½VQ-VAEæ¨¡å‹ (ç›´æ¥diffusersæ ¼å¼): {vqvae_path}")
            try:
                from models.vqvae_model import MicroDopplerVQVAE
                vqvae_model = MicroDopplerVQVAE.from_pretrained(vqvae_path)
                vqvae_model.to(self.device)
                vqvae_model.eval()
                print("âœ… æˆåŠŸåŠ è½½ç›´æ¥diffusersæ ¼å¼æ¨¡å‹")
                return vqvae_model
            except Exception as e:
                print(f"âš ï¸ ç›´æ¥diffusersæ ¼å¼åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•final_modelå­ç›®å½•...")

        # å°è¯•final_modelå­ç›®å½• (diffusersæ ¼å¼)
        final_model_path = vqvae_path / "final_model"
        if final_model_path.exists():
            print(f"ğŸ“‚ åŠ è½½VQ-VAEæ¨¡å‹ (final_modelå­ç›®å½•): {final_model_path}")
            try:
                from models.vqvae_model import MicroDopplerVQVAE
                vqvae_model = MicroDopplerVQVAE.from_pretrained(final_model_path)
                vqvae_model.to(self.device)
                vqvae_model.eval()
                print("âœ… æˆåŠŸåŠ è½½final_modelå­ç›®å½•æ ¼å¼æ¨¡å‹")
                return vqvae_model
            except Exception as e:
                print(f"âš ï¸ final_modelå­ç›®å½•æ ¼å¼åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•checkpointæ ¼å¼...")

        # å¤‡é€‰ï¼šä½¿ç”¨checkpointæ–‡ä»¶
        best_model_path = vqvae_path / "best_model.pth"
        if best_model_path.exists():
            model_file = best_model_path
        else:
            # æŸ¥æ‰¾å…¶ä»–checkpointæ–‡ä»¶
            model_files = list(vqvae_path.glob("*.pth"))
            if not model_files:
                raise FileNotFoundError(f"åœ¨ {vqvae_path} ä¸­æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹æ–‡ä»¶")
            model_file = model_files[0]

        print(f"ğŸ“‚ åŠ è½½VQ-VAEæ¨¡å‹ (checkpointæ ¼å¼): {model_file}")

        # åŠ è½½checkpoint
        checkpoint = torch.load(model_file, map_location=self.device)

        # é‡å»ºVQ-VAEæ¨¡å‹
        from models.vqvae_model import MicroDopplerVQVAE
        vqvae_model = MicroDopplerVQVAE(
            num_vq_embeddings=checkpoint['args'].codebook_size,
            commitment_cost=checkpoint['args'].commitment_cost,
            ema_decay=getattr(checkpoint['args'], 'ema_decay', 0.99),
        )

        # åŠ è½½æƒé‡
        vqvae_model.load_state_dict(checkpoint['model_state_dict'])
        vqvae_model.to(self.device)
        vqvae_model.eval()
        print("âœ… æˆåŠŸåŠ è½½checkpointæ ¼å¼æ¨¡å‹")
        
        print(f"âœ… VQ-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        return vqvae_model
        
    def _create_transformer_model(self):
        """åˆ›å»ºTransformeræ¨¡å‹"""
        model = MicroDopplerTransformer(
            vocab_size=self.args.codebook_size,
            max_seq_len=self.args.resolution * self.args.resolution // 16,  # VQ-VAEå®é™…æ˜¯4å€ä¸‹é‡‡æ ·: (128//4)^2 = 32^2 = 1024
            num_users=self.args.num_users,
            n_embd=self.args.n_embd,
            n_layer=self.args.n_layer,
            n_head=self.args.n_head,
            dropout=0.1,
            use_cross_attention=True,
        )
        model.to(self.device)

        print(f"âœ… Transformeræ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {self.args.codebook_size}")
        print(f"   åµŒå…¥ç»´åº¦: {self.args.n_embd}")
        print(f"   å±‚æ•°: {self.args.n_layer}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.args.n_head}")
        print(f"   åºåˆ—é•¿åº¦: {model.max_seq_len}")
        print(f"   ç”¨æˆ·æ•°é‡: {self.args.num_users}")

        # æµ‹è¯•å¢å¼ºåŠŸèƒ½æ˜¯å¦å·¥ä½œ
        self._test_enhanced_features(model)

    def _test_enhanced_features(self, model):
        """æµ‹è¯•å¢å¼ºåŠŸèƒ½æ˜¯å¦æ­£ç¡®å·¥ä½œ"""
        print(f"ğŸ§ª æµ‹è¯•å¢å¼ºåŠŸèƒ½:")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_user_ids = torch.tensor([1, 2], device=self.device)
        test_tokens = torch.randint(0, 1024, (2, 1024), device=self.device)

        # æµ‹è¯•ç”¨æˆ·ç¼–ç å™¨
        with torch.no_grad():
            user_embeds = model.user_encoder(test_user_ids)
            print(f"   ç”¨æˆ·åµŒå…¥å½¢çŠ¶: {user_embeds.shape} (åº”è¯¥æ˜¯[2, 512])")

            # æµ‹è¯•prepare_inputs
            input_ids, labels, encoder_hidden_states, encoder_attention_mask = model.prepare_inputs(
                test_user_ids, test_tokens
            )
            print(f"   è¾“å…¥åºåˆ—å½¢çŠ¶: {input_ids.shape}")
            print(f"   æ ‡ç­¾å½¢çŠ¶: {labels.shape}")

            if encoder_hidden_states is not None:
                print(f"   äº¤å‰æ³¨æ„åŠ›çŠ¶æ€å½¢çŠ¶: {encoder_hidden_states.shape} (åº”è¯¥æ˜¯[2, 4, 512])")
                print(f"   æ³¨æ„åŠ›æ©ç å½¢çŠ¶: {encoder_attention_mask.shape}")
            else:
                print(f"   äº¤å‰æ³¨æ„åŠ›: æœªä½¿ç”¨")

        print(f"âœ… å¢å¼ºåŠŸèƒ½æµ‹è¯•å®Œæˆ")

        return model
        
    def train(self):
        """è®­ç»ƒTransformer"""
        print(f"\nğŸš€ å¼€å§‹Transformerè®­ç»ƒ")
        print(f"   è®­ç»ƒè½®æ•°: {self.args.num_epochs}")
        print(f"   å­¦ä¹ ç‡: {self.args.learning_rate}")
        
        # åˆ›å»ºå›¾åƒå˜æ¢ - è½¬æ¢ä¸ºå¼ é‡
        from torchvision import transforms
        transform = transforms.Compose([
            transforms.Resize((self.args.resolution, self.args.resolution)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # å½’ä¸€åŒ–åˆ°[-1, 1]
        ])

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = MicroDopplerDataset(
            data_dir=self.args.data_dir,
            transform=transform,  # éœ€è¦å˜æ¢å°†PILå›¾åƒè½¬ä¸ºå¼ é‡
            return_user_id=True,  # éœ€è¦ç”¨æˆ·IDè¿›è¡Œæ¡ä»¶ç”Ÿæˆ
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=self.args.num_workers,
            pin_memory=True
        )
        
        best_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            self.transformer_model.train()
            total_loss = 0
            
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{self.args.num_epochs}")
            
            for batch_idx, (images, user_ids) in enumerate(pbar):
                images = images.to(self.device)
                user_ids = user_ids.to(self.device)

                # ç”¨æˆ·IDèŒƒå›´[1,31]ç›´æ¥ä½¿ç”¨ï¼ŒåµŒå…¥å±‚å·²è°ƒæ•´ä¸ºæ”¯æŒè¿™ä¸ªèŒƒå›´
                
                # ä½¿ç”¨VQ-VAEç¼–ç å›¾åƒä¸ºtokenåºåˆ—
                with torch.no_grad():
                    encoded = self.vqvae_model.encode(images, return_dict=True)
                    tokens = encoded['encoding_indices']  # [B, H, W] - VQ-VAEè¾“å‡ºçš„2D token map

                    # æ£€æŸ¥tokenå€¼èŒƒå›´
                    min_token = tokens.min().item()
                    max_token = tokens.max().item()
                    if min_token < 0 or max_token >= self.args.codebook_size:
                        print(f"âŒ Tokenå€¼è¶…å‡ºèŒƒå›´: [{min_token}, {max_token}], è·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue

                    # å±•å¹³ä¸ºåºåˆ— [B, H*W] - å¯¹äº128x128å›¾åƒï¼Œ8å€ä¸‹é‡‡æ ·åæ˜¯16x16=256
                    batch_size = tokens.shape[0]
                    tokens = tokens.view(batch_size, -1)  # [B, 256]
                
                # Transformerè®­ç»ƒ
                self.optimizer.zero_grad()
                
                # å‡†å¤‡è¾“å…¥å’Œç›®æ ‡ - ç¡®ä¿é•¿åº¦åŒ¹é…
                # MicroDopplerTransformerä¼šåœ¨å†…éƒ¨æ·»åŠ ç”¨æˆ·tokenå¹¶å¤„ç†åºåˆ—
                # æˆ‘ä»¬ç›´æ¥ä¼ é€’å®Œæ•´çš„tokenåºåˆ—
                input_tokens = tokens  # å®Œæ•´çš„tokenåºåˆ— [B, 1024]
                
                # å‰å‘ä¼ æ’­
                outputs = self.transformer_model(
                    user_ids=user_ids,
                    token_sequences=input_tokens
                )
                
                # ä½¿ç”¨Transformerå†…éƒ¨è®¡ç®—çš„æŸå¤±
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.transformer_model.parameters(), 1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
                
                # å®šæœŸä¿å­˜
                if batch_idx % 500 == 0:
                    self._save_checkpoint(epoch, batch_idx, loss.item())
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self._save_model("best_model.pth", epoch, avg_loss)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_model("final_model.pth", self.args.num_epochs-1, avg_loss)
        print(f"âœ… Transformerè®­ç»ƒå®Œæˆ")
        
    def _save_checkpoint(self, epoch, batch_idx, loss):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'batch_idx': batch_idx,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'args': self.args,
        }
        
        checkpoint_path = self.output_dir / f"checkpoint_epoch_{epoch}_batch_{batch_idx}.pth"
        torch.save(checkpoint, checkpoint_path)
        
    def _save_model(self, filename, epoch, loss):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'epoch': epoch,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'args': self.args,
        }
        
        model_path = self.output_dir / filename
        torch.save(model_data, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

def main():
    parser = argparse.ArgumentParser(description="Transformerè®­ç»ƒè„šæœ¬")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--vqvae_path", type=str, required=True, help="VQ-VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--resolution", type=int, default=128, help="å›¾åƒåˆ†è¾¨ç‡")
    parser.add_argument("--codebook_size", type=int, default=1024, help="ç æœ¬å¤§å°")
    parser.add_argument("--num_users", type=int, default=31, help="ç”¨æˆ·æ•°é‡")

    # Transformeræ¶æ„å‚æ•°
    parser.add_argument("--n_embd", type=int, default=512, help="TransformeråµŒå…¥ç»´åº¦")
    parser.add_argument("--n_layer", type=int, default=8, help="Transformerå±‚æ•°")
    parser.add_argument("--n_head", type=int, default=8, help="æ³¨æ„åŠ›å¤´æ•°")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--device", type=str, default="cuda", help="è®­ç»ƒè®¾å¤‡")

    # ä¿å­˜å’Œé‡‡æ ·å‚æ•°
    parser.add_argument("--save_interval", type=int, default=10, help="æ¨¡å‹ä¿å­˜é—´éš”")
    parser.add_argument("--sample_interval", type=int, default=10, help="æ ·æœ¬ç”Ÿæˆé—´éš”")
    parser.add_argument("--generation_temperature", type=float, default=1.0, help="ç”Ÿæˆæ¸©åº¦")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = TransformerTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
