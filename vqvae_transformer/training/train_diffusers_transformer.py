#!/usr/bin/env python3
"""
ä½¿ç”¨diffusersæ ‡å‡†ç»„ä»¶è®­ç»ƒTransformer
åŸºäºæˆç†Ÿçš„ã€ç»è¿‡éªŒè¯çš„diffuserså®ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import argparse
from tqdm import tqdm
import json
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.diffusers_transformer import DiffusersTransformerModel
from models.vqvae_model import MicroDopplerVQVAE
from utils.data_loader import create_micro_doppler_dataset

class DiffusersTransformerTrainer:
    """ä½¿ç”¨diffusersæ ‡å‡†ç»„ä»¶çš„Transformerè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ åˆå§‹åŒ–Diffusers Transformerè®­ç»ƒå™¨")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   VQ-VAEè·¯å¾„: {args.vqvae_path}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åŠ è½½VQ-VAEæ¨¡å‹
        self.vqvae_model = self._load_vqvae_model()
        
        # ğŸ”’ å†»ç»“VQ-VAEæ¨¡å‹
        print("ğŸ”’ å†»ç»“VQ-VAEæ¨¡å‹...")
        self.vqvae_model.eval()
        for param in self.vqvae_model.parameters():
            param.requires_grad = False
        print("   âœ… VQ-VAEå·²å†»ç»“")
        
        # åˆ›å»ºTransformeræ¨¡å‹
        self.transformer_model = self._create_transformer_model()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.transformer_model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            betas=(0.9, 0.95)
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=args.num_epochs
        )
        
        print(f"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_vqvae_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹"""
        vqvae_path = Path(self.args.vqvae_path)
        
        print(f"ğŸ“¦ åŠ è½½VQ-VAEæ¨¡å‹: {vqvae_path}")
        
        # å°è¯•diffusersæ ¼å¼
        if (vqvae_path / "config.json").exists():
            try:
                vqvae_model = MicroDopplerVQVAE.from_pretrained(vqvae_path)
                vqvae_model.to(self.device)
                print("   âœ… æˆåŠŸåŠ è½½diffusersæ ¼å¼VQ-VAE")
                return vqvae_model
            except Exception as e:
                print(f"   âŒ diffusersæ ¼å¼åŠ è½½å¤±è´¥: {e}")
        
        # å°è¯•checkpointæ ¼å¼
        checkpoint_files = list(vqvae_path.glob("*.pth"))
        if checkpoint_files:
            checkpoint_path = checkpoint_files[0]
            print(f"   ğŸ“‚ åŠ è½½checkpoint: {checkpoint_path}")
            
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # é‡å»ºæ¨¡å‹
            vqvae_model = MicroDopplerVQVAE(
                num_vq_embeddings=getattr(checkpoint.get('args', {}), 'codebook_size', 1024),
                commitment_cost=getattr(checkpoint.get('args', {}), 'commitment_cost', 0.25),
                ema_decay=getattr(checkpoint.get('args', {}), 'ema_decay', 0.99),
            )
            vqvae_model.load_state_dict(checkpoint['model_state_dict'])
            vqvae_model.to(self.device)
            print("   âœ… æˆåŠŸåŠ è½½checkpointæ ¼å¼VQ-VAE")
            return vqvae_model
        
        raise FileNotFoundError(f"æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹: {vqvae_path}")
    
    def _create_transformer_model(self):
        """åˆ›å»ºdiffusersæ ‡å‡†Transformeræ¨¡å‹"""
        print(f"ğŸ—ï¸ åˆ›å»ºDiffusers Transformeræ¨¡å‹")
        
        model = DiffusersTransformerModel(
            vocab_size=self.args.vocab_size,
            max_seq_len=self.args.max_seq_len,
            num_users=self.args.num_users,
            num_layers=self.args.num_layers,
            num_attention_heads=self.args.num_attention_heads,
            attention_head_dim=self.args.attention_head_dim,
            dropout=self.args.dropout,
            activation_fn="gelu",
        )
        
        model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"   ğŸ“Š æ¨¡å‹å‚æ•°:")
        print(f"      æ€»å‚æ•°: {total_params:,}")
        print(f"      å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"      è¯æ±‡è¡¨å¤§å°: {self.args.vocab_size}")
        print(f"      æœ€å¤§åºåˆ—é•¿åº¦: {self.args.max_seq_len}")
        print(f"      ç”¨æˆ·æ•°é‡: {self.args.num_users}")
        
        return model
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = create_micro_doppler_dataset(
            data_dir=self.args.data_dir,
            return_user_id=True
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=self.args.num_workers,
            pin_memory=True
        )
        
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
        print(f"   æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
        
        best_loss = float('inf')
        
        for epoch in range(self.args.num_epochs):
            print(f"\nğŸ¯ Epoch {epoch+1}/{self.args.num_epochs}")
            
            # ç¡®ä¿VQ-VAEä¿æŒå†»ç»“çŠ¶æ€
            self.vqvae_model.eval()
            
            # è®­ç»ƒæ¨¡å¼
            self.transformer_model.train()
            
            total_loss = 0
            num_batches = 0
            
            pbar = tqdm(dataloader, desc=f"Training")
            
            for batch_idx, batch in enumerate(pbar):
                # å¤„ç†batchæ ¼å¼
                if isinstance(batch, dict):
                    images = batch['image'].to(self.device)
                    user_ids = batch['user_id'].to(self.device)
                elif isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                    images = images.to(self.device)
                    user_ids = user_ids.to(self.device)
                else:
                    continue
                
                # ä½¿ç”¨VQ-VAEç¼–ç å›¾åƒä¸ºtokenåºåˆ—
                with torch.no_grad():
                    encoded = self.vqvae_model.encode(images, return_dict=True)
                    if isinstance(encoded, dict):
                        tokens = encoded['encoding_indices']
                    else:
                        tokens = encoded.encoding_indices
                    
                    # é‡å¡‘tokenä¸ºåºåˆ—æ ¼å¼
                    batch_size = tokens.shape[0]
                    tokens = tokens.view(batch_size, -1)  # [B, H*W]
                
                # å‡†å¤‡è¾“å…¥å’Œæ ‡ç­¾
                input_ids = tokens
                labels = tokens.clone()
                
                # å‰å‘ä¼ æ’­
                outputs = self.transformer_model(
                    input_ids=input_ids,
                    user_ids=user_ids,
                    labels=labels,
                    return_dict=True,
                )
                
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.transformer_model.parameters(), 
                    max_norm=1.0
                )
                
                self.optimizer.step()
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{total_loss/num_batches:.4f}'
                })
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / num_batches
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.scheduler.get_last_lr()[0]
            
            print(f"   ğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"      å¹³å‡æŸå¤±: {avg_loss:.4f}")
            print(f"      å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self._save_model(epoch, avg_loss, is_best=True)
                print(f"   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {avg_loss:.4f})")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.args.save_every == 0:
                self._save_model(epoch, avg_loss, is_best=False)
                print(f"   ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹")
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.4f}")
    
    def _save_model(self, epoch, loss, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'epoch': epoch,
            'model_state_dict': self.transformer_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'args': self.args,
        }
        
        if is_best:
            save_path = self.output_dir / "best_model.pth"
        else:
            save_path = self.output_dir / f"checkpoint_epoch_{epoch+1}.pth"
        
        torch.save(save_dict, save_path)

def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒDiffusers Transformer")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®ç›®å½•")
    parser.add_argument("--vqvae_path", type=str, required=True, help="VQ-VAEæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./diffusers_transformer_output", help="è¾“å‡ºç›®å½•")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--vocab_size", type=int, default=1024, help="è¯æ±‡è¡¨å¤§å°")
    parser.add_argument("--max_seq_len", type=int, default=1024, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--num_users", type=int, default=31, help="ç”¨æˆ·æ•°é‡")
    parser.add_argument("--num_layers", type=int, default=8, help="Transformerå±‚æ•°")
    parser.add_argument("--num_attention_heads", type=int, default=8, help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--attention_head_dim", type=int, default=64, help="æ³¨æ„åŠ›å¤´ç»´åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropoutç‡")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--save_every", type=int, default=10, help="ä¿å­˜æ£€æŸ¥ç‚¹é—´éš”")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = DiffusersTransformerTrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
