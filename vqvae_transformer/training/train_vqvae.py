#!/usr/bin/env python3
"""
VQ-VAEè®­ç»ƒè„šæœ¬
ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒVQ-VAEå­¦ä¹ å›¾åƒçš„ç¦»æ•£è¡¨ç¤º
"""

import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from pathlib import Path
from tqdm import tqdm
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from models.vqvae_model import MicroDopplerVQVAE
from utils.data_loader import MicroDopplerDataset
from utils.metrics import calculate_psnr, calculate_ssim

class VQVAETrainer:
    """VQ-VAEè®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ•°æ®å˜æ¢ (256x256 -> 128x128) - ä½¿ç”¨é«˜è´¨é‡ç¼©æ”¾
        interpolation_method = getattr(args, 'interpolation', 'lanczos')

        if interpolation_method == 'antialias':
            # æŠ—é”¯é½¿ç¼©æ”¾ (æ¨èç”¨äºæ·±åº¦å­¦ä¹ )
            resize_transform = transforms.Resize(
                (args.resolution, args.resolution),
                interpolation=transforms.InterpolationMode.BILINEAR,
                antialias=True
            )
        else:
            # ä¼ ç»Ÿæ’å€¼æ–¹æ³•
            interp_map = {
                'lanczos': transforms.InterpolationMode.LANCZOS,
                'bicubic': transforms.InterpolationMode.BICUBIC,
                'bilinear': transforms.InterpolationMode.BILINEAR,
            }
            interp_mode = interp_map.get(interpolation_method, transforms.InterpolationMode.LANCZOS)
            resize_transform = transforms.Resize((args.resolution, args.resolution), interpolation=interp_mode)

        self.transform = transforms.Compose([
            resize_transform,
            transforms.ToTensor(),  # [0, 1]
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]
        ])

        print(f"ğŸ–¼ï¸ å›¾åƒç¼©æ”¾: 256x256 -> {args.resolution}x{args.resolution} ({interpolation_method})")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._create_model()
        self.model.to(self.device)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            betas=(0.9, 0.999),
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=args.num_epochs,
            eta_min=args.learning_rate * 0.01,
        )
        
        # æŸå¤±å‡½æ•°
        self.recon_criterion = nn.MSELoss()
        
        print(f"ğŸš€ VQ-VAEè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   åˆ†è¾¨ç‡: {args.resolution}x{args.resolution}")
    
    def _create_model(self):
        """åˆ›å»ºVQ-VAEæ¨¡å‹"""
        return MicroDopplerVQVAE(
            in_channels=3,
            out_channels=3,
            down_block_types=("DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"),
            up_block_types=("UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"),
            block_out_channels=(128, 256, 512),
            layers_per_block=2,
            act_fn="silu",
            latent_channels=256,
            sample_size=self.args.resolution // 8,  # 8å€ä¸‹é‡‡æ ·
            num_vq_embeddings=self.args.codebook_size,
            norm_num_groups=32,
            vq_embed_dim=256,
            commitment_cost=self.args.commitment_cost,
            ema_decay=self.args.ema_decay,
            restart_threshold=self.args.restart_threshold,
        )
    
    def _create_dataloaders(self):
        """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
        # åˆ›å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = MicroDopplerDataset(
            data_dir=self.args.data_dir,
            transform=self.transform,
            return_user_id=True,  # è¿”å›ç”¨æˆ·IDç”¨äºåç»­Transformerè®­ç»ƒ
        )

        # åˆ†å±‚åˆ’åˆ†æ•°æ®é›† (80% è®­ç»ƒ, 20% éªŒè¯)
        train_indices, val_indices = self._stratified_split(full_dataset, train_ratio=0.8)

        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(full_dataset)}")
        print(f"   è®­ç»ƒé›†: {len(train_indices)} ({len(train_indices)/len(full_dataset)*100:.1f}%)")
        print(f"   éªŒè¯é›†: {len(val_indices)} ({len(val_indices)/len(full_dataset)*100:.1f}%)")

        # åˆ›å»ºå­æ•°æ®é›†
        train_dataset = torch.utils.data.Subset(full_dataset, train_indices)
        val_dataset = torch.utils.data.Subset(full_dataset, val_indices)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=self.args.num_workers,
            pin_memory=True,
            drop_last=True,
        )

        val_dataloader = DataLoader(
            val_dataset,
            batch_size=self.args.batch_size,
            shuffle=False,  # éªŒè¯é›†ä¸éœ€è¦shuffle
            num_workers=self.args.num_workers,
            pin_memory=True,
            drop_last=False,
        )

        print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_dataloader)}")
        print(f"   éªŒè¯æ‰¹æ¬¡æ•°é‡: {len(val_dataloader)}")

        return train_dataloader, val_dataloader

    def _stratified_split(self, dataset, train_ratio=0.8):
        """æŒ‰ç”¨æˆ·åˆ†å±‚åˆ’åˆ†æ•°æ®é›†ï¼Œç¡®ä¿æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬éƒ½æŒ‰æ¯”ä¾‹åˆ†é…"""
        print(f"ğŸ”„ VQ-VAEåˆ†å±‚åˆ’åˆ† (ç¡®ä¿æ¯ä¸ªç”¨æˆ·éƒ½åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­)...")

        # æ”¶é›†æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬ç´¢å¼•
        user_indices = {}
        for idx in range(len(dataset)):
            try:
                _, user_id = dataset[idx]
                user_id = user_id.item() if hasattr(user_id, 'item') else user_id

                if user_id not in user_indices:
                    user_indices[user_id] = []
                user_indices[user_id].append(idx)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ ·æœ¬{idx}æ—¶å‡ºé”™: {e}")
                continue

        print(f"   å‘ç° {len(user_indices)} ä¸ªç”¨æˆ·")

        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ†é…æ ·æœ¬åˆ°è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_indices = []
        val_indices = []

        import random
        random.seed(42)  # å›ºå®šéšæœºç§å­

        for user_id, indices in user_indices.items():
            # éšæœºæ‰“ä¹±è¯¥ç”¨æˆ·çš„æ ·æœ¬
            indices = indices.copy()
            random.shuffle(indices)

            # è®¡ç®—è®­ç»ƒé›†æ ·æœ¬æ•°ï¼ˆè‡³å°‘1ä¸ªï¼‰
            user_train_size = max(1, int(len(indices) * train_ratio))

            # å¦‚æœç”¨æˆ·åªæœ‰1ä¸ªæ ·æœ¬ï¼Œæ”¾åˆ°è®­ç»ƒé›†
            if len(indices) == 1:
                train_indices.extend(indices)
                print(f"   ç”¨æˆ·{user_id}: 1ä¸ªæ ·æœ¬ â†’ è®­ç»ƒé›†")
            else:
                # åˆ†é…æ ·æœ¬
                user_train_indices = indices[:user_train_size]
                user_val_indices = indices[user_train_size:]

                train_indices.extend(user_train_indices)
                val_indices.extend(user_val_indices)

                print(f"   ç”¨æˆ·{user_id}: {len(indices)}ä¸ªæ ·æœ¬ â†’ è®­ç»ƒé›†{len(user_train_indices)}ä¸ª, éªŒè¯é›†{len(user_val_indices)}ä¸ª")

        # éšæœºæ‰“ä¹±æœ€ç»ˆçš„ç´¢å¼•åˆ—è¡¨
        random.shuffle(train_indices)
        random.shuffle(val_indices)

        print(f"âœ… VQ-VAEåˆ†å±‚åˆ’åˆ†å®Œæˆ")
        return train_indices, val_indices
    
    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()

        # é‡ç½®epochçº§åˆ«çš„ç æœ¬ç»Ÿè®¡
        self.model.reset_epoch_stats()

        total_loss = 0
        total_recon_loss = 0
        total_vq_loss = 0

        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{self.args.num_epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            if isinstance(batch, (list, tuple)) and len(batch) == 2:
                images, user_ids = batch
            else:
                images = batch
                user_ids = None
            
            images = images.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            outputs = self.model(images, return_dict=True)
            reconstructed = outputs.sample
            vq_loss = outputs.vq_loss
            
            # è®¡ç®—é‡å»ºæŸå¤±
            recon_loss = self.recon_criterion(reconstructed, images)
            
            # æ€»æŸå¤±
            total_loss_batch = recon_loss + vq_loss
            
            # åå‘ä¼ æ’­
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.args.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += total_loss_batch.item()
            total_recon_loss += recon_loss.item()
            total_vq_loss += vq_loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{total_loss_batch.item():.4f}',
                'Recon': f'{recon_loss.item():.4f}',
                'VQ': f'{vq_loss.item():.4f}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.6f}',
            })
            
            # å®šæœŸä¿å­˜æ ·æœ¬
            if batch_idx % self.args.sample_interval == 0:
                self._save_samples(images, reconstructed, epoch, batch_idx)
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()
        
        # è¿”å›å¹³å‡æŸå¤±
        num_batches = len(dataloader)
        return {
            'total_loss': total_loss / num_batches,
            'recon_loss': total_recon_loss / num_batches,
            'vq_loss': total_vq_loss / num_batches,
        }
    
    def _save_samples(self, original, reconstructed, epoch, batch_idx):
        """ä¿å­˜é‡å»ºæ ·æœ¬"""
        sample_dir = self.output_dir / "samples"
        sample_dir.mkdir(exist_ok=True)
        
        # åå½’ä¸€åŒ–
        def denormalize(tensor):
            return (tensor * 0.5 + 0.5).clamp(0, 1)
        
        original = denormalize(original)
        reconstructed = denormalize(reconstructed)
        
        # ä¿å­˜å‰4ä¸ªæ ·æœ¬
        n_samples = min(4, original.size(0))
        
        fig, axes = plt.subplots(2, n_samples, figsize=(n_samples * 3, 6))
        if n_samples == 1:
            axes = axes.reshape(2, 1)
        
        for i in range(n_samples):
            # åŸå›¾
            axes[0, i].imshow(original[i].cpu().detach().permute(1, 2, 0).numpy())
            axes[0, i].set_title(f'Original {i+1}')
            axes[0, i].axis('off')

            # é‡å»ºå›¾
            axes[1, i].imshow(reconstructed[i].cpu().detach().permute(1, 2, 0).numpy())
            axes[1, i].set_title(f'Reconstructed {i+1}')
            axes[1, i].axis('off')
        
        plt.tight_layout()
        plt.savefig(sample_dir / f"epoch_{epoch:03d}_batch_{batch_idx:04d}.png", dpi=150, bbox_inches='tight')
        plt.close()
    
    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        total_psnr = 0
        total_ssim = 0
        num_samples = 0
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Evaluating"):
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, _ = batch
                else:
                    images = batch
                
                images = images.to(self.device)
                
                outputs = self.model(images, return_dict=True)
                reconstructed = outputs.sample
                
                # åå½’ä¸€åŒ–åˆ°[0,1]
                images_eval = (images * 0.5 + 0.5).clamp(0, 1)
                reconstructed_eval = (reconstructed * 0.5 + 0.5).clamp(0, 1)
                
                # è®¡ç®—æŒ‡æ ‡
                for i in range(images.size(0)):
                    psnr = calculate_psnr(images_eval[i], reconstructed_eval[i])
                    ssim = calculate_ssim(images_eval[i], reconstructed_eval[i])
                    
                    total_psnr += psnr
                    total_ssim += ssim
                    num_samples += 1
        
        avg_psnr = total_psnr / num_samples
        avg_ssim = total_ssim / num_samples
        
        return {'psnr': avg_psnr, 'ssim': avg_ssim}
    
    def save_model(self, epoch, is_best=False, save_checkpoint=True):
        """
        ä¿å­˜æ¨¡å‹ï¼Œæ™ºèƒ½ç®¡ç†å­˜å‚¨ç©ºé—´
        - åªåœ¨è¯„ä¼°æ—¶ä¿å­˜checkpoint
        - ä¿å­˜æœ€ä½³æ¨¡å‹
        - å®šæœŸä¿å­˜é‡Œç¨‹ç¢‘æ¨¡å‹
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'args': self.args,
        }

        # åªåœ¨è¯„ä¼°æ—¶æˆ–é‡Œç¨‹ç¢‘æ—¶ä¿å­˜checkpoint
        is_milestone = (epoch + 1) % self.args.milestone_interval == 0 or epoch == self.args.num_epochs - 1

        if save_checkpoint or is_milestone:
            checkpoint_path = self.output_dir / f"checkpoint_epoch_{epoch:03d}.pth"
            torch.save(checkpoint, checkpoint_path)
            print(f"ğŸ’¾ ä¿å­˜checkpoint: epoch_{epoch:03d}.pth")

            # æ¸…ç†æ—§çš„checkpoint (å¦‚æœå¯ç”¨è‡ªåŠ¨æ¸…ç†)
            if self.args.auto_cleanup:
                self._cleanup_old_checkpoints(epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.output_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path} (PSNRæå‡)")

        # ä¿å­˜é‡Œç¨‹ç¢‘æ¨¡å‹ (å•ç‹¬ä¿å­˜ï¼Œä¸ä¸checkpointé‡å¤)
        if is_milestone and not save_checkpoint:
            milestone_path = self.output_dir / f"milestone_epoch_{epoch:03d}.pth"
            torch.save(checkpoint, milestone_path)
            print(f"ğŸ¯ ä¿å­˜é‡Œç¨‹ç¢‘: milestone_epoch_{epoch:03d}.pth")

        # åªåœ¨æœ€åä¿å­˜final_model
        if epoch == self.args.num_epochs - 1:
            final_path = self.output_dir / "final_model"
            final_path.mkdir(exist_ok=True)
            self.model.save_pretrained(final_path)
            print(f"ğŸ‰ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}")

    def _cleanup_old_checkpoints(self, current_epoch):
        """æ¸…ç†æ—§çš„checkpointæ–‡ä»¶ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´"""
        try:
            # è·å–æ‰€æœ‰checkpointæ–‡ä»¶
            checkpoint_files = list(self.output_dir.glob("checkpoint_epoch_*.pth"))

            if len(checkpoint_files) <= self.args.keep_checkpoints:  # å¦‚æœæ–‡ä»¶æ•°é‡å°‘äºç­‰äºè®¾å®šå€¼ï¼Œä¸æ¸…ç†
                return

            # æŒ‰epochæ’åº
            checkpoint_files.sort(key=lambda x: int(x.stem.split('_')[-1]))

            # ä¿ç•™æœ€è¿‘Nä¸ªcheckpoint
            keep_recent = self.args.keep_checkpoints
            files_to_keep = set(checkpoint_files[-keep_recent:])

            # ä¿ç•™é‡Œç¨‹ç¢‘checkpoint (æ ¹æ®è®¾å®šé—´éš”)
            for f in checkpoint_files:
                epoch_num = int(f.stem.split('_')[-1])
                if epoch_num % self.args.milestone_interval == 0:  # é‡Œç¨‹ç¢‘
                    files_to_keep.add(f)

            # åˆ é™¤ä¸éœ€è¦ä¿ç•™çš„æ–‡ä»¶
            deleted_count = 0
            for f in checkpoint_files:
                if f not in files_to_keep:
                    f.unlink()  # åˆ é™¤æ–‡ä»¶
                    deleted_count += 1

            if deleted_count > 0:
                print(f"ğŸ—‘ï¸ æ¸…ç†äº† {deleted_count} ä¸ªæ—§checkpointï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´")

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†checkpointæ—¶å‡ºé”™: {e}")

    def get_storage_info(self):
        """è·å–å­˜å‚¨ç©ºé—´ä¿¡æ¯"""
        try:
            import shutil
            total, used, free = shutil.disk_usage(self.output_dir)

            # è®¡ç®—å½“å‰è¾“å‡ºç›®å½•å¤§å°
            output_size = sum(f.stat().st_size for f in self.output_dir.rglob('*') if f.is_file())

            print(f"ğŸ’¾ å­˜å‚¨ä¿¡æ¯:")
            print(f"   è¾“å‡ºç›®å½•å¤§å°: {output_size / (1024**3):.2f} GB")
            print(f"   ç£ç›˜å‰©ä½™ç©ºé—´: {free / (1024**3):.2f} GB")
            print(f"   ç£ç›˜ä½¿ç”¨ç‡: {used / total * 100:.1f}%")

            # è­¦å‘Šå­˜å‚¨ç©ºé—´ä¸è¶³
            if free < 5 * (1024**3):  # å°‘äº5GB
                print(f"âš ï¸ è­¦å‘Š: ç£ç›˜å‰©ä½™ç©ºé—´ä¸è¶³5GB!")

        except Exception as e:
            print(f"âš ï¸ è·å–å­˜å‚¨ä¿¡æ¯å¤±è´¥: {e}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸ¯ å¼€å§‹VQ-VAEè®­ç»ƒ...")

        # æ˜¾ç¤ºå­˜å‚¨ä¿¡æ¯
        self.get_storage_info()

        # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        train_dataloader, val_dataloader = self._create_dataloaders()

        best_psnr = 0

        for epoch in range(self.args.num_epochs):
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_dataloader, epoch)

            print(f"\nEpoch {epoch+1}/{self.args.num_epochs}:")
            print(f"  è®­ç»ƒæŸå¤±: {train_metrics['total_loss']:.4f}")
            print(f"  é‡å»ºæŸå¤±: {train_metrics['recon_loss']:.4f}")
            print(f"  VQæŸå¤±: {train_metrics['vq_loss']:.4f}")

            # è¯„ä¼°ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰
            if (epoch + 1) % self.args.eval_interval == 0:
                print(f"  ğŸ“Š éªŒè¯é›†è¯„ä¼°:")
                eval_metrics = self.evaluate(val_dataloader)
                print(f"  éªŒè¯PSNR: {eval_metrics['psnr']:.2f} dB")
                print(f"  éªŒè¯SSIM: {eval_metrics['ssim']:.4f}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                is_best = eval_metrics['psnr'] > best_psnr
                if is_best:
                    best_psnr = eval_metrics['psnr']
                    print(f"  ğŸ† æ–°çš„æœ€ä½³PSNR: {best_psnr:.2f} dB")

                # åœ¨è¯„ä¼°æ—¶ä¿å­˜checkpointå’Œæœ€ä½³æ¨¡å‹
                self.save_model(epoch, is_best, save_checkpoint=True)
            else:
                # éè¯„ä¼°epochï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜é‡Œç¨‹ç¢‘
                is_milestone = (epoch + 1) % self.args.milestone_interval == 0
                if is_milestone:
                    self.save_model(epoch, is_best=False, save_checkpoint=False)
            
            # æ˜¾ç¤ºç æœ¬ä½¿ç”¨æƒ…å†µ
            if (epoch + 1) % self.args.codebook_monitor_interval == 0:
                try:
                    stats = self.model.get_codebook_stats()

                    # ä¸»è¦æ˜¾ç¤ºepochçº§åˆ«ç»Ÿè®¡
                    print(f"  ğŸ“Š Epochç æœ¬ä½¿ç”¨ç‡: {stats['epoch_usage_rate']:.3f} ({stats['epoch_active_codes']}/{stats['total_codes']})")
                    print(f"  ğŸ“ˆ Epochä½¿ç”¨ç†µ: {stats['epoch_entropy']:.3f}")
                    print(f"  ğŸ“Š ç´¯ç§¯ç æœ¬ä½¿ç”¨ç‡: {stats['cumulative_usage_rate']:.3f} ({stats['cumulative_active_codes']}/{stats['total_codes']})")
                    print(f"  ğŸ”¢ æ€»æ›´æ–°æ¬¡æ•°: {stats['total_updates']}")

                    # åç¼©è­¦å‘Š (åŸºäºepochä½¿ç”¨ç‡)
                    epoch_rate = stats['epoch_usage_rate']
                    if epoch_rate < 0.1:
                        print(f"  ğŸš¨ ä¸¥é‡è­¦å‘Š: Epochç æœ¬ä½¿ç”¨ç‡è¿‡ä½ï¼Œå¯èƒ½å‘ç”Ÿåç¼©!")
                    elif epoch_rate < 0.3:
                        print(f"  âš ï¸ æ³¨æ„: Epochç æœ¬ä½¿ç”¨ç‡è¾ƒä½")
                    else:
                        print(f"  âœ… Epochç æœ¬ä½¿ç”¨ç‡æ­£å¸¸")

                except Exception as e:
                    print(f"  âŒ ç æœ¬ç»Ÿè®¡è·å–å¤±è´¥: {e}")
                    # è°ƒè¯•ä¿¡æ¯
                    print(f"  ğŸ” è°ƒè¯•: æ¨¡å‹ç±»å‹ = {type(self.model)}")
                    print(f"  ğŸ” è°ƒè¯•: æ˜¯å¦æœ‰quantizeå±æ€§ = {hasattr(self.model, 'quantize')}")
                    if hasattr(self.model, 'quantize'):
                        print(f"  ğŸ” è°ƒè¯•: quantizeç±»å‹ = {type(self.model.quantize)}")
                        print(f"  ğŸ” è°ƒè¯•: æ˜¯å¦æœ‰usage_count = {hasattr(self.model.quantize, 'usage_count')}")

                # åç¼©è­¦å‘Š
                if stats['usage_rate'] < 0.1:
                    print(f"  âš ï¸ è­¦å‘Š: ç æœ¬ä½¿ç”¨ç‡è¿‡ä½ï¼Œå¯èƒ½å‘ç”Ÿåç¼©!")
                elif stats['usage_rate'] < 0.3:
                    print(f"  âš ï¸ æ³¨æ„: ç æœ¬ä½¿ç”¨ç‡è¾ƒä½")
                else:
                    print(f"  âœ… ç æœ¬ä½¿ç”¨ç‡æ­£å¸¸")

                # ä¿å­˜ç æœ¬ä½¿ç”¨å›¾
                usage_plot_path = self.output_dir / f"codebook_usage_epoch_{epoch+1:03d}.png"
                self.model.plot_codebook_usage(str(usage_plot_path))

            # æŸå¤±è¶‹åŠ¿åˆ†æ
            if hasattr(self, 'loss_history'):
                self.loss_history.append(train_metrics['total_loss'])
                if len(self.loss_history) >= 3:
                    recent_trend = self.loss_history[-3:]
                    if all(recent_trend[i] < recent_trend[i+1] for i in range(len(recent_trend)-1)):
                        print(f"  âš ï¸ è­¦å‘Š: æŸå¤±è¿ç»­ä¸Šå‡ {recent_trend}")
            else:
                self.loss_history = [train_metrics['total_loss']]
        
        print(f"\nâœ… VQ-VAEè®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³PSNR: {best_psnr:.2f} dB")
        print(f"   æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}")

def main():
    parser = argparse.ArgumentParser(description="Train VQ-VAE for Micro-Doppler Images")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®é›†ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="outputs/vqvae", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--resolution", type=int, default=128, help="å›¾åƒåˆ†è¾¨ç‡")
    parser.add_argument("--interpolation", type=str, default="lanczos",
                       choices=["lanczos", "bicubic", "bilinear", "antialias"],
                       help="å›¾åƒç¼©æ”¾æ’å€¼æ–¹æ³•")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--codebook_size", type=int, default=1024, help="ç æœ¬å¤§å°")
    parser.add_argument("--commitment_cost", type=float, default=0.25, help="CommitmentæŸå¤±æƒé‡")
    parser.add_argument("--ema_decay", type=float, default=0.99, help="EMAè¡°å‡ç‡")
    parser.add_argument("--restart_threshold", type=float, default=1.0, help="ç æœ¬é‡ç½®é˜ˆå€¼")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="æƒé‡è¡°å‡")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--sample_interval", type=int, default=500, help="æ ·æœ¬ä¿å­˜é—´éš”")
    parser.add_argument("--eval_interval", type=int, default=5, help="è¯„ä¼°é—´éš”")
    parser.add_argument("--codebook_monitor_interval", type=int, default=1, help="ç æœ¬ç›‘æ§é—´éš”")

    # å­˜å‚¨ç®¡ç†å‚æ•°
    parser.add_argument("--keep_checkpoints", type=int, default=5, help="ä¿ç•™æœ€è¿‘Nä¸ªcheckpoint")
    parser.add_argument("--milestone_interval", type=int, default=10, help="é‡Œç¨‹ç¢‘ä¿å­˜é—´éš”")
    parser.add_argument("--auto_cleanup", action="store_true", help="è‡ªåŠ¨æ¸…ç†æ—§æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = VQVAETrainer(args)
    trainer.train()

if __name__ == "__main__":
    main()
