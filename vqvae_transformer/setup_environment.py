#!/usr/bin/env python3
"""
VQ-VAE + Transformer ç»Ÿä¸€ç¯å¢ƒå®‰è£…è„šæœ¬
è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒç±»å‹å¹¶å®‰è£…å…¼å®¹çš„ä¾èµ–ç‰ˆæœ¬
è§£å†³diffusersã€transformersç­‰APIå…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import subprocess
import importlib

def run_command(cmd, description=""):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†é”™è¯¯"""
    print(f"ğŸ”„ {description}")
    print(f"   å‘½ä»¤: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)
        print(f"âœ… {description} æˆåŠŸ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} å¤±è´¥")
        if e.stderr.strip():
            print(f"   é”™è¯¯: {e.stderr.strip()}")
        return False

def detect_environment():
    """æ£€æµ‹è¿è¡Œç¯å¢ƒ"""
    print("ğŸ” æ£€æµ‹è¿è¡Œç¯å¢ƒ...")
    
    if any([os.path.exists('/kaggle'), 'KAGGLE_KERNEL_RUN_TYPE' in os.environ]):
        print("âœ… æ£€æµ‹åˆ°Kaggleç¯å¢ƒ")
        return "kaggle"
    
    try:
        import google.colab
        print("âœ… æ£€æµ‹åˆ°Google Colabç¯å¢ƒ")
        return "colab"
    except ImportError:
        pass
    
    print("âœ… æ£€æµ‹åˆ°æœ¬åœ°ç¯å¢ƒ")
    return "local"

def complete_uninstall():
    """å®Œå…¨å¸è½½å¯èƒ½å†²çªçš„åŒ…"""
    print("\nğŸ—‘ï¸ å¸è½½å¯èƒ½å†²çªçš„åŒ…...")
    
    packages_to_remove = [
        "diffusers", "transformers", "accelerate", 
        "huggingface-hub", "tokenizers", "safetensors",
        "datasets", "evaluate", "peft", "trl",
        "torch-audio", "torchaudio", "torchtext", "torchdata",
        "sentencepiece", "protobuf", "wandb", "tensorboardX",
    ]
    
    for round_num in range(2):
        print(f"ç¬¬ {round_num + 1} è½®å¸è½½:")
        for package in packages_to_remove:
            run_command(f"pip uninstall {package} -y", f"å¸è½½ {package}")
    
    run_command("pip cache purge", "æ¸…ç†pipç¼“å­˜")

def install_pytorch(env_type):
    """å®‰è£…PyTorch"""
    print("\nğŸ”¥ å®‰è£…PyTorch...")
    
    if env_type == "kaggle":
        try:
            import torch
            print(f"âœ… ä½¿ç”¨Kaggleé¢„è£…PyTorch: {torch.__version__}")
            return True
        except ImportError:
            pass
    
    # å®‰è£…GPUç‰ˆæœ¬
    cmd = "pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118"
    return run_command(cmd, "å®‰è£…PyTorch")

def install_base_dependencies():
    """å®‰è£…åŸºç¡€ä¾èµ–"""
    print("\nğŸ“¦ å®‰è£…åŸºç¡€ä¾èµ–...")
    
    base_deps = [
        "numpy==1.24.3", "pillow==10.0.1", "requests==2.31.0",
        "packaging==23.2", "filelock==3.13.1", "tqdm==4.66.1",
        "pyyaml==6.0.1", "typing-extensions==4.8.0", "regex==2023.10.3",
    ]
    
    for dep in base_deps:
        run_command(f"pip install {dep}", f"å®‰è£… {dep}")
    
    return True

def install_huggingface_ecosystem():
    """å®‰è£…HuggingFaceç”Ÿæ€ç³»ç»Ÿ (å…¼å®¹ç‰ˆæœ¬)"""
    print("\nğŸ¤— å®‰è£…HuggingFaceç”Ÿæ€ç³»ç»Ÿ...")
    
    hf_packages = [
        ("huggingface-hub==0.17.3", "HuggingFace Hub (æ”¯æŒcached_download)"),
        ("tokenizers==0.14.1", "Tokenizers"),
        ("safetensors==0.4.0", "SafeTensors"),
        ("transformers==4.35.2", "Transformers"),
        ("accelerate==0.24.1", "Accelerate"),
        ("diffusers==0.24.0", "Diffusers"),
    ]
    
    for package, description in hf_packages:
        success = run_command(f"pip install {package} --no-deps", f"å®‰è£… {description}")
        if not success:
            run_command(f"pip install {package} --force-reinstall --no-deps", f"å¼ºåˆ¶é‡è£… {description}")
    
    return True

def install_other_dependencies():
    """å®‰è£…å…¶ä»–å¿…è¦ä¾èµ–"""
    print("\nğŸ“š å®‰è£…å…¶ä»–ä¾èµ–...")
    
    other_deps = [
        "scipy==1.11.4", "scikit-learn==1.3.0", "scikit-image==0.21.0",
        "matplotlib==3.7.2", "opencv-python==4.8.1.78", "einops==0.7.0",
        "tensorboard==2.15.1", "lpips==0.1.4",
    ]
    
    for dep in other_deps:
        run_command(f"pip install {dep}", f"å®‰è£… {dep}")
    
    return True

def verify_installation():
    """éªŒè¯å®‰è£…"""
    print("\nğŸ” éªŒè¯å®‰è£…...")
    
    critical_packages = {
        'torch': None,
        'diffusers': '0.24.0',
        'transformers': '4.35.2', 
        'accelerate': '0.24.1',
        'huggingface_hub': '0.17.3',
    }
    
    all_good = True
    
    for package, expected_version in critical_packages.items():
        try:
            module = importlib.import_module(package)
            actual_version = getattr(module, '__version__', 'unknown')
            
            if expected_version is None or expected_version in actual_version:
                print(f"âœ… {package}: {actual_version}")
            else:
                print(f"âš ï¸ {package}: æœŸæœ› {expected_version}, å®é™… {actual_version}")
                
        except ImportError as e:
            print(f"âŒ {package}: å¯¼å…¥å¤±è´¥ - {e}")
            all_good = False
    
    return all_good

def test_api_compatibility():
    """æµ‹è¯•APIå…¼å®¹æ€§"""
    print("\nğŸ§ª æµ‹è¯•APIå…¼å®¹æ€§...")
    
    # æµ‹è¯•cached_download
    try:
        from huggingface_hub import cached_download
        print("âœ… cached_download: å¯ç”¨")
    except ImportError as e:
        print(f"âŒ cached_download: ä¸å¯ç”¨ - {e}")
        return False
    
    # æµ‹è¯•VQModel
    try:
        from diffusers.models.autoencoders.vq_model import VQModel
        print("âœ… VQModel: å¯ç”¨")
        
        import torch
        model = VQModel(
            in_channels=3, out_channels=3,
            down_block_types=["DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D"],
            block_out_channels=[128],
            layers_per_block=1,
            latent_channels=128,
            sample_size=32,
            num_vq_embeddings=256,
            norm_num_groups=32,
            vq_embed_dim=128,
        )
        
        test_input = torch.randn(1, 3, 32, 32)
        with torch.no_grad():
            result = model.encode(test_input)
            print("âœ… VQModelæµ‹è¯•: é€šè¿‡")
            
    except Exception as e:
        print(f"âŒ VQModelæµ‹è¯•: å¤±è´¥ - {e}")
        return False
    
    # æµ‹è¯•Transformer
    try:
        from transformers import GPT2Config, GPT2LMHeadModel
        
        config = GPT2Config(vocab_size=256, n_positions=64, n_embd=128, n_layer=2, n_head=4)
        model = GPT2LMHeadModel(config)
        
        import torch
        test_input = torch.randint(0, 256, (1, 16))
        with torch.no_grad():
            output = model(test_input)
            print("âœ… Transformeræµ‹è¯•: é€šè¿‡")
            
    except Exception as e:
        print(f"âŒ Transformeræµ‹è¯•: å¤±è´¥ - {e}")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ VQ-VAE + Transformer ç»Ÿä¸€ç¯å¢ƒå®‰è£…å™¨")
    print("=" * 60)
    print("ğŸ”§ è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒå¹¶å®‰è£…å…¼å®¹ç‰ˆæœ¬")
    print("âš ï¸ è¿™å°†å¸è½½å¹¶é‡æ–°å®‰è£…ç›¸å…³åŒ…ï¼Œç¡®ä¿ç‰ˆæœ¬å…¼å®¹æ€§")
    
    # æ£€æµ‹ç¯å¢ƒ
    env_type = detect_environment()
    
    print(f"\nğŸ“Š ç¯å¢ƒç±»å‹: {env_type}")
    
    # ç¡®è®¤æ“ä½œ
    if env_type == "local":
        response = input("\næ˜¯å¦ç»§ç»­å®‰è£…? (y/N): ").strip().lower()
        if response != 'y':
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return
    
    # å®‰è£…æµç¨‹
    steps = [
        ("å¸è½½å†²çªåŒ…", complete_uninstall),
        ("å®‰è£…PyTorch", lambda: install_pytorch(env_type)),
        ("å®‰è£…åŸºç¡€ä¾èµ–", install_base_dependencies),
        ("å®‰è£…HuggingFaceç”Ÿæ€", install_huggingface_ecosystem),
        ("å®‰è£…å…¶ä»–ä¾èµ–", install_other_dependencies),
        ("éªŒè¯å®‰è£…", verify_installation),
        ("æµ‹è¯•APIå…¼å®¹æ€§", test_api_compatibility),
    ]
    
    for step_name, step_func in steps:
        print(f"\n{'='*20} {step_name} {'='*20}")
        if not step_func():
            print(f"âŒ {step_name} å¤±è´¥")
            if step_name in ["éªŒè¯å®‰è£…", "æµ‹è¯•APIå…¼å®¹æ€§"]:
                print("âš ï¸ å¯èƒ½ä»ç„¶å¯ç”¨ï¼Œç»§ç»­åç»­æ­¥éª¤")
            else:
                print("âŒ å®‰è£…è¿‡ç¨‹ä¸­æ–­")
                return
    
    print("\nğŸ‰ ç¯å¢ƒå®‰è£…å®Œæˆ!")
    print("âœ… æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…å¹¶éªŒè¯")
    print("âœ… APIå…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
    print("\nğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒ:")
    print("   python train_main.py --data_dir /path/to/data")

if __name__ == "__main__":
    main()
