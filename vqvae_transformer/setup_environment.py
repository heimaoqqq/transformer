#!/usr/bin/env python3
"""
VQ-VAE + Transformer ç¯å¢ƒå®‰è£…å’ŒéªŒè¯è„šæœ¬
è§£å†³APIå…¼å®¹æ€§é—®é¢˜ï¼Œç¡®ä¿æ‰€æœ‰ä¾èµ–ç‰ˆæœ¬æ­£ç¡®
"""

import os
import sys
import subprocess
import importlib
import pkg_resources
from pathlib import Path

def run_command(cmd, description=""):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†é”™è¯¯"""
    print(f"ğŸ”„ {description}")
    print(f"   å‘½ä»¤: {cmd}")
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, check=True)
        print(f"âœ… {description} æˆåŠŸ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} å¤±è´¥")
        print(f"   é”™è¯¯: {e.stderr}")
        return False

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print("ğŸ æ£€æŸ¥Pythonç‰ˆæœ¬...")
    
    version = sys.version_info
    if version.major == 3 and version.minor >= 8:
        print(f"âœ… Pythonç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
        return True
    else:
        print(f"âŒ Pythonç‰ˆæœ¬è¿‡ä½: {version.major}.{version.minor}.{version.micro}")
        print("   éœ€è¦Python 3.8+")
        return False

def uninstall_conflicting_packages():
    """å¸è½½å¯èƒ½å†²çªçš„åŒ…"""
    print("\nğŸ—‘ï¸ å¸è½½å¯èƒ½å†²çªçš„åŒ…...")
    
    # éœ€è¦å¸è½½çš„åŒ…åˆ—è¡¨
    packages_to_uninstall = [
        "torch", "torchvision", "torchaudio",
        "diffusers", "transformers", "accelerate",
        "huggingface-hub", "tokenizers", "safetensors",
        "numpy", "pillow", "opencv-python",
        "matplotlib", "scikit-image", "scikit-learn",
        "scipy", "einops", "tqdm", "tensorboard",
        "lpips", "packaging"
    ]
    
    for package in packages_to_uninstall:
        cmd = f"pip uninstall {package} -y"
        run_command(cmd, f"å¸è½½ {package}")
    
    print("âœ… å†²çªåŒ…å¸è½½å®Œæˆ")

def install_pytorch():
    """å®‰è£…PyTorch (CUDA 11.8ç‰ˆæœ¬)"""
    print("\nğŸ”¥ å®‰è£…PyTorch...")
    
    # æ£€æµ‹CUDA
    cuda_available = False
    try:
        result = subprocess.run("nvidia-smi", shell=True, capture_output=True)
        cuda_available = result.returncode == 0
    except:
        pass
    
    if cuda_available:
        print("âœ… æ£€æµ‹åˆ°CUDAï¼Œå®‰è£…GPUç‰ˆæœ¬")
        cmd = "pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118"
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼Œå®‰è£…CPUç‰ˆæœ¬")
        cmd = "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0"
    
    return run_command(cmd, "å®‰è£…PyTorch")

def install_huggingface():
    """å®‰è£…HuggingFaceç”Ÿæ€ç³»ç»Ÿ"""
    print("\nğŸ¤— å®‰è£…HuggingFaceç”Ÿæ€ç³»ç»Ÿ...")
    
    # æŒ‰é¡ºåºå®‰è£…ï¼Œé¿å…ä¾èµ–å†²çª
    hf_packages = [
        "huggingface-hub==0.19.4",
        "tokenizers==0.15.0", 
        "safetensors==0.4.1",
        "transformers==4.36.2",
        "accelerate==0.25.0",
        "diffusers==0.25.1",
    ]
    
    for package in hf_packages:
        if not run_command(f"pip install {package}", f"å®‰è£… {package}"):
            return False
    
    return True

def install_other_dependencies():
    """å®‰è£…å…¶ä»–ä¾èµ–"""
    print("\nğŸ“¦ å®‰è£…å…¶ä»–ä¾èµ–...")
    
    other_packages = [
        "numpy==1.24.3",
        "pillow==10.0.1", 
        "opencv-python==4.8.1.78",
        "matplotlib==3.7.2",
        "scikit-image==0.21.0",
        "scikit-learn==1.3.0",
        "scipy==1.11.4",
        "einops==0.7.0",
        "tqdm==4.66.1",
        "tensorboard==2.15.1",
        "packaging==23.2",
        "lpips==0.1.4",
    ]
    
    for package in other_packages:
        if not run_command(f"pip install {package}", f"å®‰è£… {package}"):
            return False
    
    return True

def verify_installation():
    """éªŒè¯å®‰è£…"""
    print("\nğŸ” éªŒè¯å®‰è£…...")
    
    # éªŒè¯å…³é”®åŒ…
    critical_packages = {
        'torch': '2.1.0',
        'diffusers': '0.25.1', 
        'transformers': '4.36.2',
        'accelerate': '0.25.0',
    }
    
    all_good = True
    
    for package, expected_version in critical_packages.items():
        try:
            module = importlib.import_module(package)
            actual_version = getattr(module, '__version__', 'unknown')
            
            if expected_version in actual_version:
                print(f"âœ… {package}: {actual_version}")
            else:
                print(f"âŒ {package}: æœŸæœ› {expected_version}, å®é™… {actual_version}")
                all_good = False
                
        except ImportError as e:
            print(f"âŒ {package}: å¯¼å…¥å¤±è´¥ - {e}")
            all_good = False
    
    return all_good

def test_vq_vae_api():
    """æµ‹è¯•VQ-VAE APIå…¼å®¹æ€§"""
    print("\nğŸ§ª æµ‹è¯•VQ-VAE APIå…¼å®¹æ€§...")
    
    try:
        # æµ‹è¯•diffusers VQModel
        from diffusers.models.autoencoders.vq_model import VQModel
        print("âœ… VQModelå¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åˆ›å»ºæ¨¡å‹
        model = VQModel(
            in_channels=3,
            out_channels=3,
            down_block_types=["DownEncoderBlock2D", "DownEncoderBlock2D"],
            up_block_types=["UpDecoderBlock2D", "UpDecoderBlock2D"],
            block_out_channels=[128, 256],
            layers_per_block=2,
            act_fn="silu",
            latent_channels=256,
            sample_size=64,
            num_vq_embeddings=512,
            norm_num_groups=32,
            vq_embed_dim=256,
        )
        print("âœ… VQModelåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        import torch
        test_input = torch.randn(1, 3, 64, 64)
        with torch.no_grad():
            result = model.encode(test_input)
            print(f"âœ… VQModelç¼–ç æˆåŠŸ: {result.latents.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ VQ-VAE APIæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_transformer_api():
    """æµ‹è¯•Transformer APIå…¼å®¹æ€§"""
    print("\nğŸ¤– æµ‹è¯•Transformer APIå…¼å®¹æ€§...")
    
    try:
        # æµ‹è¯•transformers GPT2
        from transformers import GPT2Config, GPT2LMHeadModel
        print("âœ… GPT2å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åˆ›å»ºæ¨¡å‹
        config = GPT2Config(
            vocab_size=1024,
            n_positions=256,
            n_embd=512,
            n_layer=4,
            n_head=8,
        )
        
        model = GPT2LMHeadModel(config)
        print("âœ… GPT2æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        import torch
        test_input = torch.randint(0, 1024, (1, 32))
        with torch.no_grad():
            output = model(test_input)
            print(f"âœ… GPT2å‰å‘ä¼ æ’­æˆåŠŸ: {output.logits.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Transformer APIæµ‹è¯•å¤±è´¥: {e}")
        return False

def create_environment_info():
    """åˆ›å»ºç¯å¢ƒä¿¡æ¯æ–‡ä»¶"""
    print("\nğŸ“„ åˆ›å»ºç¯å¢ƒä¿¡æ¯æ–‡ä»¶...")
    
    info_content = f"""# VQ-VAE + Transformer ç¯å¢ƒä¿¡æ¯
# ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now()}

## Pythonç‰ˆæœ¬
{sys.version}

## å·²å®‰è£…åŒ…ç‰ˆæœ¬
"""
    
    try:
        installed_packages = [str(d) for d in pkg_resources.working_set]
        installed_packages.sort()
        
        for package in installed_packages:
            info_content += f"{package}\n"
        
        with open("environment_info.txt", "w", encoding="utf-8") as f:
            f.write(info_content)
        
        print("âœ… ç¯å¢ƒä¿¡æ¯ä¿å­˜åˆ° environment_info.txt")
        return True
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºç¯å¢ƒä¿¡æ¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ VQ-VAE + Transformer ç¯å¢ƒå®‰è£…å™¨")
    print("=" * 60)
    print("âš ï¸ è¿™å°†å¸è½½å¹¶é‡æ–°å®‰è£…æ‰€æœ‰ç›¸å…³åŒ…ï¼Œç¡®ä¿ç‰ˆæœ¬å…¼å®¹æ€§")
    
    # ç¡®è®¤æ“ä½œ
    response = input("\næ˜¯å¦ç»§ç»­? (y/N): ").strip().lower()
    if response != 'y':
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if not check_python_version():
        return
    
    # æ­¥éª¤1: å¸è½½å†²çªåŒ…
    uninstall_conflicting_packages()
    
    # æ­¥éª¤2: å®‰è£…PyTorch
    if not install_pytorch():
        print("âŒ PyTorchå®‰è£…å¤±è´¥ï¼Œåœæ­¢å®‰è£…")
        return
    
    # æ­¥éª¤3: å®‰è£…HuggingFace
    if not install_huggingface():
        print("âŒ HuggingFaceå®‰è£…å¤±è´¥ï¼Œåœæ­¢å®‰è£…")
        return
    
    # æ­¥éª¤4: å®‰è£…å…¶ä»–ä¾èµ–
    if not install_other_dependencies():
        print("âŒ å…¶ä»–ä¾èµ–å®‰è£…å¤±è´¥")
        return
    
    # æ­¥éª¤5: éªŒè¯å®‰è£…
    if not verify_installation():
        print("âŒ å®‰è£…éªŒè¯å¤±è´¥")
        return
    
    # æ­¥éª¤6: æµ‹è¯•API
    vq_api_ok = test_vq_vae_api()
    transformer_api_ok = test_transformer_api()
    
    if not (vq_api_ok and transformer_api_ok):
        print("âŒ APIæµ‹è¯•å¤±è´¥")
        return
    
    # æ­¥éª¤7: åˆ›å»ºç¯å¢ƒä¿¡æ¯
    create_environment_info()
    
    print("\nğŸ‰ ç¯å¢ƒå®‰è£…å®Œæˆ!")
    print("âœ… æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…å¹¶éªŒè¯")
    print("âœ… APIå…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
    print("\nğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒ:")
    print("   python train_main.py --data_dir /path/to/data")

if __name__ == "__main__":
    main()
