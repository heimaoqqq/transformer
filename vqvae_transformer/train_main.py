#!/usr/bin/env python3
"""
VQ-VAE + Transformer ä¸»è®­ç»ƒè„šæœ¬
ä¸¤é˜¶æ®µè®­ç»ƒï¼š
1. è®­ç»ƒVQ-VAEå­¦ä¹ å›¾åƒçš„ç¦»æ•£è¡¨ç¤º
2. è®­ç»ƒTransformerä»ç”¨æˆ·IDç”Ÿæˆtokenåºåˆ—
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path
import torch

def setup_environment():
    """è®¾ç½®è®­ç»ƒç¯å¢ƒ"""
    # GPUä¼˜åŒ–
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'
    os.environ['PYTHONUNBUFFERED'] = '1'
    
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        print(f"ğŸ® GPU: {torch.cuda.get_device_properties(0).name}")
        print(f"   å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")

def get_optimized_config():
    """è·å–é’ˆå¯¹å¾®å¤šæ™®å‹’ä¼˜åŒ–çš„é…ç½®"""
    if not torch.cuda.is_available():
        return None
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    if gpu_memory >= 14:  # P100, V100ç­‰
        return {
            "vqvae_batch_size": 16,
            "transformer_batch_size": 8,
            "num_workers": 4,
            "mixed_precision": False,  # P100ä¸æ”¯æŒæ··åˆç²¾åº¦
        }
    elif gpu_memory >= 10:  # T4ç­‰
        return {
            "vqvae_batch_size": 12,
            "transformer_batch_size": 6,
            "num_workers": 2,
            "mixed_precision": True,
        }
    else:  # ä½ç«¯GPU
        return {
            "vqvae_batch_size": 8,
            "transformer_batch_size": 4,
            "num_workers": 1,
            "mixed_precision": True,
        }

def train_vqvae(args, config):
    """è®­ç»ƒVQ-VAE"""
    print("\nğŸ¯ é˜¶æ®µ1: è®­ç»ƒVQ-VAE")
    print("=" * 50)
    
    vqvae_output = Path(args.output_dir) / "vqvae"
    
    cmd = [
        "python", "training/train_vqvae.py",
        "--data_dir", args.data_dir,
        "--output_dir", str(vqvae_output),
        "--resolution", str(args.resolution),
        "--codebook_size", str(args.codebook_size),
        "--commitment_cost", str(args.commitment_cost),
        "--ema_decay", str(args.ema_decay),
        "--batch_size", str(config["vqvae_batch_size"]),
        "--num_epochs", str(args.vqvae_epochs),
        "--learning_rate", str(args.vqvae_lr),
        "--num_workers", str(config["num_workers"]),
        "--sample_interval", "500",
        "--eval_interval", "5",
        "--codebook_monitor_interval", "1",
        "--keep_checkpoints", "3",  # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpoint
        "--milestone_interval", "10",  # æ¯10ä¸ªepochä¿å­˜é‡Œç¨‹ç¢‘
        "--auto_cleanup",  # å¯ç”¨è‡ªåŠ¨æ¸…ç†
    ]
    
    print(f"ğŸš€ å¯åŠ¨VQ-VAEè®­ç»ƒ...")
    print(f"   å‘½ä»¤: {' '.join(cmd)}")
    
    # è®¾ç½®å·¥ä½œç›®å½•ä¸ºvqvae_transformer
    cwd = Path(__file__).parent
    result = subprocess.run(cmd, capture_output=False, text=True, cwd=cwd)
    
    if result.returncode != 0:
        print(f"âŒ VQ-VAEè®­ç»ƒå¤±è´¥")
        return False
    
    print(f"âœ… VQ-VAEè®­ç»ƒå®Œæˆ")
    return True

def train_transformer(args, config):
    """è®­ç»ƒTransformer"""
    print("\nğŸ¯ é˜¶æ®µ2: è®­ç»ƒTransformer")
    print("=" * 50)

    # ç¡®å®šVQ-VAEè·¯å¾„
    if args.vqvae_path:
        vqvae_path = Path(args.vqvae_path)
        print(f"ğŸ“‚ ä½¿ç”¨æŒ‡å®šçš„VQ-VAEè·¯å¾„: {vqvae_path}")
    else:
        vqvae_path = Path(args.output_dir) / "vqvae"
        print(f"ğŸ“‚ ä½¿ç”¨é»˜è®¤VQ-VAEè·¯å¾„: {vqvae_path}")

    transformer_output = Path(args.output_dir) / "transformer"
    
    # æ£€æŸ¥VQ-VAEæ˜¯å¦å­˜åœ¨
    final_model_exists = (vqvae_path / "final_model").exists()
    checkpoint_exists = (vqvae_path / "best_model.pth").exists() or len(list(vqvae_path.glob("*.pth"))) > 0

    if not final_model_exists and not checkpoint_exists:
        print(f"âŒ æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹: {vqvae_path}")
        print(f"   æœŸæœ›æ–‡ä»¶: final_model/ æˆ– *.pth")
        return False

    if final_model_exists:
        print(f"âœ… æ‰¾åˆ°VQ-VAEæ¨¡å‹ (diffusersæ ¼å¼): {vqvae_path}/final_model")
    else:
        print(f"âœ… æ‰¾åˆ°VQ-VAEæ¨¡å‹ (checkpointæ ¼å¼): {vqvae_path}/*.pth")
    
    cmd = [
        "python", "training/train_transformer.py",
        "--data_dir", args.data_dir,
        "--vqvae_path", str(vqvae_path),
        "--output_dir", str(transformer_output),
        "--resolution", str(args.resolution),
        "--codebook_size", str(args.codebook_size),
        "--num_users", str(args.num_users),
        "--n_embd", str(args.n_embd),
        "--n_layer", str(args.n_layer),
        "--n_head", str(args.n_head),
        "--batch_size", str(config["transformer_batch_size"]),
        "--num_epochs", str(args.transformer_epochs),
        "--learning_rate", str(args.transformer_lr),
        "--num_workers", str(config["num_workers"]),
        "--save_interval", "10",
        "--sample_interval", "10",
        "--generation_temperature", str(args.generation_temperature),
    ]
    
    if args.use_cross_attention:
        cmd.append("--use_cross_attention")
    
    print(f"ğŸš€ å¯åŠ¨Transformerè®­ç»ƒ...")
    print(f"   å‘½ä»¤: {' '.join(cmd)}")
    
    # è®¾ç½®å·¥ä½œç›®å½•ä¸ºvqvae_transformer
    cwd = Path(__file__).parent
    result = subprocess.run(cmd, capture_output=False, text=True, cwd=cwd)
    
    if result.returncode != 0:
        print(f"âŒ Transformerè®­ç»ƒå¤±è´¥")
        return False
    
    print(f"âœ… Transformerè®­ç»ƒå®Œæˆ")
    return True

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="VQ-VAE + Transformer å¾®å¤šæ™®å‹’ç”Ÿæˆç³»ç»Ÿ")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, default="/kaggle/input/dataset",
                       help="æ•°æ®é›†ç›®å½• (åŒ…å«ID1, ID_2, ..., ID_31ç›®å½•)")
    parser.add_argument("--output_dir", type=str, default="/kaggle/working/outputs/vqvae_transformer",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--resolution", type=int, default=128,
                       help="å›¾åƒåˆ†è¾¨ç‡")
    
    # VQ-VAEå‚æ•°
    parser.add_argument("--vqvae_path", type=str, default=None,
                       help="é¢„è®­ç»ƒVQ-VAEæ¨¡å‹è·¯å¾„ (å¦‚æœä¸æŒ‡å®šï¼Œä½¿ç”¨output_dir/vqvae)")
    parser.add_argument("--codebook_size", type=int, default=1024,
                       help="ç æœ¬å¤§å°")
    parser.add_argument("--commitment_cost", type=float, default=0.25,
                       help="CommitmentæŸå¤±æƒé‡")
    parser.add_argument("--ema_decay", type=float, default=0.99,
                       help="EMAè¡°å‡ç‡")
    parser.add_argument("--vqvae_epochs", type=int, default=80,
                       help="VQ-VAEè®­ç»ƒè½®æ•°")
    parser.add_argument("--vqvae_lr", type=float, default=1e-4,
                       help="VQ-VAEå­¦ä¹ ç‡")
    
    # Transformerå‚æ•°
    parser.add_argument("--num_users", type=int, default=31,
                       help="ç”¨æˆ·æ•°é‡")
    parser.add_argument("--n_embd", type=int, default=512,
                       help="TransformeråµŒå…¥ç»´åº¦")
    parser.add_argument("--n_layer", type=int, default=8,
                       help="Transformerå±‚æ•°")
    parser.add_argument("--n_head", type=int, default=8,
                       help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--transformer_epochs", type=int, default=50,
                       help="Transformerè®­ç»ƒè½®æ•°")
    parser.add_argument("--transformer_lr", type=float, default=1e-4,
                       help="Transformerå­¦ä¹ ç‡")
    parser.add_argument("--use_cross_attention", action="store_true",
                       help="ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--generation_temperature", type=float, default=1.0,
                       help="ç”Ÿæˆæ¸©åº¦")
    
    # è®­ç»ƒæ§åˆ¶
    parser.add_argument("--skip_vqvae", action="store_true",
                       help="è·³è¿‡VQ-VAEè®­ç»ƒ")
    parser.add_argument("--skip_transformer", action="store_true",
                       help="è·³è¿‡Transformerè®­ç»ƒ")
    
    args = parser.parse_args()
    
    print("ğŸ¨ VQ-VAE + Transformer å¾®å¤šæ™®å‹’ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 60)
    print(f"ğŸ“Š æ•°æ®é›†: {args.data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ–¼ï¸ åˆ†è¾¨ç‡: {args.resolution}x{args.resolution}")
    print(f"ğŸ“š ç æœ¬å¤§å°: {args.codebook_size}")
    print(f"ğŸ‘¥ ç”¨æˆ·æ•°é‡: {args.num_users}")
    print(f"ğŸ§  Transformer: {args.n_layer}å±‚, {args.n_embd}ç»´, {args.n_head}å¤´")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # è·å–ä¼˜åŒ–é…ç½®
    config = get_optimized_config()
    if not config:
        print("âŒ GPUé…ç½®è·å–å¤±è´¥")
        return
    
    print(f"\nğŸ”§ ä¼˜åŒ–é…ç½®:")
    print(f"   VQ-VAEæ‰¹æ¬¡å¤§å°: {config['vqvae_batch_size']}")
    print(f"   Transformeræ‰¹æ¬¡å¤§å°: {config['transformer_batch_size']}")
    print(f"   å·¥ä½œè¿›ç¨‹æ•°: {config['num_workers']}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    success = True
    
    # é˜¶æ®µ1: è®­ç»ƒVQ-VAE
    if not args.skip_vqvae:
        success = train_vqvae(args, config)
        if not success:
            print("âŒ VQ-VAEè®­ç»ƒå¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ")
            return
    else:
        print("â­ï¸ è·³è¿‡VQ-VAEè®­ç»ƒ")
    
    # é˜¶æ®µ2: è®­ç»ƒTransformer
    if not args.skip_transformer and success:
        success = train_transformer(args, config)
        if not success:
            print("âŒ Transformerè®­ç»ƒå¤±è´¥")
            return
    else:
        print("â­ï¸ è·³è¿‡Transformerè®­ç»ƒ")
    
    if success:
        print("\nğŸ‰ VQ-VAE + Transformerè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
        print(f"ğŸ” ä¸‹ä¸€æ­¥: è¿è¡Œæ¨ç†å’ŒéªŒè¯")
        print(f"   python generate_main.py --model_dir {args.output_dir}")
        print(f"   python validate_main.py --model_dir {args.output_dir}")
    else:
        print("\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")

if __name__ == "__main__":
    main()
