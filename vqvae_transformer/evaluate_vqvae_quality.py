#!/usr/bin/env python3
"""
VQ-VAEè´¨é‡å…¨é¢è¯„ä¼°è„šæœ¬
æä¾›å¤šç»´åº¦ã€å‡†ç¡®ä¸”å…¨é¢çš„VQ-VAEè´¨é‡åˆ¤æ–­
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import json
from PIL import Image
import seaborn as sns
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from scipy.stats import entropy
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from models.vqvae_model import MicroDopplerVQVAE
from utils.data_loader import MicroDopplerDataset
from utils.metrics import VQVAEMetrics, calculate_psnr, calculate_ssim

class ComprehensiveVQVAEEvaluator:
    """VQ-VAEå…¨é¢è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®æ•°æ®å˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((args.resolution, args.resolution)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
        # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
        self.vqvae_metrics = VQVAEMetrics(device=self.device)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ” VQ-VAEå…¨é¢è´¨é‡è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   åˆ†è¾¨ç‡: {args.resolution}x{args.resolution}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_model(self, model_path):
        """åŠ è½½VQ-VAEæ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½VQ-VAEæ¨¡å‹: {model_path}")
        
        if Path(model_path).is_dir():
            # ä»ç›®å½•åŠ è½½
            try:
                model = MicroDopplerVQVAE.from_pretrained(model_path)
            except:
                # å¦‚æœfrom_pretrainedå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨åŠ è½½
                config_path = Path(model_path) / "config.json"
                if config_path.exists():
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                    model = MicroDopplerVQVAE(**config)
                    model_file = Path(model_path) / "pytorch_model.bin"
                    if model_file.exists():
                        model.load_state_dict(torch.load(model_file, map_location=self.device))
                else:
                    raise ValueError(f"æ— æ³•ä»ç›®å½• {model_path} åŠ è½½æ¨¡å‹")
        else:
            # ä»checkpointåŠ è½½
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # å°è¯•ä»checkpointä¸­è·å–æ¨¡å‹å‚æ•°
            if 'args' in checkpoint:
                args_dict = vars(checkpoint['args']) if hasattr(checkpoint['args'], '__dict__') else checkpoint['args']
                model = MicroDopplerVQVAE(
                    in_channels=3,
                    out_channels=3,
                    sample_size=args_dict.get('resolution', 128) // 8,
                    num_vq_embeddings=args_dict.get('codebook_size', 1024),
                    commitment_cost=args_dict.get('commitment_cost', 0.25),
                    ema_decay=args_dict.get('ema_decay', 0.99),
                )
            else:
                # ä½¿ç”¨é»˜è®¤å‚æ•°
                model = MicroDopplerVQVAE(
                    in_channels=3,
                    out_channels=3,
                    sample_size=self.args.resolution // 8,
                    num_vq_embeddings=1024,
                )
            
            model.load_state_dict(checkpoint['model_state_dict'])
        
        model.to(self.device)
        model.eval()
        
        print(f"âœ… VQ-VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
    
    def create_dataloader(self):
        """åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨"""
        dataset = MicroDopplerDataset(
            data_dir=self.args.data_dir,
            transform=self.transform,
            return_user_id=True,
        )
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œè¿›è¡Œé‡‡æ ·
        if self.args.max_samples and self.args.max_samples < len(dataset):
            indices = np.random.choice(len(dataset), self.args.max_samples, replace=False)
            dataset = torch.utils.data.Subset(dataset, indices)
            print(f"ğŸ“Š é‡‡æ ·éªŒè¯æ•°æ®é›†: {len(dataset)} æ ·æœ¬")
        
        dataloader = DataLoader(
            dataset,
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=self.args.num_workers,
            pin_memory=True,
        )
        
        print(f"ğŸ“Š éªŒè¯æ•°æ®é›†: {len(dataset)} æ ·æœ¬, {len(dataloader)} æ‰¹æ¬¡")
        return dataloader
    
    def evaluate_reconstruction_quality(self, model, dataloader):
        """è¯„ä¼°é‡å»ºè´¨é‡ - æ ¸å¿ƒæŒ‡æ ‡"""
        print(f"\nğŸ¯ 1. é‡å»ºè´¨é‡è¯„ä¼°...")
        
        all_psnr = []
        all_ssim = []
        all_lpips = []
        all_mse = []
        all_mae = []
        
        reconstruction_errors = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(dataloader, desc="é‡å»ºè´¨é‡è¯„ä¼°")):
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                else:
                    images = batch
                    user_ids = None
                
                images = images.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = model(images, return_dict=True)
                reconstructed = outputs.sample
                
                # åå½’ä¸€åŒ–åˆ°[0,1]è¿›è¡Œè¯„ä¼°
                images_eval = (images * 0.5 + 0.5).clamp(0, 1)
                reconstructed_eval = (reconstructed * 0.5 + 0.5).clamp(0, 1)
                
                # è®¡ç®—å„ç§æŒ‡æ ‡
                for i in range(images.size(0)):
                    # PSNRå’ŒSSIM
                    psnr = calculate_psnr(images_eval[i], reconstructed_eval[i])
                    ssim = calculate_ssim(images_eval[i], reconstructed_eval[i])
                    
                    # MSEå’ŒMAE
                    mse = F.mse_loss(images_eval[i], reconstructed_eval[i]).item()
                    mae = F.l1_loss(images_eval[i], reconstructed_eval[i]).item()
                    
                    all_psnr.append(psnr)
                    all_ssim.append(ssim)
                    all_mse.append(mse)
                    all_mae.append(mae)
                    
                    # è®°å½•é‡å»ºè¯¯å·®åˆ†å¸ƒ
                    error = torch.abs(images_eval[i] - reconstructed_eval[i]).mean().item()
                    reconstruction_errors.append(error)
                    
                    # LPIPS (å¦‚æœå¯ç”¨)
                    if self.vqvae_metrics.lpips_model is not None:
                        try:
                            lpips_score = self.vqvae_metrics.lpips_model(
                                images_eval[i:i+1] * 2 - 1,  # è½¬æ¢åˆ°[-1,1]
                                reconstructed_eval[i:i+1] * 2 - 1
                            ).item()
                            all_lpips.append(lpips_score)
                        except:
                            pass
                
                # ä¿å­˜æ ·æœ¬
                if batch_idx < 5:  # ä¿å­˜å‰5ä¸ªbatchçš„æ ·æœ¬
                    self._save_reconstruction_samples(images_eval, reconstructed_eval, batch_idx)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        results = {
            'psnr': {
                'mean': np.mean(all_psnr),
                'std': np.std(all_psnr),
                'min': np.min(all_psnr),
                'max': np.max(all_psnr),
                'median': np.median(all_psnr),
                'q25': np.percentile(all_psnr, 25),
                'q75': np.percentile(all_psnr, 75),
            },
            'ssim': {
                'mean': np.mean(all_ssim),
                'std': np.std(all_ssim),
                'min': np.min(all_ssim),
                'max': np.max(all_ssim),
                'median': np.median(all_ssim),
                'q25': np.percentile(all_ssim, 25),
                'q75': np.percentile(all_ssim, 75),
            },
            'mse': {
                'mean': np.mean(all_mse),
                'std': np.std(all_mse),
                'min': np.min(all_mse),
                'max': np.max(all_mse),
            },
            'mae': {
                'mean': np.mean(all_mae),
                'std': np.std(all_mae),
                'min': np.min(all_mae),
                'max': np.max(all_mae),
            },
            'reconstruction_error_distribution': reconstruction_errors,
        }
        
        if all_lpips:
            results['lpips'] = {
                'mean': np.mean(all_lpips),
                'std': np.std(all_lpips),
                'min': np.min(all_lpips),
                'max': np.max(all_lpips),
            }
        
        # ç»˜åˆ¶åˆ†å¸ƒå›¾
        self._plot_reconstruction_metrics(all_psnr, all_ssim, all_mse, reconstruction_errors)

        return results

    def evaluate_codebook_quality(self, model, dataloader):
        """è¯„ä¼°ç æœ¬è´¨é‡ - å…³é”®æŒ‡æ ‡"""
        print(f"\nğŸ¯ 2. ç æœ¬è´¨é‡è¯„ä¼°...")

        # æ”¶é›†æ‰€æœ‰ç¼–ç ç´¢å¼•å’Œç‰¹å¾
        all_indices = []
        all_features = []
        codebook_embeddings = model.quantize.embedding.weight.data.cpu().numpy()

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="ç æœ¬è´¨é‡è¯„ä¼°"):
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                else:
                    images = batch

                images = images.to(self.device)

                # ç¼–ç 
                encode_result = model.encode(images, return_dict=True)
                indices = encode_result['encoding_indices']  # [B, H, W]
                latents = encode_result['latents']  # [B, C, H, W]

                # æ”¶é›†ç´¢å¼•å’Œç‰¹å¾
                all_indices.extend(indices.cpu().flatten().numpy())
                all_features.extend(latents.cpu().view(latents.size(0), -1).numpy())

        all_indices = np.array(all_indices)
        all_features = np.array(all_features)

        # 1. ç æœ¬ä½¿ç”¨ç»Ÿè®¡
        unique_indices, counts = np.unique(all_indices, return_counts=True)
        usage_rate = len(unique_indices) / model.quantize.n_embed

        # 2. ä½¿ç”¨åˆ†å¸ƒç†µ
        usage_probs = counts / counts.sum()
        usage_entropy = entropy(usage_probs)
        max_entropy = np.log(len(unique_indices))
        normalized_entropy = usage_entropy / max_entropy if max_entropy > 0 else 0

        # 3. ç æœ¬å‘é‡èšç±»è´¨é‡
        if len(unique_indices) > 1:
            used_embeddings = codebook_embeddings[unique_indices]
            if len(used_embeddings) >= 2:
                try:
                    # ä½¿ç”¨K-meansè¯„ä¼°èšç±»è´¨é‡
                    n_clusters = min(10, len(used_embeddings))
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(used_embeddings)
                    silhouette_avg = silhouette_score(used_embeddings, cluster_labels)
                except:
                    silhouette_avg = 0.0
            else:
                silhouette_avg = 0.0
        else:
            silhouette_avg = 0.0

        # 4. ç æœ¬å‘é‡é—´è·ç¦»åˆ†æ
        if len(unique_indices) > 1:
            used_embeddings = codebook_embeddings[unique_indices]
            # è®¡ç®—æ‰€æœ‰å‘é‡å¯¹ä¹‹é—´çš„è·ç¦»
            distances = []
            for i in range(len(used_embeddings)):
                for j in range(i+1, len(used_embeddings)):
                    dist = np.linalg.norm(used_embeddings[i] - used_embeddings[j])
                    distances.append(dist)

            min_distance = np.min(distances) if distances else 0
            mean_distance = np.mean(distances) if distances else 0
            std_distance = np.std(distances) if distances else 0
        else:
            min_distance = mean_distance = std_distance = 0

        # 5. é‡å»ºä¸€è‡´æ€§æ£€æŸ¥
        reconstruction_consistency = self._check_reconstruction_consistency(model, dataloader)

        results = {
            'usage_statistics': {
                'total_codes': model.quantize.n_embed,
                'used_codes': len(unique_indices),
                'usage_rate': usage_rate,
                'unused_codes': model.quantize.n_embed - len(unique_indices),
            },
            'usage_distribution': {
                'entropy': usage_entropy,
                'normalized_entropy': normalized_entropy,
                'max_entropy': max_entropy,
                'gini_coefficient': self._calculate_gini_coefficient(counts),
            },
            'clustering_quality': {
                'silhouette_score': silhouette_avg,
            },
            'distance_analysis': {
                'min_distance': min_distance,
                'mean_distance': mean_distance,
                'std_distance': std_distance,
            },
            'reconstruction_consistency': reconstruction_consistency,
        }

        # ç»˜åˆ¶ç æœ¬åˆ†æå›¾
        self._plot_codebook_analysis(unique_indices, counts, codebook_embeddings, used_embeddings if len(unique_indices) > 1 else None)

        return results

    def evaluate_latent_space_quality(self, model, dataloader):
        """è¯„ä¼°æ½œåœ¨ç©ºé—´è´¨é‡"""
        print(f"\nğŸ¯ 3. æ½œåœ¨ç©ºé—´è´¨é‡è¯„ä¼°...")

        all_latents = []
        all_user_ids = []

        with torch.no_grad():
            for batch in tqdm(dataloader, desc="æ½œåœ¨ç©ºé—´è¯„ä¼°"):
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, user_ids = batch
                else:
                    images = batch
                    user_ids = None

                images = images.to(self.device)

                # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
                encode_result = model.encode(images, return_dict=True)
                latents = encode_result['latents']  # [B, C, H, W]

                # å±•å¹³æ½œåœ¨è¡¨ç¤º
                latents_flat = latents.view(latents.size(0), -1)
                all_latents.append(latents_flat.cpu())

                if user_ids is not None:
                    all_user_ids.extend(user_ids.cpu().numpy())

        all_latents = torch.cat(all_latents, dim=0).numpy()

        # 1. æ½œåœ¨ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§
        latent_mean = np.mean(all_latents, axis=0)
        latent_std = np.std(all_latents, axis=0)
        latent_var = np.var(all_latents, axis=0)

        # 2. æ½œåœ¨ç©ºé—´çš„åˆ†å¸ƒç‰¹æ€§
        latent_range = np.max(all_latents) - np.min(all_latents)
        latent_sparsity = np.mean(np.abs(all_latents) < 0.1)  # æ¥è¿‘0çš„æ¯”ä¾‹

        # 3. å¦‚æœæœ‰ç”¨æˆ·IDï¼Œè¯„ä¼°ç”¨æˆ·åˆ†ç¦»åº¦
        user_separation = None
        if all_user_ids:
            user_separation = self._evaluate_user_separation(all_latents, all_user_ids)

        results = {
            'statistical_properties': {
                'mean_magnitude': np.mean(np.abs(latent_mean)),
                'std_magnitude': np.mean(latent_std),
                'variance_magnitude': np.mean(latent_var),
                'range': latent_range,
                'sparsity': latent_sparsity,
            },
            'user_separation': user_separation,
        }

        # ç»˜åˆ¶æ½œåœ¨ç©ºé—´åˆ†æå›¾
        self._plot_latent_space_analysis(all_latents, all_user_ids)

        return results

    def _check_reconstruction_consistency(self, model, dataloader):
        """æ£€æŸ¥é‡å»ºä¸€è‡´æ€§"""
        consistency_scores = []

        with torch.no_grad():
            for batch in dataloader:
                if isinstance(batch, (list, tuple)) and len(batch) == 2:
                    images, _ = batch
                else:
                    images = batch

                images = images.to(self.device)

                # ç¬¬ä¸€æ¬¡é‡å»º
                outputs1 = model(images, return_dict=True)
                reconstructed1 = outputs1.sample

                # ç¬¬äºŒæ¬¡é‡å»ºï¼ˆåº”è¯¥å®Œå…¨ä¸€è‡´ï¼‰
                outputs2 = model(images, return_dict=True)
                reconstructed2 = outputs2.sample

                # è®¡ç®—ä¸€è‡´æ€§
                consistency = F.mse_loss(reconstructed1, reconstructed2).item()
                consistency_scores.append(consistency)

                # åªæ£€æŸ¥å‡ ä¸ªbatch
                if len(consistency_scores) >= 5:
                    break

        return {
            'mean_consistency_error': np.mean(consistency_scores),
            'max_consistency_error': np.max(consistency_scores),
            'is_deterministic': np.max(consistency_scores) < 1e-6,
        }

    def _calculate_gini_coefficient(self, counts):
        """è®¡ç®—åŸºå°¼ç³»æ•°ï¼Œè¡¡é‡ä½¿ç”¨åˆ†å¸ƒçš„ä¸å‡åŒ€ç¨‹åº¦"""
        if len(counts) <= 1:
            return 0.0

        # æ’åº
        sorted_counts = np.sort(counts)
        n = len(sorted_counts)

        # è®¡ç®—åŸºå°¼ç³»æ•°
        cumsum = np.cumsum(sorted_counts)
        gini = (2 * np.sum((np.arange(1, n + 1) * sorted_counts))) / (n * cumsum[-1]) - (n + 1) / n

        return gini

    def _evaluate_user_separation(self, latents, user_ids):
        """è¯„ä¼°ç”¨æˆ·åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„åˆ†ç¦»åº¦"""
        unique_users = np.unique(user_ids)
        if len(unique_users) < 2:
            return None

        # è®¡ç®—ç±»å†…å’Œç±»é—´è·ç¦»
        intra_class_distances = []
        inter_class_distances = []

        for user in unique_users:
            user_latents = latents[np.array(user_ids) == user]

            if len(user_latents) > 1:
                # ç±»å†…è·ç¦»
                for i in range(len(user_latents)):
                    for j in range(i+1, len(user_latents)):
                        dist = np.linalg.norm(user_latents[i] - user_latents[j])
                        intra_class_distances.append(dist)

            # ç±»é—´è·ç¦»
            other_users = unique_users[unique_users != user]
            for other_user in other_users:
                other_latents = latents[np.array(user_ids) == other_user]
                for user_latent in user_latents:
                    for other_latent in other_latents:
                        dist = np.linalg.norm(user_latent - other_latent)
                        inter_class_distances.append(dist)

        if not intra_class_distances or not inter_class_distances:
            return None

        mean_intra = np.mean(intra_class_distances)
        mean_inter = np.mean(inter_class_distances)
        separation_ratio = mean_inter / mean_intra if mean_intra > 0 else float('inf')

        return {
            'mean_intra_class_distance': mean_intra,
            'mean_inter_class_distance': mean_inter,
            'separation_ratio': separation_ratio,
            'num_users': len(unique_users),
        }

    def _save_reconstruction_samples(self, original, reconstructed, batch_idx):
        """ä¿å­˜é‡å»ºæ ·æœ¬"""
        sample_dir = self.output_dir / "reconstruction_samples"
        sample_dir.mkdir(exist_ok=True)

        # ä¿å­˜å‰4ä¸ªæ ·æœ¬
        n_samples = min(4, original.size(0))

        fig, axes = plt.subplots(2, n_samples, figsize=(n_samples * 3, 6))
        if n_samples == 1:
            axes = axes.reshape(2, 1)

        for i in range(n_samples):
            # åŸå›¾
            axes[0, i].imshow(original[i].cpu().permute(1, 2, 0).numpy())
            axes[0, i].set_title(f'Original {i+1}')
            axes[0, i].axis('off')

            # é‡å»ºå›¾
            axes[1, i].imshow(reconstructed[i].cpu().permute(1, 2, 0).numpy())
            axes[1, i].set_title(f'Reconstructed {i+1}')
            axes[1, i].axis('off')

        plt.tight_layout()
        plt.savefig(sample_dir / f"batch_{batch_idx:04d}.png", dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_reconstruction_metrics(self, psnr_values, ssim_values, mse_values, error_values):
        """ç»˜åˆ¶é‡å»ºæŒ‡æ ‡åˆ†å¸ƒå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # PSNRåˆ†å¸ƒ
        axes[0, 0].hist(psnr_values, bins=50, alpha=0.7, color='blue')
        axes[0, 0].set_xlabel('PSNR (dB)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title(f'PSNR Distribution\nMean: {np.mean(psnr_values):.2f} Â± {np.std(psnr_values):.2f}')
        axes[0, 0].axvline(np.mean(psnr_values), color='red', linestyle='--', label='Mean')
        axes[0, 0].legend()

        # SSIMåˆ†å¸ƒ
        axes[0, 1].hist(ssim_values, bins=50, alpha=0.7, color='green')
        axes[0, 1].set_xlabel('SSIM')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'SSIM Distribution\nMean: {np.mean(ssim_values):.4f} Â± {np.std(ssim_values):.4f}')
        axes[0, 1].axvline(np.mean(ssim_values), color='red', linestyle='--', label='Mean')
        axes[0, 1].legend()

        # MSEåˆ†å¸ƒ
        axes[1, 0].hist(mse_values, bins=50, alpha=0.7, color='orange')
        axes[1, 0].set_xlabel('MSE')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title(f'MSE Distribution\nMean: {np.mean(mse_values):.6f}')
        axes[1, 0].set_yscale('log')

        # é‡å»ºè¯¯å·®åˆ†å¸ƒ
        axes[1, 1].hist(error_values, bins=50, alpha=0.7, color='purple')
        axes[1, 1].set_xlabel('Reconstruction Error')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title(f'Reconstruction Error Distribution\nMean: {np.mean(error_values):.6f}')

        plt.tight_layout()
        plt.savefig(self.output_dir / "reconstruction_metrics.png", dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_codebook_analysis(self, unique_indices, counts, all_embeddings, used_embeddings):
        """ç»˜åˆ¶ç æœ¬åˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. ç æœ¬ä½¿ç”¨åˆ†å¸ƒ
        axes[0, 0].bar(range(len(counts)), sorted(counts, reverse=True))
        axes[0, 0].set_xlabel('Code Index (sorted by usage)')
        axes[0, 0].set_ylabel('Usage Count')
        axes[0, 0].set_title(f'Codebook Usage Distribution\n{len(unique_indices)}/{len(all_embeddings)} codes used')
        axes[0, 0].set_yscale('log')

        # 2. ä½¿ç”¨é¢‘ç‡ç›´æ–¹å›¾
        axes[0, 1].hist(counts, bins=min(50, len(counts)), alpha=0.7)
        axes[0, 1].set_xlabel('Usage Count')
        axes[0, 1].set_ylabel('Number of Codes')
        axes[0, 1].set_title('Usage Frequency Histogram')

        # 3. ç æœ¬å‘é‡çš„2DæŠ•å½±ï¼ˆå¦‚æœæœ‰ä½¿ç”¨çš„å‘é‡ï¼‰
        if used_embeddings is not None and len(used_embeddings) > 1:
            try:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=2)
                embeddings_2d = pca.fit_transform(used_embeddings)

                scatter = axes[1, 0].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                                           c=counts, cmap='viridis', alpha=0.7)
                axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
                axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
                axes[1, 0].set_title('Codebook Embeddings (PCA)')
                plt.colorbar(scatter, ax=axes[1, 0], label='Usage Count')
            except:
                axes[1, 0].text(0.5, 0.5, 'PCA failed',
                              ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Codebook Embeddings (PCA)')
        else:
            axes[1, 0].text(0.5, 0.5, 'Insufficient data for PCA',
                          ha='center', va='center', transform=axes[1, 0].transAxes)
            axes[1, 0].set_title('Codebook Embeddings (PCA)')

        # 4. ç´¯ç§¯ä½¿ç”¨ç‡
        sorted_counts = sorted(counts, reverse=True)
        cumulative_usage = np.cumsum(sorted_counts) / np.sum(sorted_counts)
        axes[1, 1].plot(range(1, len(cumulative_usage) + 1), cumulative_usage)
        axes[1, 1].set_xlabel('Number of Most Used Codes')
        axes[1, 1].set_ylabel('Cumulative Usage Ratio')
        axes[1, 1].set_title('Cumulative Usage Distribution')
        axes[1, 1].grid(True, alpha=0.3)

        # æ·»åŠ 80%ä½¿ç”¨ç‡çº¿
        if len(cumulative_usage) > 0:
            idx_80 = np.where(cumulative_usage >= 0.8)[0]
            if len(idx_80) > 0:
                axes[1, 1].axvline(idx_80[0] + 1, color='red', linestyle='--',
                                 label=f'80% usage: {idx_80[0] + 1} codes')
                axes[1, 1].legend()

        plt.tight_layout()
        plt.savefig(self.output_dir / "codebook_analysis.png", dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_latent_space_analysis(self, latents, user_ids):
        """ç»˜åˆ¶æ½œåœ¨ç©ºé—´åˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. æ½œåœ¨å‘é‡çš„ç»Ÿè®¡åˆ†å¸ƒ
        latent_means = np.mean(latents, axis=0)
        axes[0, 0].hist(latent_means, bins=50, alpha=0.7)
        axes[0, 0].set_xlabel('Mean Value')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Distribution of Latent Dimension Means')

        # 2. æ½œåœ¨å‘é‡çš„æ–¹å·®åˆ†å¸ƒ
        latent_vars = np.var(latents, axis=0)
        axes[0, 1].hist(latent_vars, bins=50, alpha=0.7)
        axes[0, 1].set_xlabel('Variance')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title('Distribution of Latent Dimension Variances')
        axes[0, 1].set_yscale('log')

        # 3. æ½œåœ¨ç©ºé—´çš„2DæŠ•å½±
        if latents.shape[0] > 1:
            try:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=2)
                latents_2d = pca.fit_transform(latents)

                if user_ids:
                    unique_users = np.unique(user_ids)
                    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_users)))
                    for i, user in enumerate(unique_users):
                        mask = np.array(user_ids) == user
                        axes[1, 0].scatter(latents_2d[mask, 0], latents_2d[mask, 1],
                                         c=[colors[i]], label=f'User {user}', alpha=0.6)
                    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
                else:
                    axes[1, 0].scatter(latents_2d[:, 0], latents_2d[:, 1], alpha=0.6)

                axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
                axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
                axes[1, 0].set_title('Latent Space (PCA)')
            except:
                axes[1, 0].text(0.5, 0.5, 'PCA failed',
                              ha='center', va='center', transform=axes[1, 0].transAxes)
        else:
            axes[1, 0].text(0.5, 0.5, 'Insufficient data for PCA',
                          ha='center', va='center', transform=axes[1, 0].transAxes)

        # 4. æ½œåœ¨å‘é‡çš„èŒƒæ•°åˆ†å¸ƒ
        latent_norms = np.linalg.norm(latents, axis=1)
        axes[1, 1].hist(latent_norms, bins=50, alpha=0.7)
        axes[1, 1].set_xlabel('L2 Norm')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Distribution of Latent Vector Norms')

        plt.tight_layout()
        plt.savefig(self.output_dir / "latent_space_analysis.png", dpi=150, bbox_inches='tight')
        plt.close()

    def generate_comprehensive_report(self, reconstruction_results, codebook_results, latent_results):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print(f"\nğŸ“‹ ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")

        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = self._calculate_quality_score(reconstruction_results, codebook_results, latent_results)

        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report = self._generate_text_report(reconstruction_results, codebook_results, latent_results, quality_score)

        # ä¿å­˜æŠ¥å‘Š
        with open(self.output_dir / "comprehensive_report.txt", 'w', encoding='utf-8') as f:
            f.write(report)

        # ä¿å­˜JSONç»“æœ
        results = {
            'reconstruction_quality': reconstruction_results,
            'codebook_quality': codebook_results,
            'latent_space_quality': latent_results,
            'overall_quality_score': quality_score,
        }

        with open(self.output_dir / "evaluation_results.json", 'w') as f:
            json.dump(results, f, indent=2, default=str)

        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir}")
        return quality_score

    def _calculate_quality_score(self, recon_results, codebook_results, latent_results):
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•° (0-100)"""
        scores = {}

        # 1. é‡å»ºè´¨é‡åˆ†æ•° (40%)
        psnr_score = min(100, max(0, (recon_results['psnr']['mean'] - 20) * 2))  # 20dB=0åˆ†, 70dB=100åˆ†
        ssim_score = recon_results['ssim']['mean'] * 100  # ç›´æ¥è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
        recon_score = (psnr_score + ssim_score) / 2
        scores['reconstruction'] = recon_score

        # 2. ç æœ¬è´¨é‡åˆ†æ•° (40%)
        usage_score = codebook_results['usage_statistics']['usage_rate'] * 100
        entropy_score = codebook_results['usage_distribution']['normalized_entropy'] * 100
        consistency_score = 100 if codebook_results['reconstruction_consistency']['is_deterministic'] else 50
        codebook_score = (usage_score + entropy_score + consistency_score) / 3
        scores['codebook'] = codebook_score

        # 3. æ½œåœ¨ç©ºé—´è´¨é‡åˆ†æ•° (20%)
        sparsity_penalty = latent_results['statistical_properties']['sparsity'] * 50  # ç¨€ç–æ€§æƒ©ç½š
        latent_score = max(0, 100 - sparsity_penalty)
        scores['latent_space'] = latent_score

        # ç»¼åˆåˆ†æ•°
        overall_score = (
            scores['reconstruction'] * 0.4 +
            scores['codebook'] * 0.4 +
            scores['latent_space'] * 0.2
        )

        scores['overall'] = overall_score
        return scores

    def _generate_text_report(self, recon_results, codebook_results, latent_results, quality_scores):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("VQ-VAE è´¨é‡å…¨é¢è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")

        # æ€»ä½“è¯„åˆ†
        overall_score = quality_scores['overall']
        if overall_score >= 80:
            grade = "ä¼˜ç§€ (A)"
        elif overall_score >= 70:
            grade = "è‰¯å¥½ (B)"
        elif overall_score >= 60:
            grade = "åŠæ ¼ (C)"
        else:
            grade = "éœ€è¦æ”¹è¿› (D)"

        report.append(f"ğŸ“Š æ€»ä½“è´¨é‡è¯„åˆ†: {overall_score:.1f}/100 - {grade}")
        report.append("")

        # 1. é‡å»ºè´¨é‡è¯„ä¼°
        report.append("ğŸ¯ 1. é‡å»ºè´¨é‡è¯„ä¼°")
        report.append("-" * 40)
        psnr = recon_results['psnr']
        ssim = recon_results['ssim']
        report.append(f"PSNR: {psnr['mean']:.2f} Â± {psnr['std']:.2f} dB (èŒƒå›´: {psnr['min']:.2f} - {psnr['max']:.2f})")
        report.append(f"SSIM: {ssim['mean']:.4f} Â± {ssim['std']:.4f} (èŒƒå›´: {ssim['min']:.4f} - {ssim['max']:.4f})")
        report.append(f"MSE: {recon_results['mse']['mean']:.6f}")
        report.append(f"MAE: {recon_results['mae']['mean']:.6f}")

        if 'lpips' in recon_results:
            lpips = recon_results['lpips']
            report.append(f"LPIPS: {lpips['mean']:.4f} Â± {lpips['std']:.4f}")

        # é‡å»ºè´¨é‡åˆ¤æ–­
        if psnr['mean'] >= 30:
            report.append("âœ… é‡å»ºè´¨é‡: ä¼˜ç§€")
        elif psnr['mean'] >= 25:
            report.append("âœ… é‡å»ºè´¨é‡: è‰¯å¥½")
        elif psnr['mean'] >= 20:
            report.append("âš ï¸ é‡å»ºè´¨é‡: ä¸€èˆ¬")
        else:
            report.append("âŒ é‡å»ºè´¨é‡: è¾ƒå·®")
        report.append("")

        # 2. ç æœ¬è´¨é‡è¯„ä¼°
        report.append("ğŸ¯ 2. ç æœ¬è´¨é‡è¯„ä¼°")
        report.append("-" * 40)
        usage_stats = codebook_results['usage_statistics']
        usage_dist = codebook_results['usage_distribution']

        report.append(f"ç æœ¬ä½¿ç”¨ç‡: {usage_stats['usage_rate']:.3f} ({usage_stats['used_codes']}/{usage_stats['total_codes']})")
        report.append(f"ä½¿ç”¨ç†µ: {usage_dist['entropy']:.3f} (å½’ä¸€åŒ–: {usage_dist['normalized_entropy']:.3f})")
        report.append(f"åŸºå°¼ç³»æ•°: {usage_dist['gini_coefficient']:.3f}")

        consistency = codebook_results['reconstruction_consistency']
        report.append(f"é‡å»ºä¸€è‡´æ€§: {'ç¡®å®šæ€§' if consistency['is_deterministic'] else 'éç¡®å®šæ€§'}")

        # ç æœ¬è´¨é‡åˆ¤æ–­
        usage_rate = usage_stats['usage_rate']
        if usage_rate >= 0.7:
            report.append("âœ… ç æœ¬ä½¿ç”¨: ä¼˜ç§€ï¼Œæ— åç¼©é£é™©")
        elif usage_rate >= 0.5:
            report.append("âœ… ç æœ¬ä½¿ç”¨: è‰¯å¥½")
        elif usage_rate >= 0.3:
            report.append("âš ï¸ ç æœ¬ä½¿ç”¨: ä¸€èˆ¬ï¼Œéœ€è¦å…³æ³¨")
        else:
            report.append("âŒ ç æœ¬ä½¿ç”¨: è¾ƒå·®ï¼Œå­˜åœ¨åç¼©é£é™©")
        report.append("")

        # 3. æ½œåœ¨ç©ºé—´è´¨é‡
        report.append("ğŸ¯ 3. æ½œåœ¨ç©ºé—´è´¨é‡è¯„ä¼°")
        report.append("-" * 40)
        stat_props = latent_results['statistical_properties']
        report.append(f"å¹³å‡å¹…åº¦: {stat_props['mean_magnitude']:.4f}")
        report.append(f"æ ‡å‡†å·®å¹…åº¦: {stat_props['std_magnitude']:.4f}")
        report.append(f"å€¼åŸŸèŒƒå›´: {stat_props['range']:.4f}")
        report.append(f"ç¨€ç–æ€§: {stat_props['sparsity']:.3f}")

        if latent_results['user_separation']:
            user_sep = latent_results['user_separation']
            report.append(f"ç”¨æˆ·åˆ†ç¦»æ¯”: {user_sep['separation_ratio']:.2f}")
            report.append(f"ç”¨æˆ·æ•°é‡: {user_sep['num_users']}")

        report.append("")

        # 4. å»ºè®®å’Œæ€»ç»“
        report.append("ğŸ¯ 4. æ”¹è¿›å»ºè®®")
        report.append("-" * 40)

        suggestions = []
        if psnr['mean'] < 25:
            suggestions.append("â€¢ è€ƒè™‘å¢åŠ æ¨¡å‹å®¹é‡æˆ–è°ƒæ•´æŸå¤±å‡½æ•°æƒé‡ä»¥æé«˜é‡å»ºè´¨é‡")

        if usage_rate < 0.5:
            suggestions.append("â€¢ ç æœ¬ä½¿ç”¨ç‡åä½ï¼Œå»ºè®®è°ƒæ•´EMAè¡°å‡ç‡æˆ–commitmentæƒé‡")

        if stat_props['sparsity'] > 0.8:
            suggestions.append("â€¢ æ½œåœ¨ç©ºé—´è¿‡äºç¨€ç–ï¼Œå¯èƒ½å½±å“è¡¨è¾¾èƒ½åŠ›")

        if not suggestions:
            suggestions.append("â€¢ æ¨¡å‹è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–è¶…å‚æ•°")

        for suggestion in suggestions:
            report.append(suggestion)

        report.append("")
        report.append("=" * 80)

        return "\n".join(report)

    def run_comprehensive_evaluation(self):
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        print(f"ğŸš€ å¼€å§‹VQ-VAEå…¨é¢è´¨é‡è¯„ä¼°...")

        # åŠ è½½æ¨¡å‹
        model = self.load_model(self.args.model_path)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = self.create_dataloader()

        # 1. é‡å»ºè´¨é‡è¯„ä¼°
        reconstruction_results = self.evaluate_reconstruction_quality(model, dataloader)

        # 2. ç æœ¬è´¨é‡è¯„ä¼°
        codebook_results = self.evaluate_codebook_quality(model, dataloader)

        # 3. æ½œåœ¨ç©ºé—´è´¨é‡è¯„ä¼°
        latent_results = self.evaluate_latent_space_quality(model, dataloader)

        # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        quality_scores = self.generate_comprehensive_report(
            reconstruction_results, codebook_results, latent_results
        )

        # æ‰“å°æ€»ç»“
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š æ€»ä½“è´¨é‡åˆ†æ•°: {quality_scores['overall']:.1f}/100")
        print(f"   - é‡å»ºè´¨é‡: {quality_scores['reconstruction']:.1f}/100")
        print(f"   - ç æœ¬è´¨é‡: {quality_scores['codebook']:.1f}/100")
        print(f"   - æ½œåœ¨ç©ºé—´: {quality_scores['latent_space']:.1f}/100")
        print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šä¿å­˜åœ¨: {self.output_dir}")

        return quality_scores


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="VQ-VAEè´¨é‡å…¨é¢è¯„ä¼°")

    # å¿…éœ€å‚æ•°
    parser.add_argument("--model_path", type=str, required=True,
                       help="VQ-VAEæ¨¡å‹è·¯å¾„ (checkpointæ–‡ä»¶æˆ–æ¨¡å‹ç›®å½•)")
    parser.add_argument("--data_dir", type=str, required=True,
                       help="éªŒè¯æ•°æ®é›†ç›®å½•")

    # å¯é€‰å‚æ•°
    parser.add_argument("--output_dir", type=str, default="outputs/vqvae_evaluation",
                       help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--resolution", type=int, default=128,
                       help="å›¾åƒåˆ†è¾¨ç‡")
    parser.add_argument("--batch_size", type=int, default=16,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_samples", type=int, default=1000,
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•° (Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨)")
    parser.add_argument("--num_workers", type=int, default=4,
                       help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")

    args = parser.parse_args()

    # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
    evaluator = ComprehensiveVQVAEEvaluator(args)
    quality_scores = evaluator.run_comprehensive_evaluation()

    # æ ¹æ®åˆ†æ•°ç»™å‡ºæœ€ç»ˆå»ºè®®
    overall_score = quality_scores['overall']
    print(f"\nğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“è®º:")

    if overall_score >= 80:
        print(f"ğŸ† æ¨¡å‹è´¨é‡ä¼˜ç§€! å¯ä»¥ç”¨äºç”Ÿäº§ç¯å¢ƒ")
    elif overall_score >= 70:
        print(f"âœ… æ¨¡å‹è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘éƒ¨ç½²")
    elif overall_score >= 60:
        print(f"âš ï¸ æ¨¡å‹è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print(f"âŒ æ¨¡å‹è´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´æ¶æ„")

    print(f"ğŸ“‹ è¯¦ç»†åˆ†ææŠ¥å‘Šè¯·æŸ¥çœ‹: {args.output_dir}/comprehensive_report.txt")


if __name__ == "__main__":
    main()
