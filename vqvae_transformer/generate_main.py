#!/usr/bin/env python3
"""
VQ-VAE + Transformer ä¸»ç”Ÿæˆè„šæœ¬
ä»ç”¨æˆ·IDç”Ÿæˆå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from pathlib import Path
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# æ¡ä»¶å¯¼å…¥æ¨¡å‹ - æ£€æŸ¥ç¯å¢ƒå…¼å®¹æ€§
try:
    from models.vqvae_model import MicroDopplerVQVAE
    VQVAE_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥VQ-VAEæ¨¡å‹: {e}")
    print("   è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œç”Ÿæˆè„šæœ¬")
    VQVAE_AVAILABLE = False

try:
    from models.transformer_model import MicroDopplerTransformer
    TRANSFORMER_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥Transformeræ¨¡å‹: {e}")
    print("   è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œç”Ÿæˆè„šæœ¬")
    TRANSFORMER_AVAILABLE = False

# æ£€æŸ¥å¿…è¦ç»„ä»¶
if not (VQVAE_AVAILABLE and TRANSFORMER_AVAILABLE):
    print("âŒ ç”Ÿæˆè„šæœ¬éœ€è¦åŒæ—¶æ”¯æŒVQ-VAEå’ŒTransformer")
    print("   å»ºè®®åœ¨Transformerç¯å¢ƒä¸­è¿è¡Œï¼Œå› ä¸ºå®ƒå¯ä»¥åŠ è½½VQ-VAEæ¨¡å‹")
    sys.exit(1)

class VQVAETransformerGenerator:
    """VQ-VAE + Transformer ç”Ÿæˆå™¨"""
    
    def __init__(self, model_dir, device="auto", vqvae_path=None, transformer_path=None):
        self.model_dir = Path(model_dir)
        self.vqvae_path = Path(vqvae_path) if vqvae_path else None
        self.transformer_path = Path(transformer_path) if transformer_path else None

        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½æ¨¡å‹
        self.vqvae_model = self._load_vqvae()
        self.transformer_model = self._load_transformer()

        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _load_vqvae(self):
        """åŠ è½½VQ-VAEæ¨¡å‹"""
        if self.vqvae_path:
            vqvae_path = self.vqvae_path
        else:
            vqvae_path = self.model_dir / "vqvae"

        print(f"ğŸ“¦ åŠ è½½VQ-VAE: {vqvae_path}")

        # å°è¯•diffusersæ ¼å¼åŠ è½½
        try:
            if (vqvae_path / "config.json").exists():
                print("   æ£€æµ‹åˆ°diffusersæ ¼å¼ï¼Œä½¿ç”¨from_pretrainedåŠ è½½...")
                model = MicroDopplerVQVAE.from_pretrained(str(vqvae_path))
                model.to(self.device)
                model.eval()
                return model
        except Exception as e:
            print(f"   diffusersæ ¼å¼åŠ è½½å¤±è´¥: {e}")

        # å°è¯•checkpointæ ¼å¼åŠ è½½
        checkpoint_path = None
        if vqvae_path.is_file() and vqvae_path.suffix == '.pth':
            checkpoint_path = vqvae_path
        else:
            for filename in ["best_model.pth", "final_model.pth", "model.pth"]:
                potential_path = vqvae_path / filename
                if potential_path.exists():
                    checkpoint_path = potential_path
                    break

        if checkpoint_path and checkpoint_path.exists():
            print(f"   ä½¿ç”¨checkpointæ ¼å¼: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location="cpu")

            # é‡å»ºæ¨¡å‹
            model = MicroDopplerVQVAE(
                num_vq_embeddings=checkpoint['args'].codebook_size,
                commitment_cost=checkpoint['args'].commitment_cost,
                ema_decay=getattr(checkpoint['args'], 'ema_decay', 0.99),
            )
            model.load_state_dict(checkpoint['model_state_dict'])

            model.to(self.device)
            model.eval()
            return model

        raise FileNotFoundError(f"æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹: {vqvae_path}")
    
    def _load_transformer(self):
        """åŠ è½½Transformeræ¨¡å‹"""
        if self.transformer_path:
            # ä½¿ç”¨æŒ‡å®šçš„Transformerè·¯å¾„
            if self.transformer_path.is_file():
                checkpoint_path = self.transformer_path
            else:
                checkpoint_path = self.transformer_path / "best_model.pth"
                if not checkpoint_path.exists():
                    checkpoint_path = self.transformer_path / "final_model.pth"
        else:
            # ä½¿ç”¨é»˜è®¤çš„ç›®å½•ç»“æ„
            transformer_path = self.model_dir / "transformer"
            checkpoint_path = transformer_path / "best_model.pth"
            if not checkpoint_path.exists():
                checkpoint_path = transformer_path / "final_model.pth"

        print(f"ğŸ“¦ åŠ è½½Transformer: {checkpoint_path}")

        if not checkpoint_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°Transformeræ¨¡å‹: {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path, map_location="cpu")

        # æ£€æŸ¥æ¨¡å‹ç±»å‹
        state_dict = checkpoint.get('model_state_dict', checkpoint)

        # æ£€æŸ¥æ˜¯å¦æ˜¯VQ-VAEæ¨¡å‹ï¼ˆé”™è¯¯çš„æ–‡ä»¶ï¼‰
        vqvae_keys = ['encoder.conv_in.weight', 'decoder.conv_in.weight', 'quantize.embedding.weight']
        transformer_keys = ['transformer.transformer.wte.weight', 'user_encoder.user_embedding.weight']

        is_vqvae = any(key in state_dict for key in vqvae_keys)
        is_transformer = any(key in state_dict for key in transformer_keys)

        if is_vqvae and not is_transformer:
            raise ValueError(
                f"âŒ æ£€æµ‹åˆ°VQ-VAEæ¨¡å‹æƒé‡ï¼Œä½†æœŸæœ›Transformeræ¨¡å‹ï¼\n"
                f"   æ–‡ä»¶è·¯å¾„: {checkpoint_path}\n"
                f"   è¯·æ£€æŸ¥æ‚¨çš„Transformeræ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\n"
                f"   å½“å‰æ–‡ä»¶åŒ…å«çš„æ˜¯VQ-VAEæ¨¡å‹æƒé‡ï¼Œä¸æ˜¯Transformeræƒé‡ã€‚"
            )

        args = checkpoint.get('args')
        if args is None:
            raise ValueError(f"Checkpointä¸­ç¼ºå°‘argsä¿¡æ¯: {checkpoint_path}")

        # é‡å»ºæ¨¡å‹ - æ·»åŠ é»˜è®¤å€¼å¤„ç†
        model = MicroDopplerTransformer(
            vocab_size=getattr(args, 'codebook_size', 1024),
            max_seq_len=getattr(args, 'max_seq_len', 1024),
            num_users=getattr(args, 'num_users', 31),  # é»˜è®¤31ä¸ªç”¨æˆ·
            n_embd=getattr(args, 'n_embd', 512),
            n_layer=getattr(args, 'n_layer', 8),
            n_head=getattr(args, 'n_head', 8),
            dropout=getattr(args, 'dropout', 0.1),
            use_cross_attention=getattr(args, 'use_cross_attention', True),
        )

        model.load_state_dict(state_dict)
        model.to(self.device)
        model.eval()
        return model
    
    def generate_for_user(
        self,
        user_id: int,
        num_samples: int = 5,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        diversity_boost: float = 1.2,
    ):
        """
        ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆå›¾åƒ
        Args:
            user_id: ç”¨æˆ·ID (0-30)
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            diversity_boost: å¤šæ ·æ€§å¢å¼ºå› å­
        Returns:
            generated_images: [num_samples, 3, H, W] ç”Ÿæˆçš„å›¾åƒ
        """
        print(f"ğŸ¨ ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆ {num_samples} å¼ å›¾åƒ...")
        
        generated_images = []
        
        with torch.no_grad():
            for i in range(num_samples):
                # ä¸ºæ¯ä¸ªæ ·æœ¬ä½¿ç”¨ç•¥å¾®ä¸åŒçš„é‡‡æ ·å‚æ•°ä»¥å¢åŠ å¤šæ ·æ€§
                sample_temp = temperature * (1 + (i / num_samples - 0.5) * 0.2 * diversity_boost)
                sample_top_p = max(0.7, top_p - (i / num_samples) * 0.1 * diversity_boost)
                
                # ç”Ÿæˆtokenåºåˆ—
                user_ids = torch.tensor([user_id], device=self.device)
                
                generated_tokens = self.transformer_model.generate(
                    user_ids=user_ids,
                    max_length=self.transformer_model.max_seq_len,
                    temperature=sample_temp,
                    top_k=top_k,
                    top_p=sample_top_p,
                    do_sample=True,
                    num_return_sequences=1,
                )
                
                # è½¬æ¢ä¸ºå›¾åƒ
                image = self._tokens_to_image(generated_tokens[0])
                generated_images.append(image)
                
                print(f"  âœ… æ ·æœ¬ {i+1}/{num_samples} å®Œæˆ")
        
        return torch.stack(generated_images)
    
    def _tokens_to_image(self, tokens):
        """å°†tokenåºåˆ—è½¬æ¢ä¸ºå›¾åƒ"""
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†ç”¨æˆ·tokenå’Œåºåˆ—é•¿åº¦

        # 1. ç§»é™¤ç”¨æˆ·tokenï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if len(tokens) == 1025:
            # ç¬¬ä¸€ä¸ªtokenæ˜¯ç”¨æˆ·tokenï¼Œç§»é™¤å®ƒ
            image_tokens = tokens[1:]
            print(f"   ç§»é™¤ç”¨æˆ·tokenï¼Œå‰©ä½™å›¾åƒtoken: {len(image_tokens)}")
        elif len(tokens) == 1024:
            # å·²ç»æ˜¯çº¯å›¾åƒtoken
            image_tokens = tokens
        else:
            # è°ƒæ•´åˆ°1024ä¸ªtoken
            if len(tokens) > 1024:
                image_tokens = tokens[:1024]
                print(f"   æˆªæ–­tokenåºåˆ—åˆ°1024")
            else:
                # å¡«å……åˆ°1024
                pad_tokens = torch.full((1024 - len(tokens),), 0, device=tokens.device)
                image_tokens = torch.cat([tokens, pad_tokens])
                print(f"   å¡«å……tokenåºåˆ—åˆ°1024")

        # 2. é‡å¡‘ä¸º32x32ï¼ˆVQ-VAEçš„å®é™…è¾“å‡ºå°ºå¯¸ï¼‰
        tokens_2d = image_tokens.view(32, 32)

        # 3. è·å–ç æœ¬åµŒå…¥å¹¶è§£ç 
        with torch.no_grad():
            # ç¡®ä¿tokenç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            tokens_2d = torch.clamp(tokens_2d, 0, self.vqvae_model.quantize.n_embed - 1)

            # æ·»åŠ batchç»´åº¦
            batch_tokens = tokens_2d.unsqueeze(0)  # [1, 32, 32]

            # è·å–é‡åŒ–å‘é‡
            quantized_latents = self.vqvae_model.quantize.embedding(batch_tokens)  # [1, 32, 32, 256]
            quantized_latents = quantized_latents.permute(0, 3, 1, 2)  # [1, 256, 32, 32]

            # è§£ç ä¸ºå›¾åƒ
            generated_image = self.vqvae_model.decode(quantized_latents, force_not_quantize=True)

            # å½’ä¸€åŒ–åˆ°[0,1]
            image = (generated_image.squeeze(0) * 0.5 + 0.5).clamp(0, 1)

            return image
    
    def generate_dataset(
        self,
        output_dir: str,
        samples_per_user: int = 10,
        user_list: list = None,
        **generation_kwargs
    ):
        """
        ç”Ÿæˆå®Œæ•´æ•°æ®é›†
        Args:
            output_dir: è¾“å‡ºç›®å½•
            samples_per_user: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°
            user_list: ç”¨æˆ·åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰ç”¨æˆ·
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        if user_list is None:
            user_list = list(range(self.transformer_model.num_users))
        
        print(f"ğŸ¯ ç”Ÿæˆæ•°æ®é›†:")
        print(f"   è¾“å‡ºç›®å½•: {output_path}")
        print(f"   ç”¨æˆ·æ•°é‡: {len(user_list)}")
        print(f"   æ¯ç”¨æˆ·æ ·æœ¬æ•°: {samples_per_user}")
        print(f"   æ€»æ ·æœ¬æ•°: {len(user_list) * samples_per_user}")
        
        total_generated = 0
        
        for user_id in tqdm(user_list, desc="ç”Ÿæˆç”¨æˆ·æ•°æ®"):
            user_dir = output_path / f"user_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå›¾åƒ
            generated_images = self.generate_for_user(
                user_id=user_id,
                num_samples=samples_per_user,
                **generation_kwargs
            )
            
            # ä¿å­˜å›¾åƒ
            for i, image in enumerate(generated_images):
                image_pil = transforms.ToPILImage()(image.cpu())
                save_path = user_dir / f"generated_{i:03d}.png"
                image_pil.save(save_path)
                total_generated += 1
            
            print(f"âœ… ç”¨æˆ· {user_id}: {samples_per_user} å¼ å›¾åƒä¿å­˜åˆ° {user_dir}")
        
        print(f"\nğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        print(f"   æ€»è®¡ç”Ÿæˆ: {total_generated} å¼ å›¾åƒ")
        print(f"   ä¿å­˜ä½ç½®: {output_path}")
        
        return output_path
    
    def visualize_generation(self, user_id: int, num_samples: int = 4, save_path: str = None):
        """å¯è§†åŒ–ç”Ÿæˆç»“æœ"""
        generated_images = self.generate_for_user(user_id, num_samples)
        
        fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 4, 4))
        if num_samples == 1:
            axes = [axes]
        
        for i, image in enumerate(generated_images):
            axes[i].imshow(image.cpu().permute(1, 2, 0))
            axes[i].set_title(f'User {user_id} - Sample {i+1}')
            axes[i].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ’¾ å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {save_path}")
        
        plt.show()

def main():
    parser = argparse.ArgumentParser(description="VQ-VAE + Transformer å›¾åƒç”Ÿæˆ")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_dir", type=str, required=True,
                       help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--vqvae_path", type=str, default=None,
                       help="VQ-VAEæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸åœ¨é»˜è®¤ä½ç½®ï¼‰")
    parser.add_argument("--transformer_path", type=str, default=None,
                       help="Transformeræ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸åœ¨é»˜è®¤ä½ç½®ï¼‰")
    parser.add_argument("--output_dir", type=str, default="generated_images",
                       help="è¾“å‡ºç›®å½•")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--user_id", type=int, default=None,
                       help="æŒ‡å®šç”¨æˆ·ID (ä¸æŒ‡å®šåˆ™ç”Ÿæˆæ‰€æœ‰ç”¨æˆ·)")
    parser.add_argument("--samples_per_user", type=int, default=10,
                       help="æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°")
    parser.add_argument("--temperature", type=float, default=1.0,
                       help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top_k", type=int, default=50,
                       help="Top-ké‡‡æ ·")
    parser.add_argument("--top_p", type=float, default=0.9,
                       help="Top-pé‡‡æ ·")
    parser.add_argument("--diversity_boost", type=float, default=1.2,
                       help="å¤šæ ·æ€§å¢å¼ºå› å­")
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument("--visualize_only", action="store_true",
                       help="åªå¯è§†åŒ–ï¼Œä¸ä¿å­˜æ•°æ®é›†")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    print("ğŸ¨ VQ-VAE + Transformer å›¾åƒç”Ÿæˆå™¨")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = VQVAETransformerGenerator(
        model_dir=args.model_dir,
        device=args.device,
        vqvae_path=args.vqvae_path,
        transformer_path=args.transformer_path
    )
    
    if args.visualize_only:
        # åªå¯è§†åŒ–
        user_id = args.user_id if args.user_id is not None else 0
        save_path = f"visualization_user_{user_id}.png"
        generator.visualize_generation(user_id, 4, save_path)
    else:
        # ç”Ÿæˆæ•°æ®é›†
        user_list = [args.user_id] if args.user_id is not None else None
        
        output_path = generator.generate_dataset(
            output_dir=args.output_dir,
            samples_per_user=args.samples_per_user,
            user_list=user_list,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
            diversity_boost=args.diversity_boost,
        )
        
        print(f"\nğŸ” ä¸‹ä¸€æ­¥: è¿è¡ŒéªŒè¯")
        print(f"   python validate_main.py \\")
        print(f"     --model_dir {args.model_dir} \\")
        print(f"     --real_data_dir /path/to/real/data \\")
        print(f"     --generated_data_dir {output_path} \\")
        print(f"     --target_user_id {args.user_id or 0}")

if __name__ == "__main__":
    main()
