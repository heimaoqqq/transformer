#!/usr/bin/env python3
"""
VQ-VAE + Transformer ä¸»ç”Ÿæˆè„šæœ¬
ä»ç”¨æˆ·IDç”Ÿæˆå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from pathlib import Path
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# æ¡ä»¶å¯¼å…¥æ¨¡å‹ - æ£€æŸ¥ç¯å¢ƒå…¼å®¹æ€§
try:
    from models.vqvae_model import MicroDopplerVQVAE
    VQVAE_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥VQ-VAEæ¨¡å‹: {e}")
    print("   è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œç”Ÿæˆè„šæœ¬")
    VQVAE_AVAILABLE = False

try:
    from models.transformer_model import MicroDopplerTransformer
    TRANSFORMER_AVAILABLE = True
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥Transformeræ¨¡å‹: {e}")
    print("   è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œç”Ÿæˆè„šæœ¬")
    TRANSFORMER_AVAILABLE = False

# æ£€æŸ¥å¿…è¦ç»„ä»¶
if not (VQVAE_AVAILABLE and TRANSFORMER_AVAILABLE):
    print("âŒ ç”Ÿæˆè„šæœ¬éœ€è¦åŒæ—¶æ”¯æŒVQ-VAEå’ŒTransformer")
    print("   å»ºè®®åœ¨Transformerç¯å¢ƒä¸­è¿è¡Œï¼Œå› ä¸ºå®ƒå¯ä»¥åŠ è½½VQ-VAEæ¨¡å‹")
    sys.exit(1)

class VQVAETransformerGenerator:
    """VQ-VAE + Transformer ç”Ÿæˆå™¨"""
    
    def __init__(self, model_dir, device="auto"):
        self.model_dir = Path(model_dir)
        
        # è®¾ç½®è®¾å¤‡
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
        
        print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        self.vqvae_model = self._load_vqvae()
        self.transformer_model = self._load_transformer()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _load_vqvae(self):
        """åŠ è½½VQ-VAEæ¨¡å‹"""
        vqvae_path = self.model_dir / "vqvae"
        
        print(f"ğŸ“¦ åŠ è½½VQ-VAE: {vqvae_path}")
        
        checkpoint_path = vqvae_path / "best_model.pth"
        if not checkpoint_path.exists():
            checkpoint_path = vqvae_path / "final_model.pth"
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°VQ-VAEæ¨¡å‹: {vqvae_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        
        # é‡å»ºæ¨¡å‹
        model = MicroDopplerVQVAE(
            num_vq_embeddings=checkpoint['args'].codebook_size,
            commitment_cost=checkpoint['args'].commitment_cost,
            ema_decay=checkpoint['args'].ema_decay,
        )
        model.load_state_dict(checkpoint['model_state_dict'])
        
        model.to(self.device)
        model.eval()
        return model
    
    def _load_transformer(self):
        """åŠ è½½Transformeræ¨¡å‹"""
        transformer_path = self.model_dir / "transformer"
        
        print(f"ğŸ“¦ åŠ è½½Transformer: {transformer_path}")
        
        checkpoint_path = transformer_path / "best_model.pth"
        if not checkpoint_path.exists():
            checkpoint_path = transformer_path / "final_model.pth"
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°Transformeræ¨¡å‹: {transformer_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        args = checkpoint['args']
        
        # é‡å»ºæ¨¡å‹
        model = MicroDopplerTransformer(
            vocab_size=args.codebook_size,
            max_seq_len=getattr(args, 'max_seq_len', 256),
            num_users=args.num_users,
            n_embd=args.n_embd,
            n_layer=args.n_layer,
            n_head=args.n_head,
            dropout=args.dropout,
            use_cross_attention=args.use_cross_attention,
        )
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        return model
    
    def generate_for_user(
        self,
        user_id: int,
        num_samples: int = 5,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        diversity_boost: float = 1.2,
    ):
        """
        ä¸ºæŒ‡å®šç”¨æˆ·ç”Ÿæˆå›¾åƒ
        Args:
            user_id: ç”¨æˆ·ID (0-30)
            num_samples: ç”Ÿæˆæ ·æœ¬æ•°é‡
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            diversity_boost: å¤šæ ·æ€§å¢å¼ºå› å­
        Returns:
            generated_images: [num_samples, 3, H, W] ç”Ÿæˆçš„å›¾åƒ
        """
        print(f"ğŸ¨ ä¸ºç”¨æˆ· {user_id} ç”Ÿæˆ {num_samples} å¼ å›¾åƒ...")
        
        generated_images = []
        
        with torch.no_grad():
            for i in range(num_samples):
                # ä¸ºæ¯ä¸ªæ ·æœ¬ä½¿ç”¨ç•¥å¾®ä¸åŒçš„é‡‡æ ·å‚æ•°ä»¥å¢åŠ å¤šæ ·æ€§
                sample_temp = temperature * (1 + (i / num_samples - 0.5) * 0.2 * diversity_boost)
                sample_top_p = max(0.7, top_p - (i / num_samples) * 0.1 * diversity_boost)
                
                # ç”Ÿæˆtokenåºåˆ—
                user_ids = torch.tensor([user_id], device=self.device)
                
                generated_tokens = self.transformer_model.generate(
                    user_ids=user_ids,
                    max_length=self.transformer_model.max_seq_len,
                    temperature=sample_temp,
                    top_k=top_k,
                    top_p=sample_top_p,
                    do_sample=True,
                    num_return_sequences=1,
                )
                
                # è½¬æ¢ä¸ºå›¾åƒ
                image = self._tokens_to_image(generated_tokens[0])
                generated_images.append(image)
                
                print(f"  âœ… æ ·æœ¬ {i+1}/{num_samples} å®Œæˆ")
        
        return torch.stack(generated_images)
    
    def _tokens_to_image(self, tokens):
        """å°†tokenåºåˆ—è½¬æ¢ä¸ºå›¾åƒ"""
        # é‡å¡‘ä¸º2D
        seq_len = len(tokens)
        latent_size = int(np.sqrt(seq_len))
        
        if latent_size * latent_size != seq_len:
            # å¦‚æœä¸æ˜¯å®Œå…¨å¹³æ–¹æ•°ï¼Œæˆªæ–­æˆ–å¡«å……
            target_len = latent_size * latent_size
            if seq_len > target_len:
                tokens = tokens[:target_len]
            else:
                pad_tokens = torch.full((target_len - seq_len,), 0, device=tokens.device)
                tokens = torch.cat([tokens, pad_tokens])
        
        # é‡å¡‘ä¸º2D
        tokens_2d = tokens.view(1, latent_size, latent_size)
        
        # è·å–ç æœ¬åµŒå…¥
        with torch.no_grad():
            # ç¡®ä¿tokenç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            tokens_2d = torch.clamp(tokens_2d, 0, self.vqvae_model.quantize.n_embed - 1)
            
            # è·å–é‡åŒ–å‘é‡
            quantized_latents = self.vqvae_model.quantize.embedding(tokens_2d)
            quantized_latents = quantized_latents.permute(0, 3, 1, 2)  # [B, C, H, W]
            
            # è§£ç ä¸ºå›¾åƒ
            generated_image = self.vqvae_model.decode(quantized_latents, force_not_quantize=True)
            
            # å½’ä¸€åŒ–åˆ°[0,1]
            image = (generated_image.squeeze(0) * 0.5 + 0.5).clamp(0, 1)
            
            return image
    
    def generate_dataset(
        self,
        output_dir: str,
        samples_per_user: int = 10,
        user_list: list = None,
        **generation_kwargs
    ):
        """
        ç”Ÿæˆå®Œæ•´æ•°æ®é›†
        Args:
            output_dir: è¾“å‡ºç›®å½•
            samples_per_user: æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°
            user_list: ç”¨æˆ·åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰ç”¨æˆ·
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        if user_list is None:
            user_list = list(range(self.transformer_model.num_users))
        
        print(f"ğŸ¯ ç”Ÿæˆæ•°æ®é›†:")
        print(f"   è¾“å‡ºç›®å½•: {output_path}")
        print(f"   ç”¨æˆ·æ•°é‡: {len(user_list)}")
        print(f"   æ¯ç”¨æˆ·æ ·æœ¬æ•°: {samples_per_user}")
        print(f"   æ€»æ ·æœ¬æ•°: {len(user_list) * samples_per_user}")
        
        total_generated = 0
        
        for user_id in tqdm(user_list, desc="ç”Ÿæˆç”¨æˆ·æ•°æ®"):
            user_dir = output_path / f"user_{user_id:02d}"
            user_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå›¾åƒ
            generated_images = self.generate_for_user(
                user_id=user_id,
                num_samples=samples_per_user,
                **generation_kwargs
            )
            
            # ä¿å­˜å›¾åƒ
            for i, image in enumerate(generated_images):
                image_pil = transforms.ToPILImage()(image.cpu())
                save_path = user_dir / f"generated_{i:03d}.png"
                image_pil.save(save_path)
                total_generated += 1
            
            print(f"âœ… ç”¨æˆ· {user_id}: {samples_per_user} å¼ å›¾åƒä¿å­˜åˆ° {user_dir}")
        
        print(f"\nğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        print(f"   æ€»è®¡ç”Ÿæˆ: {total_generated} å¼ å›¾åƒ")
        print(f"   ä¿å­˜ä½ç½®: {output_path}")
        
        return output_path
    
    def visualize_generation(self, user_id: int, num_samples: int = 4, save_path: str = None):
        """å¯è§†åŒ–ç”Ÿæˆç»“æœ"""
        generated_images = self.generate_for_user(user_id, num_samples)
        
        fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 4, 4))
        if num_samples == 1:
            axes = [axes]
        
        for i, image in enumerate(generated_images):
            axes[i].imshow(image.cpu().permute(1, 2, 0))
            axes[i].set_title(f'User {user_id} - Sample {i+1}')
            axes[i].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ’¾ å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {save_path}")
        
        plt.show()

def main():
    parser = argparse.ArgumentParser(description="VQ-VAE + Transformer å›¾åƒç”Ÿæˆ")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_dir", type=str, required=True,
                       help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="generated_images",
                       help="è¾“å‡ºç›®å½•")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--user_id", type=int, default=None,
                       help="æŒ‡å®šç”¨æˆ·ID (ä¸æŒ‡å®šåˆ™ç”Ÿæˆæ‰€æœ‰ç”¨æˆ·)")
    parser.add_argument("--samples_per_user", type=int, default=10,
                       help="æ¯ä¸ªç”¨æˆ·ç”Ÿæˆçš„æ ·æœ¬æ•°")
    parser.add_argument("--temperature", type=float, default=1.0,
                       help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--top_k", type=int, default=50,
                       help="Top-ké‡‡æ ·")
    parser.add_argument("--top_p", type=float, default=0.9,
                       help="Top-pé‡‡æ ·")
    parser.add_argument("--diversity_boost", type=float, default=1.2,
                       help="å¤šæ ·æ€§å¢å¼ºå› å­")
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument("--visualize_only", action="store_true",
                       help="åªå¯è§†åŒ–ï¼Œä¸ä¿å­˜æ•°æ®é›†")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    print("ğŸ¨ VQ-VAE + Transformer å›¾åƒç”Ÿæˆå™¨")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = VQVAETransformerGenerator(args.model_dir, args.device)
    
    if args.visualize_only:
        # åªå¯è§†åŒ–
        user_id = args.user_id if args.user_id is not None else 0
        save_path = f"visualization_user_{user_id}.png"
        generator.visualize_generation(user_id, 4, save_path)
    else:
        # ç”Ÿæˆæ•°æ®é›†
        user_list = [args.user_id] if args.user_id is not None else None
        
        output_path = generator.generate_dataset(
            output_dir=args.output_dir,
            samples_per_user=args.samples_per_user,
            user_list=user_list,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
            diversity_boost=args.diversity_boost,
        )
        
        print(f"\nğŸ” ä¸‹ä¸€æ­¥: è¿è¡ŒéªŒè¯")
        print(f"   python validate_main.py \\")
        print(f"     --model_dir {args.model_dir} \\")
        print(f"     --real_data_dir /path/to/real/data \\")
        print(f"     --generated_data_dir {output_path} \\")
        print(f"     --target_user_id {args.user_id or 0}")

if __name__ == "__main__":
    main()
