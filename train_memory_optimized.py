#!/usr/bin/env python3
"""
å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒå¯åŠ¨å™¨
ä¸“é—¨é’ˆå¯¹KaggleåŒGPU T4ç¯å¢ƒçš„å†…å­˜é™åˆ¶
"""

import os
import sys
import subprocess
import torch
from pathlib import Path

def setup_memory_environment():
    """è®¾ç½®å†…å­˜ä¼˜åŒ–ç¯å¢ƒ"""
    # è®¾ç½®PyTorchå†…å­˜ç®¡ç†
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # è®¾ç½®æ— ç¼“å†²è¾“å‡º
    os.environ['PYTHONUNBUFFERED'] = '1'
    
    # å¯ç”¨å†…å­˜æ˜ å°„
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

def get_optimal_config():
    """æ ¹æ®GPUå†…å­˜è·å–æœ€ä¼˜é…ç½®"""

    gpu_count = torch.cuda.device_count()

    if gpu_count >= 2:
        # åŒGPUé…ç½® - 256åˆ†è¾¨ç‡ä¼˜åŒ–
        return {
            "batch_size": 6,      # æ¯GPU 3ä¸ªæ ·æœ¬ï¼Œæ€»å…±6ä¸ª
            "resolution": 256,    # æ¢å¤256åˆ†è¾¨ç‡
            "num_workers": 2,     # å¤šçº¿ç¨‹
            "gradient_accumulation_steps": 2,  # æ­£å¸¸æ¢¯åº¦ç´¯ç§¯
        }
    else:
        # å•GPUé…ç½®
        return {
            "batch_size": 4,      # å•GPUæ›´å¤§æ‰¹æ¬¡
            "resolution": 256,    # 256åˆ†è¾¨ç‡
            "num_workers": 2,     # å¤šçº¿ç¨‹
            "gradient_accumulation_steps": 4, # é€‚ä¸­æ¢¯åº¦ç´¯ç§¯
        }

def launch_optimized_training():
    """å¯åŠ¨å†…å­˜ä¼˜åŒ–è®­ç»ƒ"""
    
    setup_memory_environment()
    
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        for i in range(gpu_count):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()
    
    config = get_optimal_config()
    print(f"ğŸ“Š ä½¿ç”¨é…ç½®: {config}")
    
    if gpu_count > 1:
        print("ğŸš€ å¯åŠ¨åŒGPUå†…å­˜ä¼˜åŒ–è®­ç»ƒ...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
        os.environ['WORLD_SIZE'] = str(gpu_count)
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12355'
        
        cmd = [
            "accelerate", "launch",
            "--config_file", "accelerate_config.yaml",
            "--num_processes", str(gpu_count),
            "training/train_vae.py",
            "--data_dir", "/kaggle/input/dataset",
            "--output_dir", "/kaggle/working/outputs/vae",
            "--batch_size", str(config["batch_size"]),
            "--num_epochs", "40",  # æ¢å¤æ­£å¸¸epochæ•°
            "--learning_rate", "0.0001",
            "--mixed_precision", "fp16",
            "--gradient_accumulation_steps", str(config["gradient_accumulation_steps"]),
            "--kl_weight", "1e-6",
            "--perceptual_weight", "0.0",  # ç¦ç”¨æ„ŸçŸ¥æŸå¤±
            "--freq_weight", "0.05",
            "--resolution", str(config["resolution"]),
            "--num_workers", str(config["num_workers"]),
            "--save_interval", "10",
            "--log_interval", "5",
            "--sample_interval", "100",  # æ¢å¤æ­£å¸¸é‡‡æ ·
            "--experiment_name", "kaggle_vae_optimized"
        ]
    else:
        print("ğŸš€ å¯åŠ¨å•GPUå†…å­˜ä¼˜åŒ–è®­ç»ƒ...")
        
        cmd = [
            "python", "-u",
            "training/train_vae.py",
            "--data_dir", "/kaggle/input/dataset",
            "--output_dir", "/kaggle/working/outputs/vae",
            "--batch_size", str(config["batch_size"]),
            "--num_epochs", "40",
            "--learning_rate", "0.0001",
            "--mixed_precision", "fp16",
            "--gradient_accumulation_steps", str(config["gradient_accumulation_steps"]),
            "--kl_weight", "1e-6",
            "--perceptual_weight", "0.0",
            "--freq_weight", "0.05",
            "--resolution", str(config["resolution"]),
            "--num_workers", str(config["num_workers"]),
            "--save_interval", "10",
            "--log_interval", "5",
            "--sample_interval", "100",
            "--experiment_name", "kaggle_vae_optimized"
        ]
    
    print(f"Command: {' '.join(cmd)}")
    print("=" * 80)
    
    try:
        # å¯åŠ¨è®­ç»ƒ
        process = subprocess.Popen(
            cmd,
            stdout=None,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=0
        )
        
        return_code = process.wait()
        
        if return_code == 0:
            print("\nâœ… å†…å­˜ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
            return True
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥ (é€€å‡ºç : {return_code})")
            return False
            
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¯åŠ¨å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Kaggleå†…å­˜ä¼˜åŒ–VAEè®­ç»ƒ")
    print("=" * 50)
    
    # æ˜¾ç¤ºå†…å­˜ä¿¡æ¯
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"GPU {i}: {props.name} - {props.total_memory / 1024**3:.1f} GB")
    
    success = launch_optimized_training()
    
    if success:
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        print("ğŸ’¡ å»ºè®®: å°è¯•æ›´å°çš„æ‰¹æ¬¡å¤§å°æˆ–åˆ†è¾¨ç‡")
        sys.exit(1)

if __name__ == "__main__":
    main()
