#!/usr/bin/env python3
"""
è°ƒè¯•UNetå†…éƒ¨ç»´åº¦é—®é¢˜
æ£€æŸ¥UNetå†…éƒ¨å„å±‚çš„ç»´åº¦é…ç½®
"""

import torch
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

def debug_unet_internal():
    """è°ƒè¯•UNetå†…éƒ¨é…ç½®"""
    print("ğŸ” è°ƒè¯•UNetå†…éƒ¨é…ç½®...")
    
    # æ¨¡å‹è·¯å¾„
    unet_path = "/kaggle/input/diffusion-final-model"
    
    try:
        from diffusers import UNet2DConditionModel
        
        # åŠ è½½UNet
        print(f"ä»è·¯å¾„åŠ è½½UNet: {unet_path}")
        unet = UNet2DConditionModel.from_pretrained(unet_path)
        
        print(f"\nğŸ“‹ UNetå®Œæ•´é…ç½®:")
        config = unet.config
        for key, value in config.items():
            print(f"  - {key}: {value}")
        
        print(f"\nğŸ” å…³é”®ç»´åº¦åˆ†æ:")
        print(f"  - cross_attention_dim: {config.cross_attention_dim}")
        print(f"  - attention_head_dim: {config.attention_head_dim}")
        print(f"  - block_out_channels: {config.block_out_channels}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ä¸€è‡´çš„é…ç½®
        if hasattr(config, 'attention_head_dim'):
            if isinstance(config.attention_head_dim, (list, tuple)):
                print(f"  - attention_head_dimæ˜¯åˆ—è¡¨: {config.attention_head_dim}")
            else:
                print(f"  - attention_head_dimæ˜¯æ ‡é‡: {config.attention_head_dim}")
                
                # è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
                if config.cross_attention_dim % config.attention_head_dim != 0:
                    print(f"  âš ï¸  cross_attention_dim ({config.cross_attention_dim}) ä¸èƒ½è¢« attention_head_dim ({config.attention_head_dim}) æ•´é™¤!")
                else:
                    num_heads = config.cross_attention_dim // config.attention_head_dim
                    print(f"  âœ… æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½å¯¼è‡´1024ç»´åº¦çš„é…ç½®
        suspicious_configs = []
        for key, value in config.items():
            if isinstance(value, (int, list, tuple)):
                if (isinstance(value, int) and value == 1024) or \
                   (isinstance(value, (list, tuple)) and 1024 in value):
                    suspicious_configs.append((key, value))
        
        if suspicious_configs:
            print(f"\nğŸš¨ å‘ç°å¯èƒ½å¯¼è‡´1024ç»´åº¦çš„é…ç½®:")
            for key, value in suspicious_configs:
                print(f"  - {key}: {value}")
        else:
            print(f"\nâœ… æ²¡æœ‰å‘ç°1024ç»´åº¦çš„é…ç½®")
        
        return config
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        return None

def test_attention_dimensions():
    """æµ‹è¯•æ³¨æ„åŠ›å±‚çš„ç»´åº¦è®¡ç®—"""
    print(f"\nğŸ§ª æµ‹è¯•æ³¨æ„åŠ›å±‚ç»´åº¦è®¡ç®—...")
    
    try:
        # æ¨¡æ‹Ÿä¸åŒçš„é…ç½®
        configs_to_test = [
            {"cross_attention_dim": 512, "attention_head_dim": 8},
            {"cross_attention_dim": 512, "attention_head_dim": 64},
            {"cross_attention_dim": 1024, "attention_head_dim": 8},
            {"cross_attention_dim": 1024, "attention_head_dim": 64},
        ]
        
        for config in configs_to_test:
            cross_dim = config["cross_attention_dim"]
            head_dim = config["attention_head_dim"]
            
            if cross_dim % head_dim == 0:
                num_heads = cross_dim // head_dim
                print(f"  âœ… cross_dim={cross_dim}, head_dim={head_dim} -> {num_heads} heads")
            else:
                print(f"  âŒ cross_dim={cross_dim}, head_dim={head_dim} -> ä¸å…¼å®¹!")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def check_model_state_dict():
    """æ£€æŸ¥æ¨¡å‹çŠ¶æ€å­—å…¸ä¸­çš„ç»´åº¦"""
    print(f"\nğŸ” æ£€æŸ¥æ¨¡å‹æƒé‡ä¸­çš„ç»´åº¦...")
    
    unet_path = "/kaggle/input/diffusion-final-model"
    
    try:
        from diffusers import UNet2DConditionModel
        
        # åŠ è½½UNet
        unet = UNet2DConditionModel.from_pretrained(unet_path)
        
        # æ£€æŸ¥çŠ¶æ€å­—å…¸ä¸­çš„æƒé‡ç»´åº¦
        suspicious_weights = []
        
        for name, param in unet.named_parameters():
            if any(dim == 1024 for dim in param.shape):
                suspicious_weights.append((name, param.shape))
        
        if suspicious_weights:
            print(f"ğŸš¨ å‘ç°åŒ…å«1024ç»´åº¦çš„æƒé‡:")
            for name, shape in suspicious_weights[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {name}: {shape}")
            if len(suspicious_weights) > 10:
                print(f"  ... è¿˜æœ‰ {len(suspicious_weights) - 10} ä¸ªæƒé‡")
        else:
            print(f"âœ… æ²¡æœ‰å‘ç°1024ç»´åº¦çš„æƒé‡")
        
        # ç‰¹åˆ«æ£€æŸ¥äº¤å‰æ³¨æ„åŠ›ç›¸å…³çš„æƒé‡
        cross_attn_weights = []
        for name, param in unet.named_parameters():
            if "cross_attn" in name.lower() or "encoder_hidden_states" in name.lower():
                cross_attn_weights.append((name, param.shape))
        
        if cross_attn_weights:
            print(f"\nğŸ” äº¤å‰æ³¨æ„åŠ›ç›¸å…³æƒé‡:")
            for name, shape in cross_attn_weights[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {name}: {shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False

def create_minimal_test():
    """åˆ›å»ºæœ€å°åŒ–æµ‹è¯•æ¥å¤ç°é—®é¢˜"""
    print(f"\nğŸ§ª åˆ›å»ºæœ€å°åŒ–æµ‹è¯•...")
    
    try:
        from diffusers import UNet2DConditionModel
        from training.train_diffusion import UserConditionEncoder
        
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é…ç½®åˆ›å»ºUNet
        print("åˆ›å»ºUNet (ä½¿ç”¨è®­ç»ƒæ—¶é…ç½®)...")
        unet = UNet2DConditionModel(
            sample_size=32,
            in_channels=4,
            out_channels=4,
            layers_per_block=2,
            block_out_channels=(128, 256, 512, 512),
            down_block_types=(
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "CrossAttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
                "CrossAttnUpBlock2D",
            ),
            cross_attention_dim=512,
            attention_head_dim=8,
            use_linear_projection=True,
        )
        
        print("åˆ›å»ºæ¡ä»¶ç¼–ç å™¨...")
        condition_encoder = UserConditionEncoder(
            num_users=31,
            embed_dim=512
        )
        
        print("æµ‹è¯•å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            latents = torch.randn(1, 4, 32, 32)
            timesteps = torch.tensor([100])
            user_tensor = torch.tensor([0])
            
            # ç¼–ç æ¡ä»¶
            encoder_hidden_states = condition_encoder(user_tensor)
            encoder_hidden_states = encoder_hidden_states.unsqueeze(1)  # [1, 1, 512]
            
            print(f"è¾“å…¥å½¢çŠ¶:")
            print(f"  - latents: {latents.shape}")
            print(f"  - timesteps: {timesteps.shape}")
            print(f"  - encoder_hidden_states: {encoder_hidden_states.shape}")
            
            # æµ‹è¯•UNetå‰å‘ä¼ æ’­
            try:
                noise_pred = unet(
                    latents,
                    timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    return_dict=False
                )[0]
                
                print(f"âœ… æµ‹è¯•æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {noise_pred.shape}")
                return True
                
            except Exception as e:
                print(f"âŒ UNetå‰å‘ä¼ æ’­å¤±è´¥: {e}")
                return False
        
    except Exception as e:
        print(f"âŒ æœ€å°åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” UNetå†…éƒ¨ç»´åº¦è°ƒè¯•å·¥å…·")
    print("=" * 50)
    
    # è°ƒè¯•UNeté…ç½®
    config = debug_unet_internal()
    
    # æµ‹è¯•æ³¨æ„åŠ›ç»´åº¦
    test_attention_dimensions()
    
    # æ£€æŸ¥æ¨¡å‹æƒé‡
    check_model_state_dict()
    
    # æœ€å°åŒ–æµ‹è¯•
    create_minimal_test()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ è°ƒè¯•å®Œæˆ!")
    
    if config:
        print("\nğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥UNetçš„attention_head_dimé…ç½®")
        print("2. ç¡®è®¤cross_attention_dimä¸attention_head_dimçš„å…¼å®¹æ€§")
        print("3. æ£€æŸ¥æ˜¯å¦æœ‰ç¡¬ç¼–ç çš„1024ç»´åº¦")
        print("4. è€ƒè™‘é‡æ–°è®­ç»ƒUNetä½¿ç”¨æ­£ç¡®çš„é…ç½®")

if __name__ == "__main__":
    main()
