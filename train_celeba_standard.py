#!/usr/bin/env python3
"""
CelebAæ ‡å‡†å¾®å¤šæ™®å‹’VAEè®­ç»ƒå™¨
é‡‡ç”¨64Ã—64åˆ†è¾¨ç‡ï¼Œéµå¾ªæˆç†Ÿé¡¹ç›®çš„æ ‡å‡†åšæ³•
"""

import os
import sys
import subprocess
import torch
from pathlib import Path

def setup_celeba_environment():
    """è®¾ç½®CelebAæ ‡å‡†ç¯å¢ƒ"""
    # å¼ºåˆ¶å•GPU (CelebAæ ‡å‡†é€šå¸¸å•GPUè®­ç»ƒ)
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    # ä¼˜åŒ–å†…å­˜åˆ†é…
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'
    os.environ['PYTHONUNBUFFERED'] = '1'
    
    # åŸºç¡€ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True

def get_celeba_standard_config():
    """è·å–CelebAæ ‡å‡†é…ç½®"""
    
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦CUDAæ”¯æŒ")
        return None
        
    gpu_name = torch.cuda.get_device_properties(0).name
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    print(f"ğŸ® GPU: {gpu_name}")
    print(f"ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f} GB")
    
    # CelebAæ ‡å‡†é…ç½® - åŸºäºGPUç±»å‹ä¼˜åŒ–
    if "P100" in gpu_name or gpu_memory > 14:
        # é«˜ç«¯GPUé…ç½®
        return {
            "batch_size": 32,           # 64Ã—64å¯ä»¥ç”¨å¤§æ‰¹æ¬¡
            "mixed_precision": "no",    # P100ç”¨FP32
            "learning_rate": "0.0002",  # CelebAæ ‡å‡†å­¦ä¹ ç‡
            "gradient_accumulation": 1, # ä¸éœ€è¦ç´¯ç§¯
            "num_workers": 4,           # å¤šçº¿ç¨‹
        }
    elif "T4" in gpu_name or gpu_memory > 10:
        # ä¸­ç«¯GPUé…ç½®
        return {
            "batch_size": 16,           # T4ä¹Ÿèƒ½ç”¨è¾ƒå¤§æ‰¹æ¬¡
            "mixed_precision": "fp16",  # T4ç”¨FP16
            "learning_rate": "0.0002",  
            "gradient_accumulation": 2, 
            "num_workers": 2,           
        }
    else:
        # ä½ç«¯GPUé…ç½®
        return {
            "batch_size": 8,            # ä¿å®ˆæ‰¹æ¬¡
            "mixed_precision": "fp16",  
            "learning_rate": "0.0001",  # ç¨ä½å­¦ä¹ ç‡
            "gradient_accumulation": 4, 
            "num_workers": 1,           
        }

def launch_celeba_training():
    """å¯åŠ¨CelebAæ ‡å‡†è®­ç»ƒ"""
    
    setup_celeba_environment()
    
    # è·å–é…ç½®
    config = get_celeba_standard_config()
    if config is None:
        return False
    
    # æ¸…ç†GPUç¼“å­˜
    torch.cuda.empty_cache()
    
    print(f"\nğŸ¨ å¯åŠ¨CelebAæ ‡å‡†è®­ç»ƒ...")
    print("=" * 60)
    
    cmd = [
        "python", "-u",
        "training/train_vae.py",
        "--data_dir", "/kaggle/input/dataset",
        "--output_dir", "/kaggle/working/outputs/vae_celeba_standard",
        "--batch_size", str(config["batch_size"]),
        "--num_epochs", "50",  # å¢åŠ è®­ç»ƒè½®æ•°ä»¥æå‡è´¨é‡
        "--learning_rate", config["learning_rate"],
        "--mixed_precision", config["mixed_precision"],
        "--gradient_accumulation_steps", str(config["gradient_accumulation"]),
        "--kl_weight", "1e-4",  # Stable Diffusionæ ‡å‡†KLæƒé‡
        "--perceptual_weight", "1.0",  # å¯ç”¨æ„ŸçŸ¥æŸå¤± (Stable Diffusionæ ‡å‡†)
        "--freq_weight", "0.1",  # å¾®å¤šæ™®å‹’ç‰¹æœ‰ï¼Œå¢å¼ºé¢‘åŸŸä¿æŒ
        "--resolution", "64",  # CelebAæ ‡å‡†åˆ†è¾¨ç‡
        "--num_workers", str(config["num_workers"]),
        "--save_interval", "5",
        "--log_interval", "2",
        "--sample_interval", "50",
        "--experiment_name", "micro_doppler_celeba_standard"
    ]
    
    print(f"ğŸ“Š CelebAæ ‡å‡†é…ç½®:")
    print(f"   ğŸ“ åˆ†è¾¨ç‡: 64Ã—64 (CelebAæ ‡å‡†)")
    print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"   ğŸ”¢ æ··åˆç²¾åº¦: {config['mixed_precision']}")
    print(f"   ğŸ“ˆ å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"   âš¡ æ¢¯åº¦ç´¯ç§¯: {config['gradient_accumulation']}")
    print(f"   ğŸ§µ æ•°æ®çº¿ç¨‹: {config['num_workers']}")
    
    # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
    effective_batch = config["batch_size"] * config["gradient_accumulation"]
    print(f"   ğŸ¯ æœ‰æ•ˆæ‰¹æ¬¡: {effective_batch}")
    
    print(f"\nğŸ—ï¸  CelebAæ ‡å‡†æ¶æ„:")
    print(f"   ğŸ“ è¾“å…¥: 64Ã—64Ã—3")
    print(f"   ğŸ”½ ä¸‹é‡‡æ ·: 64â†’32â†’16â†’8 (3å±‚)")
    print(f"   ğŸ“Š é€šé“æ•°: [64, 128, 256]")
    print(f"   ğŸ§± æ¯å±‚å—æ•°: 1 (CelebAæ ‡å‡†)")
    print(f"   ğŸ¯ æ½œåœ¨ç©ºé—´: 8Ã—8Ã—4")
    print(f"   ğŸ“Š å‹ç¼©æ¯”: 48:1")
    
    print(f"\nğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡:")
    print(f"   ğŸ’¾ æ˜¾å­˜ä½¿ç”¨: ~3-4GB (vs 15GB)")
    print(f"   â±ï¸  è®­ç»ƒæ—¶é—´: ~7-10åˆ†é’Ÿ/è½® (vs 30åˆ†é’Ÿ)")
    print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config['batch_size']} (vs 4)")
    print(f"   ğŸš€ æ€»ä½“æå‡: 4-5å€")
    
    print(f"\nğŸ’¡ CelebAæ ‡å‡†ä¼˜åŠ¿:")
    print(f"   âœ… éµå¾ªæˆç†Ÿé¡¹ç›®æ ‡å‡†åšæ³•")
    print(f"   âœ… å¤§å¹…å‡å°‘æ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´")
    print(f"   âœ… æ›´å¤§æ‰¹æ¬¡è®­ç»ƒæ›´ç¨³å®š")
    print(f"   âœ… ä¿æŒç›¸åŒå‹ç¼©æ¯”å’Œç‰¹å¾æå–èƒ½åŠ›")
    print(f"   âœ… é€‚åˆå¾®å¤šæ™®å‹’çš„ç»“æ„åŒ–ç‰¹å¾")
    
    print(f"\nCommand: {' '.join(cmd)}")
    print("=" * 60)
    
    try:
        # å¯åŠ¨è®­ç»ƒ
        process = subprocess.Popen(
            cmd,
            stdout=None,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=0
        )
        
        return_code = process.wait()
        
        if return_code == 0:
            print(f"\nâœ… CelebAæ ‡å‡†è®­ç»ƒå®Œæˆ!")
            return True
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥ (é€€å‡ºç : {return_code})")
            return False
            
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        process.terminate()
        return False
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¯åŠ¨å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ CelebAæ ‡å‡†å¾®å¤šæ™®å‹’VAEè®­ç»ƒ")
    print("=" * 50)
    
    print("ğŸ¯ CelebAæ ‡å‡†ç­–ç•¥:")
    print("   âœ… 64Ã—64åˆ†è¾¨ç‡ (æˆç†Ÿé¡¹ç›®æ ‡å‡†)")
    print("   âœ… 3å±‚ä¸‹é‡‡æ · (64â†’32â†’16â†’8)")
    print("   âœ… è½»é‡é€šé“é…ç½® [64,128,256]")
    print("   âœ… æ¯å±‚1ä¸ªResNetå—")
    print("   âœ… å¤§æ‰¹æ¬¡è®­ç»ƒ (8-32)")
    
    print("\nğŸ“Š ä¸ä¹‹å‰256Ã—256å¯¹æ¯”:")
    print("   ğŸ“‰ æ˜¾å­˜ä½¿ç”¨: 15GB â†’ 3GB (5å€å‡å°‘)")
    print("   ğŸ“‰ è®­ç»ƒæ—¶é—´: 30åˆ†é’Ÿ â†’ 7åˆ†é’Ÿ (4å€åŠ é€Ÿ)")
    print("   ğŸ“ˆ æ‰¹æ¬¡å¤§å°: 4 â†’ 16-32 (4-8å€å¢å¤§)")
    print("   ğŸ“ˆ è®­ç»ƒç¨³å®šæ€§: å¤§å¹…æå‡")
    
    print("\nğŸ”¬ ä¸ºä»€ä¹ˆ64Ã—64é€‚åˆå¾®å¤šæ™®å‹’:")
    print("   ğŸ”„ æ—¶é¢‘å›¾æœ‰æ˜æ˜¾çš„å‘¨æœŸæ€§æ¨¡å¼")
    print("   ğŸ“Š ä¸»è¦ä¿¡æ¯é›†ä¸­åœ¨ä½é¢‘æˆåˆ†")
    print("   ğŸ¯ ä¸éœ€è¦åƒè‡ªç„¶å›¾åƒçš„åƒç´ çº§ç»†èŠ‚")
    print("   âš¡ å¿«é€Ÿè¿­ä»£æ¯”æè‡´ç»†èŠ‚æ›´é‡è¦")
    
    print("\nğŸ“š éµå¾ªçš„æ ‡å‡†:")
    print("   ğŸ† Efficient-VDVAE: CelebA 64Ã—64")
    print("   ğŸ† å¤§å¤šæ•°VAEè®ºæ–‡: CelebA 64Ã—64")
    print("   ğŸ† Huggingface Diffusers: å…ˆç¼©æ”¾å†ä¸‹é‡‡æ ·")
    print("   ğŸ† æˆç†Ÿé¡¹ç›®é€šç”¨åšæ³•")
    
    success = launch_celeba_training()
    
    if success:
        print("\nğŸ‰ CelebAæ ‡å‡†è®­ç»ƒå®Œæˆ!")
        print("ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: /kaggle/working/outputs/vae_celeba_standard/final_model")
        print("ğŸ’¡ å¦‚æœæ•ˆæœæ»¡æ„ï¼Œè¿™å°±æ˜¯æœ€ä½³é…ç½®!")
        print("ğŸ’¡ å¦‚æœéœ€è¦æ›´é«˜åˆ†è¾¨ç‡ï¼Œå¯ä»¥åç»­ç”¨æ­¤æ¨¡å‹ä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥!")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   - GPUå†…å­˜æ˜¯å¦è¢«å…¶ä»–è¿›ç¨‹å ç”¨")
        print("   - æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   - CUDAç¯å¢ƒæ˜¯å¦æ­£å¸¸")
        sys.exit(1)

if __name__ == "__main__":
    main()
