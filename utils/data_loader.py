#!/usr/bin/env python3
"""
æ•°æ®åŠ è½½å™¨
å¤ç”¨ä¸»é¡¹ç›®çš„æ•°æ®åŠ è½½é€»è¾‘ï¼Œé€‚é…VQ-VAE + Transformeréœ€æ±‚
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import torch
from torch.utils.data import Dataset
from PIL import Image
import torchvision.transforms as transforms

# æ·»åŠ ä¸»é¡¹ç›®è·¯å¾„ä»¥å¤ç”¨æ•°æ®åŠ è½½å™¨
sys.path.append(str(Path(__file__).parent.parent.parent))

try:
    from utils.data_loader import MicroDopplerDataset as BaseMicroDopplerDataset
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œæä¾›åŸºç¡€å®ç°
    class BaseMicroDopplerDataset(Dataset):
        def __init__(self, data_dir, transform=None, return_user_id=False):
            self.data_dir = Path(data_dir)
            self.transform = transform
            self.return_user_id = return_user_id
            
            # åŠ è½½æ•°æ®
            self.data = []
            self._load_data()
        
        def _load_data(self):
            """åŠ è½½æ•°æ®"""
            for user_dir in self.data_dir.iterdir():
                if user_dir.is_dir() and user_dir.name.startswith('ID'):
                    try:
                        # å¤„ç† ID1, ID_2, ID_3 ç­‰æ ¼å¼
                        dir_name = user_dir.name
                        if '_' in dir_name:
                            user_id = int(dir_name.split('_')[1])  # ID_2 -> 2
                        else:
                            user_id = int(dir_name[2:])  # ID1 -> 1

                        # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶
                        for ext in ['*.png', '*.jpg', '*.jpeg']:
                            for img_path in user_dir.glob(ext):
                                self.data.append({
                                    'image_path': str(img_path),
                                    'user_id': user_id,
                                })
                    except ValueError:
                        continue
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            item = self.data[idx]
            
            # åŠ è½½å›¾åƒ
            image = Image.open(item['image_path']).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            if self.return_user_id:
                return image, item['user_id']
            else:
                return image

class MicroDopplerDataset(BaseMicroDopplerDataset):
    """
    å¾®å¤šæ™®å‹’æ•°æ®é›†
    æ‰©å±•åŸºç¡€æ•°æ®é›†ä»¥æ”¯æŒVQ-VAE + Transformerçš„éœ€æ±‚
    """
    
    def __init__(
        self,
        data_dir: str,
        transform: Optional[transforms.Compose] = None,
        return_user_id: bool = False,
        user_filter: Optional[List[int]] = None,
        max_samples_per_user: Optional[int] = None,
    ):
        """
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            transform: å›¾åƒå˜æ¢
            return_user_id: æ˜¯å¦è¿”å›ç”¨æˆ·ID
            user_filter: ç”¨æˆ·IDè¿‡æ»¤åˆ—è¡¨
            max_samples_per_user: æ¯ä¸ªç”¨æˆ·çš„æœ€å¤§æ ·æœ¬æ•°
        """
        self.user_filter = user_filter
        self.max_samples_per_user = max_samples_per_user
        
        super().__init__(data_dir, transform, return_user_id)
        
        # åº”ç”¨è¿‡æ»¤å™¨
        if self.user_filter is not None:
            self.data = [item for item in self.data if item['user_id'] in self.user_filter]
        
        # é™åˆ¶æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬æ•°
        if self.max_samples_per_user is not None:
            self._limit_samples_per_user()
        
        print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.data)}")
        print(f"   ç”¨æˆ·æ•°é‡: {len(set(item['user_id'] for item in self.data))}")
    
    def _limit_samples_per_user(self):
        """é™åˆ¶æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬æ•°"""
        user_counts = {}
        filtered_data = []
        
        for item in self.data:
            user_id = item['user_id']
            if user_id not in user_counts:
                user_counts[user_id] = 0
            
            if user_counts[user_id] < self.max_samples_per_user:
                filtered_data.append(item)
                user_counts[user_id] += 1
        
        self.data = filtered_data
    
    def get_user_statistics(self) -> Dict[int, int]:
        """è·å–ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯"""
        user_counts = {}
        for item in self.data:
            user_id = item['user_id']
            user_counts[user_id] = user_counts.get(user_id, 0) + 1
        return user_counts
    
    def get_user_samples(self, user_id: int) -> List[str]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰æ ·æœ¬è·¯å¾„"""
        return [item['image_path'] for item in self.data if item['user_id'] == user_id]

def create_user_data_dict(data_dir: str) -> Dict[int, List[str]]:
    """
    åˆ›å»ºç”¨æˆ·æ•°æ®å­—å…¸
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
    Returns:
        user_data: {user_id: [image_paths]}
    """
    data_path = Path(data_dir)
    user_data = {}
    
    for user_dir in data_path.iterdir():
        if user_dir.is_dir() and user_dir.name.startswith('ID'):
            try:
                # å¤„ç† ID1, ID_2, ID_3 ç­‰æ ¼å¼
                dir_name = user_dir.name
                if '_' in dir_name:
                    user_id = int(dir_name.split('_')[1])  # ID_2 -> 2
                else:
                    user_id = int(dir_name[2:])  # ID1 -> 1

                image_paths = []

                # æ”¶é›†æ‰€æœ‰å›¾åƒæ–‡ä»¶
                for ext in ['*.png', '*.jpg', '*.jpeg']:
                    image_paths.extend(list(user_dir.glob(ext)))

                if image_paths:
                    user_data[user_id] = [str(p) for p in image_paths]

            except ValueError:
                continue
    
    return user_data

def create_balanced_dataset(
    data_dir: str,
    samples_per_user: int = 100,
    transform: Optional[transforms.Compose] = None,
) -> MicroDopplerDataset:
    """
    åˆ›å»ºå¹³è¡¡æ•°æ®é›†
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        samples_per_user: æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬æ•°
        transform: å›¾åƒå˜æ¢
    Returns:
        dataset: å¹³è¡¡çš„æ•°æ®é›†
    """
    return MicroDopplerDataset(
        data_dir=data_dir,
        transform=transform,
        return_user_id=True,
        max_samples_per_user=samples_per_user,
    )

def get_default_transform(resolution: int = 128, normalize: bool = True) -> transforms.Compose:
    """
    è·å–é»˜è®¤çš„å›¾åƒå˜æ¢
    Args:
        resolution: ç›®æ ‡åˆ†è¾¨ç‡ (ä»256x256ç¼©æ”¾åˆ°æŒ‡å®šå°ºå¯¸)
        normalize: æ˜¯å¦å½’ä¸€åŒ–åˆ°[-1, 1]
    Returns:
        transform: å›¾åƒå˜æ¢
    """
    transform_list = [
        transforms.Resize((resolution, resolution), interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.ToTensor(),  # [0, 1]
    ]

    if normalize:
        # å½’ä¸€åŒ–åˆ°[-1, 1]ï¼Œé€‚é…VQ-VAEè®­ç»ƒ
        transform_list.append(
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        )

    return transforms.Compose(transform_list)

def denormalize_tensor(tensor: torch.Tensor) -> torch.Tensor:
    """
    åå½’ä¸€åŒ–tensorä»[-1, 1]åˆ°[0, 1]
    Args:
        tensor: å½’ä¸€åŒ–çš„tensor [-1, 1]
    Returns:
        tensor: åå½’ä¸€åŒ–çš„tensor [0, 1]
    """
    return (tensor + 1.0) / 2.0

def tensor_to_pil(tensor: torch.Tensor) -> Image.Image:
    """
    å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ
    Args:
        tensor: [C, H, W] tensorï¼ŒèŒƒå›´[-1, 1]æˆ–[0, 1]
    Returns:
        image: PILå›¾åƒ
    """
    # å¦‚æœæ˜¯[-1, 1]èŒƒå›´ï¼Œå…ˆåå½’ä¸€åŒ–
    if tensor.min() < 0:
        tensor = denormalize_tensor(tensor)

    # è½¬æ¢ä¸ºPIL
    tensor = torch.clamp(tensor, 0, 1)
    tensor = (tensor * 255).byte()

    if tensor.dim() == 3:
        tensor = tensor.permute(1, 2, 0)  # [C, H, W] -> [H, W, C]

    return Image.fromarray(tensor.cpu().numpy())

class VQTokenDataset(Dataset):
    """
    VQ Tokenæ•°æ®é›†
    ç”¨äºTransformerè®­ç»ƒï¼ŒåŒ…å«é¢„è®¡ç®—çš„VQ tokens
    """
    
    def __init__(
        self,
        token_data: List[Dict],
        max_seq_len: int = 256,
        pad_token_id: int = 1024,
    ):
        """
        Args:
            token_data: tokenæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«user_idå’Œtokens
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
            pad_token_id: å¡«å……token ID
        """
        self.token_data = token_data
        self.max_seq_len = max_seq_len
        self.pad_token_id = pad_token_id
        
        print(f"ğŸ“Š VQ Tokenæ•°æ®é›†:")
        print(f"   æ€»åºåˆ—æ•°: {len(self.token_data)}")
        print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {self.max_seq_len}")
    
    def __len__(self):
        return len(self.token_data)
    
    def __getitem__(self, idx):
        item = self.token_data[idx]
        
        user_id = torch.tensor(item['user_id'], dtype=torch.long)
        tokens = item['tokens']
        
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        if len(tokens) > self.max_seq_len:
            tokens = tokens[:self.max_seq_len]
        else:
            pad_length = self.max_seq_len - len(tokens)
            pad_tokens = torch.full((pad_length,), self.pad_token_id, dtype=tokens.dtype)
            tokens = torch.cat([tokens, pad_tokens])
        
        return {
            'user_id': user_id,
            'tokens': tokens,
        }

def create_vq_token_dataset(
    vqvae_model,
    data_dir: str,
    transform: transforms.Compose,
    max_seq_len: int = 256,
    device: str = "cuda",
) -> VQTokenDataset:
    """
    åˆ›å»ºVQ Tokenæ•°æ®é›†
    Args:
        vqvae_model: é¢„è®­ç»ƒçš„VQ-VAEæ¨¡å‹
        data_dir: æ•°æ®ç›®å½•
        transform: å›¾åƒå˜æ¢
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        device: è®¡ç®—è®¾å¤‡
    Returns:
        dataset: VQ Tokenæ•°æ®é›†
    """
    from tqdm import tqdm
    
    # åˆ›å»ºåŸºç¡€æ•°æ®é›†
    base_dataset = MicroDopplerDataset(
        data_dir=data_dir,
        transform=transform,
        return_user_id=True,
    )
    
    # é¢„è®¡ç®—tokens
    vqvae_model.eval()
    token_data = []
    
    with torch.no_grad():
        for i in tqdm(range(len(base_dataset)), desc="é¢„è®¡ç®—VQ tokens"):
            image, user_id = base_dataset[i]
            image_tensor = image.unsqueeze(0).to(device)
            
            # ç¼–ç ä¸ºtokens
            result = vqvae_model.encode(image_tensor, return_dict=True)
            tokens = result['encoding_indices'].flatten().cpu()
            
            token_data.append({
                'user_id': user_id,
                'tokens': tokens,
            })
    
    return VQTokenDataset(token_data, max_seq_len)
