#!/usr/bin/env python3
"""
å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®åŠ è½½å™¨
æ”¯æŒç”¨æˆ·IDæ ‡ç­¾å’Œæ•°æ®å¢å¹¿
"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np
from pathlib import Path
import torchvision.transforms as transforms
import json
from typing import Dict, List, Optional, Tuple

class MicroDopplerDataset(Dataset):
    """å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾æ•°æ®é›†"""
    
    def __init__(
        self,
        data_dir: str,
        resolution: int = 256,
        augment: bool = True,
        split: str = "train",
        user_ids: Optional[List[int]] = None
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            resolution: å›¾åƒåˆ†è¾¨ç‡
            augment: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¹¿
            split: æ•°æ®é›†åˆ†å‰² ("train", "val", "test")
            user_ids: æŒ‡å®šç”¨æˆ·IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ç”¨æˆ·
        """
        self.data_dir = Path(data_dir)
        self.resolution = resolution
        self.augment = augment
        self.split = split
        
        # æ‰«ææ•°æ®æ–‡ä»¶
        self.image_paths, self.user_labels = self._scan_data(user_ids)
        
        # åˆ›å»ºç”¨æˆ·IDåˆ°ç´¢å¼•çš„æ˜ å°„
        unique_users = sorted(list(set(self.user_labels)))
        self.user_to_idx = {user: idx for idx, user in enumerate(unique_users)}
        self.idx_to_user = {idx: user for user, idx in self.user_to_idx.items()}
        self.num_users = len(unique_users)
        
        print(f"Loaded {len(self.image_paths)} images from {self.num_users} users")
        print(f"Users: {unique_users}")
        
        # å®šä¹‰å›¾åƒå˜æ¢
        self.transform = self._create_transforms()
    
    def _scan_data(self, user_ids: Optional[List[int]]) -> Tuple[List[Path], List[int]]:
        """
        æ‰«ææ•°æ®ç›®å½•ï¼Œè·å–å›¾åƒè·¯å¾„å’Œç”¨æˆ·æ ‡ç­¾

        é¢„æœŸç›®å½•ç»“æ„:
        data_dir/
        â”œâ”€â”€ ID_1/
        â”‚   â”œâ”€â”€ image_001.png
        â”‚   â”œâ”€â”€ image_002.png
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ ID_2/
        â”‚   â””â”€â”€ ...
        â””â”€â”€ ID_31/
        """
        image_paths = []
        user_labels = []

        # è·å–æ‰€æœ‰ç”¨æˆ·ç›®å½•å¹¶æŒ‰æ•°å­—é¡ºåºæ’åº
        user_dirs = []
        for user_dir in self.data_dir.iterdir():
            if not user_dir.is_dir():
                continue

            # æå–ç”¨æˆ·ID
            user_name = user_dir.name
            if user_name.startswith('ID_'):
                try:
                    user_id = int(user_name.split('_')[1])
                    user_dirs.append((user_id, user_dir))
                except ValueError:
                    print(f"Warning: Invalid user directory name: {user_name}")
                    continue
            else:
                print(f"Warning: Unexpected directory name: {user_name}")
                continue

        # æŒ‰ç”¨æˆ·IDæ•°å­—é¡ºåºæ’åº (1, 2, 3, ..., 31)
        user_dirs.sort(key=lambda x: x[0])

        # å¤„ç†æ¯ä¸ªç”¨æˆ·ç›®å½•
        for user_id, user_dir in user_dirs:
            # å¦‚æœæŒ‡å®šäº†ç”¨æˆ·IDåˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å½“å‰ç”¨æˆ·
            if user_ids is not None and user_id not in user_ids:
                continue

            # æ‰«æç”¨æˆ·ç›®å½•ä¸‹çš„å›¾åƒæ–‡ä»¶
            user_images = []
            for ext in ['*.png', '*.jpg', '*.jpeg', '*.bmp']:
                user_images.extend(user_dir.glob(ext))

            if len(user_images) == 0:
                print(f"Warning: No images found for user {user_id}")
                continue

            # æŒ‰æ–‡ä»¶åæ’åºç¡®ä¿ä¸€è‡´æ€§
            user_images = sorted(user_images, key=lambda x: x.name)

            # æ·»åŠ åˆ°åˆ—è¡¨
            for img_path in user_images:
                image_paths.append(img_path)
                user_labels.append(user_id)

        if len(image_paths) == 0:
            raise ValueError(f"No valid images found in {self.data_dir}")

        return image_paths, user_labels
    
    def _create_transforms(self):
        """
        åˆ›å»ºå›¾åƒå˜æ¢ - é’ˆå¯¹å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ä¼˜åŒ–

        æ³¨æ„ï¼šå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾å¯¹ä¼ ç»Ÿæ•°æ®å¢å¼ºå¾ˆæ•æ„Ÿï¼Œå› ä¸ºï¼š
        1. æ—¶é—´è½´å’Œé¢‘ç‡è½´æœ‰ç‰©ç†æ„ä¹‰ï¼Œä¸èƒ½éšæ„å˜æ¢
        2. é¢œè‰²ä»£è¡¨å¤šæ™®å‹’é¢‘ç§»ï¼Œä¸èƒ½éšæ„è°ƒæ•´
        3. å‡ ä½•å˜æ¢ä¼šç ´åæ—¶é¢‘å…³ç³»
        """
        transform_list = []

        # åŸºç¡€å˜æ¢ - é‡‡ç”¨CelebAæ ‡å‡†åšæ³•
        # CelebAæ ‡å‡†: å…ˆç¼©æ”¾åˆ°64Ã—64ï¼Œç„¶åVAEä¸‹é‡‡æ ·åˆ°8Ã—8
        # è¿™æ˜¯æˆç†Ÿé¡¹ç›®çš„æ ‡å‡†åšæ³•ï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡
        celeba_resolution = 64
        print(f"ğŸ¨ é‡‡ç”¨CelebAæ ‡å‡†: ç¼©æ”¾åˆ° {celeba_resolution}Ã—{celeba_resolution} (åŸå§‹: {self.resolution}Ã—{self.resolution})")
        print(f"   ğŸ“Š ä¼˜åŠ¿: æ˜¾å­˜å‡å°‘5å€, è®­ç»ƒé€Ÿåº¦æå‡4å€, æ‰¹æ¬¡å¤§å°å¢åŠ 8å€")

        transform_list.extend([
            transforms.Resize((celeba_resolution, celeba_resolution)),  # CelebAæ ‡å‡†åˆ†è¾¨ç‡
            transforms.ToTensor(),
        ])

        # å¾®å¤šæ™®å‹’ä¸“ç”¨çš„è½»å¾®å¢å¹¿ (ä»…è®­ç»ƒæ—¶ï¼Œéå¸¸ä¿å®ˆ)
        if self.augment and self.split == "train":
            print("Warning: Using minimal augmentation for micro-Doppler spectrograms")

            # åªä½¿ç”¨å¯¹æ—¶é¢‘å›¾å®‰å…¨çš„å¢å¹¿
            safe_augment_transforms = [
                # æå°çš„éšæœºå™ªå£° (æ¨¡æ‹Ÿæµ‹é‡å™ªå£°)
                transforms.Lambda(lambda x: self._add_measurement_noise(x, noise_factor=0.005)),

                # æå°çš„äº®åº¦è°ƒæ•´ (æ¨¡æ‹Ÿä¿¡å·å¼ºåº¦å˜åŒ–)
                transforms.Lambda(lambda x: self._adjust_signal_strength(x, factor_range=0.05)),
            ]

            # åœ¨ToTensorä¹‹ååº”ç”¨
            for aug_transform in safe_augment_transforms:
                transform_list.append(aug_transform)

        # å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ImageNetçš„å½’ä¸€åŒ–ï¼Œå› ä¸ºå¾®å¤šæ™®å‹’å›¾åƒçš„åˆ†å¸ƒä¸åŒ

        return transforms.Compose(transform_list)
    
    def _add_measurement_noise(self, tensor: torch.Tensor, noise_factor: float = 0.005) -> torch.Tensor:
        """
        æ·»åŠ æµ‹é‡å™ªå£° - æ¨¡æ‹Ÿé›·è¾¾ç³»ç»Ÿçš„æµ‹é‡ä¸ç¡®å®šæ€§
        ä½¿ç”¨å¾ˆå°çš„å™ªå£°å› å­ï¼Œä¸ç ´åæ—¶é¢‘ç»“æ„
        """
        noise = torch.randn_like(tensor) * noise_factor
        return torch.clamp(tensor + noise, 0, 1)

    def _adjust_signal_strength(self, tensor: torch.Tensor, factor_range: float = 0.05) -> torch.Tensor:
        """
        è°ƒæ•´ä¿¡å·å¼ºåº¦ - æ¨¡æ‹Ÿä¸åŒè·ç¦»æˆ–RCSçš„å½±å“
        åªè¿›è¡Œå¾ˆå°çš„äº®åº¦è°ƒæ•´ï¼Œä¿æŒç›¸å¯¹å¼ºåº¦å…³ç³»
        """
        # éšæœºè°ƒæ•´å› å­ [-factor_range, +factor_range]
        adjustment = (torch.rand(1).item() - 0.5) * 2 * factor_range
        adjusted = tensor * (1 + adjustment)
        return torch.clamp(adjusted, 0, 1)
    
    def __len__(self) -> int:
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        # åŠ è½½å›¾åƒ
        img_path = self.image_paths[idx]
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {img_path}: {e}")
            # è¿”å›é»‘è‰²å›¾åƒä½œä¸ºfallback
            image = Image.new('RGB', (self.resolution, self.resolution), (0, 0, 0))
        
        # åº”ç”¨å˜æ¢
        image = self.transform(image)
        
        # è·å–ç”¨æˆ·æ ‡ç­¾
        user_id = self.user_labels[idx]
        user_idx = self.user_to_idx[user_id]
        
        return {
            'image': image,
            'user_id': user_id,
            'user_idx': user_idx,
            'path': str(img_path)
        }
    
    def get_user_samples(self, user_id: int, num_samples: int = 5) -> List[torch.Tensor]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„æ ·æœ¬å›¾åƒ"""
        user_indices = [i for i, uid in enumerate(self.user_labels) if uid == user_id]
        
        if len(user_indices) == 0:
            raise ValueError(f"User {user_id} not found in dataset")
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        selected_indices = np.random.choice(
            user_indices, 
            size=min(num_samples, len(user_indices)), 
            replace=False
        )
        
        samples = []
        for idx in selected_indices:
            sample = self[idx]
            samples.append(sample['image'])
        
        return samples

class MicroDopplerDataModule:
    """æ•°æ®æ¨¡å—ï¼Œç®¡ç†è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†"""
    
    def __init__(
        self,
        data_dir: str,
        batch_size: int = 16,
        num_workers: int = 4,
        resolution: int = 256,
        val_split: float = 0.2,
        test_split: float = 0.1,
        seed: int = 42
    ):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.resolution = resolution
        self.val_split = val_split
        self.test_split = test_split
        self.seed = seed
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(seed)
        
        # æ‰«ææ‰€æœ‰ç”¨æˆ·
        self.all_users = self._get_all_users()
        
        # åˆ†å‰²ç”¨æˆ·
        self.train_users, self.val_users, self.test_users = self._split_users()
        
        print(f"Data split:")
        print(f"  Train users: {len(self.train_users)} - {self.train_users}")
        print(f"  Val users: {len(self.val_users)} - {self.val_users}")
        print(f"  Test users: {len(self.test_users)} - {self.test_users}")
    
    def _get_all_users(self) -> List[int]:
        """è·å–æ‰€æœ‰ç”¨æˆ·ID"""
        data_path = Path(self.data_dir)
        users = []

        for user_dir in data_path.iterdir():
            if user_dir.is_dir() and user_dir.name.startswith('ID_'):
                try:
                    user_id = int(user_dir.name.split('_')[1])
                    users.append(user_id)
                except ValueError:
                    continue

        return sorted(users)
    
    def _split_users(self) -> Tuple[List[int], List[int], List[int]]:
        """æŒ‰ç”¨æˆ·åˆ†å‰²æ•°æ®é›†"""
        users = self.all_users.copy()
        np.random.shuffle(users)
        
        n_users = len(users)
        n_test = max(1, int(n_users * self.test_split))
        n_val = max(1, int(n_users * self.val_split))
        n_train = n_users - n_test - n_val
        
        train_users = users[:n_train]
        val_users = users[n_train:n_train + n_val]
        test_users = users[n_train + n_val:]
        
        return sorted(train_users), sorted(val_users), sorted(test_users)
    
    def get_dataloader(self, split: str = "train") -> DataLoader:
        """è·å–æ•°æ®åŠ è½½å™¨"""
        if split == "train":
            user_ids = self.train_users
            shuffle = True
            augment = True
        elif split == "val":
            user_ids = self.val_users
            shuffle = False
            augment = False
        elif split == "test":
            user_ids = self.test_users
            shuffle = False
            augment = False
        else:
            raise ValueError(f"Invalid split: {split}")
        
        dataset = MicroDopplerDataset(
            data_dir=self.data_dir,
            resolution=self.resolution,
            augment=augment,
            split=split,
            user_ids=user_ids
        )
        
        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=shuffle,
            num_workers=self.num_workers,
            pin_memory=True,
            drop_last=(split == "train")
        )
    
    def get_all_dataloader(self) -> DataLoader:
        """è·å–åŒ…å«æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®åŠ è½½å™¨"""
        dataset = MicroDopplerDataset(
            data_dir=self.data_dir,
            resolution=self.resolution,
            augment=False,
            split="all",
            user_ids=None
        )
        
        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )

def test_dataloader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_dir = Path("./test_data")
    test_dir.mkdir(exist_ok=True)

    # åˆ›å»ºç”¨æˆ·ç›®å½•å’Œæµ‹è¯•å›¾åƒ (ä½¿ç”¨ID_æ ¼å¼)
    for user_id in [1, 2, 3]:
        user_dir = test_dir / f"ID_{user_id}"
        user_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        for i in range(5):
            img = Image.new('RGB', (256, 256),
                          (user_id * 50, i * 40, 100))
            img.save(user_dir / f"image_{i:03d}.png")

    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    dataset = MicroDopplerDataset(
        data_dir=str(test_dir),
        resolution=256,
        augment=True
    )

    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

    # æµ‹è¯•ä¸€ä¸ªbatch
    for batch in dataloader:
        print(f"Batch shape: {batch['image'].shape}")
        print(f"User IDs: {batch['user_id']}")
        print(f"User indices: {batch['user_idx']}")
        break

    print("Data loader test passed!")

if __name__ == "__main__":
    test_dataloader()
