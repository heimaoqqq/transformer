#!/usr/bin/env python3
"""
å¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ VAE è®­ç»ƒè„šæœ¬
åŸºäº Diffusers AutoencoderKL å®ç°
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from diffusers import AutoencoderKL
from accelerate import Accelerator
from tqdm import tqdm
import wandb
from pathlib import Path
import argparse
import json
from PIL import Image
import numpy as np

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))

from utils.data_loader import MicroDopplerDataset
from utils.metrics import calculate_psnr, calculate_ssim

class MicroDopplerVAELoss(nn.Module):
    """å¾®å¤šæ™®å‹’VAEæŸå¤±å‡½æ•°"""
    
    def __init__(self, kl_weight=1e-6, perceptual_weight=0.1, freq_weight=0.05):
        super().__init__()
        self.kl_weight = kl_weight
        self.perceptual_weight = perceptual_weight
        self.freq_weight = freq_weight
        
        # åŸºç¡€æŸå¤±
        self.mse_loss = nn.MSELoss()
        
        # æ„ŸçŸ¥æŸå¤± (å¯é€‰ï¼Œéœ€è¦å®‰è£… lpips)
        try:
            import lpips
            self.lpips_loss = lpips.LPIPS(net='vgg')
        except ImportError:
            print("Warning: LPIPS not available, using MSE only")
            self.lpips_loss = None
    
    def forward(self, reconstruction, target, posterior):
        """
        è®¡ç®—VAEæŸå¤±
        Args:
            reconstruction: é‡å»ºå›¾åƒ [B, C, H, W]
            target: ç›®æ ‡å›¾åƒ [B, C, H, W]
            posterior: VAEåéªŒåˆ†å¸ƒ
        """
        batch_size = target.shape[0]
        
        # 1. é‡å»ºæŸå¤±
        recon_loss = self.mse_loss(reconstruction, target)
        
        # 2. KLæ•£åº¦æŸå¤±
        kl_loss = posterior.kl().mean()
        
        # 3. æ„ŸçŸ¥æŸå¤±
        perceptual_loss = 0.0
        if self.lpips_loss is not None:
            # å½’ä¸€åŒ–åˆ°[-1, 1]
            recon_norm = reconstruction * 2.0 - 1.0
            target_norm = target * 2.0 - 1.0
            perceptual_loss = self.lpips_loss(recon_norm, target_norm).mean()
        
        # 4. é¢‘åŸŸæŸå¤± (ä¿æŒæ—¶é¢‘ç‰¹æ€§)
        freq_loss = 0.0
        if self.freq_weight > 0:
            # FFTå˜æ¢
            recon_fft = torch.fft.fft2(reconstruction)
            target_fft = torch.fft.fft2(target)
            
            # å¹…åº¦è°±æŸå¤±
            recon_mag = torch.abs(recon_fft)
            target_mag = torch.abs(target_fft)
            freq_loss = self.mse_loss(recon_mag, target_mag)
        
        # æ€»æŸå¤±
        total_loss = (recon_loss + 
                     self.kl_weight * kl_loss + 
                     self.perceptual_weight * perceptual_loss + 
                     self.freq_weight * freq_loss)
        
        return {
            'total_loss': total_loss,
            'recon_loss': recon_loss,
            'kl_loss': kl_loss,
            'perceptual_loss': perceptual_loss,
            'freq_loss': freq_loss
        }

def train_vae(args):
    """VAEè®­ç»ƒä¸»å‡½æ•°"""

    # å†…å­˜ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True

    # æ£€æŸ¥GPUé…ç½®
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")

        for i in range(gpu_count):
            gpu_props = torch.cuda.get_device_properties(i)
            gpu_memory = gpu_props.total_memory / 1024**3
            print(f"   GPU {i}: {gpu_props.name} - {gpu_memory:.1f} GB")
            torch.cuda.empty_cache()

        # å¦‚æœæœ‰å¤šä¸ªGPUï¼Œç¡®ä¿ä½¿ç”¨æ‰€æœ‰GPU
        if gpu_count > 1:
            print(f"âœ… å°†ä½¿ç”¨æ‰€æœ‰ {gpu_count} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        else:
            print("âš ï¸  åªæ£€æµ‹åˆ°1ä¸ªGPUï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥GPUé…ç½®")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°GPU")

    # åˆå§‹åŒ–åŠ é€Ÿå™¨ (è‡ªåŠ¨æ£€æµ‹å¤šGPU)
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with="wandb" if args.use_wandb else None,
        project_dir=args.output_dir
    )

    # æ‰“å°å®é™…ä½¿ç”¨çš„è®¾å¤‡ä¿¡æ¯
    print(f"ğŸš€ è®­ç»ƒè®¾å¤‡: {accelerator.device}")
    print(f"ğŸ”¢ è¿›ç¨‹æ•°: {accelerator.num_processes}")
    print(f"ğŸ“Š æ˜¯å¦åˆ†å¸ƒå¼: {accelerator.distributed_type}")
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–wandb
    if args.use_wandb and accelerator.is_main_process:
        wandb.init(
            project="micro-doppler-vae",
            config=vars(args),
            name=f"vae-{args.experiment_name}"
        )
    
    # åˆ›å»ºVAEæ¨¡å‹
    vae = AutoencoderKL(
        in_channels=3,
        out_channels=3,
        down_block_types=[
            "DownEncoderBlock2D",
            "DownEncoderBlock2D", 
            "DownEncoderBlock2D",
            "DownEncoderBlock2D"
        ],
        up_block_types=[
            "UpDecoderBlock2D",
            "UpDecoderBlock2D",
            "UpDecoderBlock2D",
            "UpDecoderBlock2D"
        ],
        block_out_channels=[128, 256, 512, 512],
        latent_channels=4,
        sample_size=args.resolution,
        layers_per_block=2,
        act_fn="silu",
        norm_num_groups=32,
        scaling_factor=0.18215,
    )
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = MicroDopplerDataset(
        data_dir=args.data_dir,
        resolution=args.resolution,
        augment=args.use_augmentation
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        vae.parameters(),
        lr=args.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=args.weight_decay,
        eps=1e-8
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=args.num_epochs,
        eta_min=args.learning_rate * 0.1
    )
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_fn = MicroDopplerVAELoss(
        kl_weight=args.kl_weight,
        perceptual_weight=args.perceptual_weight,
        freq_weight=args.freq_weight
    )
    
    # ä½¿ç”¨acceleratorå‡†å¤‡æ¨¡å‹å’Œæ•°æ®
    vae, optimizer, dataloader, lr_scheduler = accelerator.prepare(
        vae, optimizer, dataloader, lr_scheduler
    )
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    
    for epoch in range(args.num_epochs):
        vae.train()
        epoch_loss = 0.0
        
        progress_bar = tqdm(
            dataloader, 
            desc=f"Epoch {epoch+1}/{args.num_epochs}",
            disable=not accelerator.is_local_main_process
        )
        
        for step, batch in enumerate(progress_bar):
            with accelerator.accumulate(vae):
                # è·å–å›¾åƒæ•°æ®
                images = batch['image']  # [B, C, H, W]
                
                # VAEå‰å‘ä¼ æ’­
                posterior = vae.encode(images).latent_dist
                latents = posterior.sample()
                reconstruction = vae.decode(latents).sample
                
                # è®¡ç®—æŸå¤±
                loss_dict = loss_fn(reconstruction, images, posterior)
                loss = loss_dict['total_loss']
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(vae.parameters(), args.max_grad_norm)
                
                optimizer.step()
                optimizer.zero_grad()

                # å®šæœŸæ¸…ç†GPUç¼“å­˜
                if global_step % 50 == 0 and torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # æ›´æ–°è¿›åº¦æ¡
            if accelerator.sync_gradients:
                global_step += 1
                epoch_loss += loss.item()
                
                # è®°å½•æ—¥å¿—
                if global_step % args.log_interval == 0:
                    avg_loss = epoch_loss / (step + 1)
                    
                    logs = {
                        "epoch": epoch,
                        "step": global_step,
                        "lr": lr_scheduler.get_last_lr()[0],
                        "loss/total": loss_dict['total_loss'].item(),
                        "loss/recon": loss_dict['recon_loss'].item(),
                        "loss/kl": loss_dict['kl_loss'].item(),
                        "loss/perceptual": loss_dict['perceptual_loss'].item(),
                        "loss/freq": loss_dict['freq_loss'].item(),
                    }
                    
                    progress_bar.set_postfix(loss=f"{avg_loss:.4f}")
                    
                    if args.use_wandb and accelerator.is_main_process:
                        wandb.log(logs, step=global_step)
                
                # ä¿å­˜æ ·æœ¬å›¾åƒ
                if global_step % args.sample_interval == 0 and accelerator.is_main_process:
                    save_sample_images(
                        images[:4], reconstruction[:4], 
                        args.output_dir, global_step
                    )
        
        # æ›´æ–°å­¦ä¹ ç‡
        lr_scheduler.step()
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_interval == 0 and accelerator.is_main_process:
            save_checkpoint(vae, optimizer, epoch, global_step, args.output_dir)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if accelerator.is_main_process:
        save_final_model(vae, args.output_dir)
        print(f"Training completed! Model saved to {args.output_dir}")

def save_sample_images(original, reconstruction, output_dir, step):
    """ä¿å­˜æ ·æœ¬å›¾åƒ"""
    import torchvision.utils as vutils
    
    # æ‹¼æ¥åŸå›¾å’Œé‡å»ºå›¾
    comparison = torch.cat([original, reconstruction], dim=0)
    
    # ä¿å­˜å›¾åƒ
    sample_dir = Path(output_dir) / "samples"
    sample_dir.mkdir(exist_ok=True)
    
    vutils.save_image(
        comparison,
        sample_dir / f"step_{step:06d}.png",
        nrow=4,
        normalize=True,
        value_range=(0, 1)
    )

def save_checkpoint(model, optimizer, epoch, step, output_dir):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
    checkpoint_dir = Path(output_dir) / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)
    
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'step': step,
    }
    
    torch.save(checkpoint, checkpoint_dir / f"checkpoint_epoch_{epoch+1}.pt")

def save_final_model(model, output_dir):
    """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
    model.save_pretrained(Path(output_dir) / "final_model")

def main():
    parser = argparse.ArgumentParser(description="Train VAE for Micro-Doppler Images")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, required=True, help="æ•°æ®é›†ç›®å½•")
    parser.add_argument("--resolution", type=int, default=256, help="å›¾åƒåˆ†è¾¨ç‡")
    parser.add_argument("--use_augmentation", action="store_true", help="ä½¿ç”¨æ•°æ®å¢å¹¿")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--num_epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æ¢¯åº¦è£å‰ª")
    
    # æŸå¤±æƒé‡
    parser.add_argument("--kl_weight", type=float, default=1e-6, help="KLæ•£åº¦æƒé‡")
    parser.add_argument("--perceptual_weight", type=float, default=0.1, help="æ„ŸçŸ¥æŸå¤±æƒé‡")
    parser.add_argument("--freq_weight", type=float, default=0.05, help="é¢‘åŸŸæŸå¤±æƒé‡")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--mixed_precision", type=str, default="fp16", help="æ··åˆç²¾åº¦")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    
    # æ—¥å¿—å’Œä¿å­˜
    parser.add_argument("--output_dir", type=str, default="./outputs/vae", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--experiment_name", type=str, default="baseline", help="å®éªŒåç§°")
    parser.add_argument("--log_interval", type=int, default=10, help="æ—¥å¿—è®°å½•é—´éš”")
    parser.add_argument("--sample_interval", type=int, default=500, help="æ ·æœ¬ä¿å­˜é—´éš”")
    parser.add_argument("--save_interval", type=int, default=10, help="æ¨¡å‹ä¿å­˜é—´éš”")
    parser.add_argument("--use_wandb", action="store_true", help="ä½¿ç”¨wandbè®°å½•")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # å¼€å§‹è®­ç»ƒ
    train_vae(args)

if __name__ == "__main__":
    main()
