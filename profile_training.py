#!/usr/bin/env python3
"""
è®­ç»ƒæ€§èƒ½åˆ†æå™¨ - æ‰¾å‡ºçœŸæ­£çš„ç“¶é¢ˆ
"""

import os
import sys
import torch
import time
import psutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/kaggle/working/VAE')

def profile_model_creation():
    """åˆ†ææ¨¡å‹åˆ›å»ºæ—¶é—´"""
    print("ğŸ” åˆ†ææ¨¡å‹åˆ›å»º...")
    
    from diffusers import AutoencoderKL
    
    # 3å±‚ä¸‹é‡‡æ ·é…ç½®
    start_time = time.time()
    vae_3layer = AutoencoderKL(
        in_channels=3,
        out_channels=3,
        down_block_types=[
            "DownEncoderBlock2D",  # 256â†’128
            "DownEncoderBlock2D",  # 128â†’64
            "DownEncoderBlock2D"   # 64â†’32
        ],
        up_block_types=[
            "UpDecoderBlock2D",    # 32â†’64
            "UpDecoderBlock2D",    # 64â†’128
            "UpDecoderBlock2D"     # 128â†’256
        ],
        block_out_channels=[128, 256, 512],
        latent_channels=4,
        sample_size=256,
        layers_per_block=2,
    )
    creation_time = time.time() - start_time
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in vae_3layer.parameters())
    
    print(f"   âœ… 3å±‚VAEåˆ›å»ºæ—¶é—´: {creation_time:.2f}s")
    print(f"   ğŸ“Š å‚æ•°é‡: {total_params:,}")
    print(f"   ğŸ’¾ æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f} MB")
    
    # å¯¹æ¯”4å±‚é…ç½®
    start_time = time.time()
    vae_4layer = AutoencoderKL(
        in_channels=3,
        out_channels=3,
        down_block_types=[
            "DownEncoderBlock2D",
            "DownEncoderBlock2D", 
            "DownEncoderBlock2D",
            "DownEncoderBlock2D"
        ],
        up_block_types=[
            "UpDecoderBlock2D",
            "UpDecoderBlock2D",
            "UpDecoderBlock2D",
            "UpDecoderBlock2D"
        ],
        block_out_channels=[128, 256, 512, 512],
        latent_channels=4,
        sample_size=256,
        layers_per_block=2,
    )
    creation_time_4 = time.time() - start_time
    
    total_params_4 = sum(p.numel() for p in vae_4layer.parameters())
    
    print(f"\n   ğŸ“Š å¯¹æ¯”4å±‚VAE:")
    print(f"   â±ï¸  åˆ›å»ºæ—¶é—´: {creation_time_4:.2f}s")
    print(f"   ğŸ“Š å‚æ•°é‡: {total_params_4:,}")
    print(f"   ğŸ“‰ å‚æ•°å‡å°‘: {(total_params_4 - total_params) / total_params_4 * 100:.1f}%")
    
    return vae_3layer

def profile_data_loading():
    """åˆ†ææ•°æ®åŠ è½½æ€§èƒ½"""
    print("\nğŸ” åˆ†ææ•°æ®åŠ è½½...")
    
    try:
        from utils.data_loader import MicroDopplerDataset
        from torch.utils.data import DataLoader
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MicroDopplerDataset(
            data_dir="/kaggle/input/dataset",
            resolution=256,
            split="train"
        )
        
        print(f"   ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # æµ‹è¯•ä¸åŒé…ç½®çš„æ•°æ®åŠ è½½å™¨
        configs = [
            {"batch_size": 2, "num_workers": 0, "pin_memory": False},
            {"batch_size": 4, "num_workers": 0, "pin_memory": False},
            {"batch_size": 4, "num_workers": 1, "pin_memory": True},
            {"batch_size": 4, "num_workers": 2, "pin_memory": True},
        ]
        
        for config in configs:
            dataloader = DataLoader(dataset, **config)
            
            # æµ‹è¯•åŠ è½½æ—¶é—´
            start_time = time.time()
            for i, batch in enumerate(dataloader):
                if i >= 5:  # åªæµ‹è¯•å‰5ä¸ªæ‰¹æ¬¡
                    break
            load_time = time.time() - start_time
            
            print(f"   â±ï¸  é…ç½® {config}: {load_time/5:.2f}s/æ‰¹æ¬¡")
            
    except Exception as e:
        print(f"   âŒ æ•°æ®åŠ è½½åˆ†æå¤±è´¥: {e}")

def profile_forward_pass():
    """åˆ†æå‰å‘ä¼ æ’­æ€§èƒ½"""
    print("\nğŸ” åˆ†æå‰å‘ä¼ æ’­...")
    
    device = torch.device("cuda:0")
    vae = profile_model_creation()
    vae = vae.to(device)
    vae.eval()
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
    batch_sizes = [1, 2, 4, 6]
    
    for batch_size in batch_sizes:
        print(f"\n   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_input = torch.randn(batch_size, 3, 256, 256).to(device)
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = vae.encode(test_input).latent_dist.sample()
            _ = vae.decode(_).sample
        
        torch.cuda.synchronize()
        
        # æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­
        times = []
        for _ in range(10):
            torch.cuda.synchronize()
            start_time = time.time()
            
            with torch.no_grad():
                posterior = vae.encode(test_input).latent_dist
                latents = posterior.sample()
                reconstruction = vae.decode(latents).sample
            
            torch.cuda.synchronize()
            times.append(time.time() - start_time)
        
        avg_time = sum(times) / len(times)
        throughput = batch_size / avg_time
        
        print(f"      â±ï¸  å‰å‘ä¼ æ’­: {avg_time*1000:.1f}ms")
        print(f"      ğŸš€ ååé‡: {throughput:.1f} æ ·æœ¬/ç§’")
        
        # å†…å­˜ä½¿ç”¨
        memory_used = torch.cuda.max_memory_allocated() / 1024**3
        print(f"      ğŸ’¾ GPUå†…å­˜: {memory_used:.1f} GB")
        
        torch.cuda.empty_cache()

def estimate_training_time():
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    print("\nğŸ” ä¼°ç®—è®­ç»ƒæ—¶é—´...")
    
    try:
        from utils.data_loader import MicroDopplerDataset
        
        dataset = MicroDopplerDataset(
            data_dir="/kaggle/input/dataset",
            resolution=256,
            split="train"
        )
        
        dataset_size = len(dataset)
        batch_size = 4  # train_safe.pyçš„é…ç½®
        steps_per_epoch = dataset_size // batch_size
        
        print(f"   ğŸ“Š æ•°æ®é›†: {dataset_size} æ ·æœ¬")
        print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   ğŸ“ˆ æ¯è½®æ­¥æ•°: {steps_per_epoch}")
        
        # åŸºäºå®é™…æµ‹è¯•çš„æ—¶é—´ä¼°ç®—
        estimated_time_per_step = 2.5  # ç§’ (3å±‚ä¸‹é‡‡æ ·åº”è¯¥æ›´å¿«)
        time_per_epoch = steps_per_epoch * estimated_time_per_step
        
        print(f"\n   â±ï¸  æ—¶é—´ä¼°ç®—:")
        print(f"      æ¯æ­¥æ—¶é—´: ~{estimated_time_per_step:.1f}s")
        print(f"      æ¯è½®æ—¶é—´: ~{time_per_epoch/60:.1f}åˆ†é’Ÿ")
        print(f"      30è½®æ€»æ—¶é—´: ~{time_per_epoch*30/3600:.1f}å°æ—¶")
        
        # å¦‚æœè¿˜æ˜¯30åˆ†é’Ÿä¸€è½®ï¼Œè¯´æ˜æœ‰å…¶ä»–ç“¶é¢ˆ
        if time_per_epoch > 25 * 60:  # è¶…è¿‡25åˆ†é’Ÿ
            print(f"\n   âš ï¸  è­¦å‘Š: é¢„ä¼°æ—¶é—´ä»ç„¶å¾ˆé•¿!")
            print(f"      å¯èƒ½çš„ç“¶é¢ˆ:")
            print(f"      - æ•°æ®åŠ è½½æ…¢ (ç£ç›˜I/O)")
            print(f"      - ç½‘ç»œé€šä¿¡æ…¢ (å¤šGPUåŒæ­¥)")
            print(f"      - å†…å­˜ç¢ç‰‡åŒ–")
            print(f"      - CPUç“¶é¢ˆ")
        
    except Exception as e:
        print(f"   âŒ æ—¶é—´ä¼°ç®—å¤±è´¥: {e}")

def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    print("\nğŸ” æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
    
    # CPUä¿¡æ¯
    cpu_percent = psutil.cpu_percent(interval=1)
    cpu_count = psutil.cpu_count()
    
    # å†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()
    
    # GPUä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    
    print(f"   ğŸ–¥ï¸  CPU: {cpu_count}æ ¸, ä½¿ç”¨ç‡: {cpu_percent:.1f}%")
    print(f"   ğŸ’¾ RAM: {memory.total/1024**3:.1f}GB, ä½¿ç”¨ç‡: {memory.percent:.1f}%")
    print(f"   ğŸ® GPU: {gpu_count}ä¸ª")
    
    if torch.cuda.is_available():
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            memory_used = torch.cuda.memory_allocated(i) / 1024**3
            memory_total = props.total_memory / 1024**3
            print(f"      GPU {i}: {props.name}, {memory_used:.1f}/{memory_total:.1f}GB")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” VAEè®­ç»ƒæ€§èƒ½åˆ†æ")
    print("=" * 80)
    
    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    check_system_resources()
    
    # åˆ†ææ¨¡å‹
    profile_model_creation()
    
    # åˆ†ææ•°æ®åŠ è½½
    profile_data_loading()
    
    # åˆ†æå‰å‘ä¼ æ’­
    profile_forward_pass()
    
    # ä¼°ç®—è®­ç»ƒæ—¶é—´
    estimate_training_time()
    
    print("\n" + "=" * 80)
    print("âœ… æ€§èƒ½åˆ†æå®Œæˆ!")
    print("\nğŸ’¡ å¦‚æœè®­ç»ƒä»ç„¶å¾ˆæ…¢ï¼Œå¯èƒ½çš„åŸå› :")
    print("   1. æ•°æ®åŠ è½½ç“¶é¢ˆ (ç£ç›˜I/O)")
    print("   2. å¤šGPUé€šä¿¡å¼€é”€")
    print("   3. æ¢¯åº¦ç´¯ç§¯å»¶è¿Ÿ")
    print("   4. ç³»ç»Ÿèµ„æºç«äº‰")

if __name__ == "__main__":
    main()
